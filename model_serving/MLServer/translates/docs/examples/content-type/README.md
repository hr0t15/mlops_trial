<!--
# Content Type Decoding

MLServer extends the V2 inference protocol by adding support for a `content_type` annotation.
This annotation can be provided either through the model metadata `parameters`, or through the input `parameters`.
By leveraging the `content_type` annotation, we can provide the necessary information to MLServer so that it can _decode_ the input payload from the "wire" V2 protocol to something meaningful to the model / user (e.g. a NumPy array).

This example will walk you through some examples which illustrate how this works, and how it can be extended.
-->

<!--
## Echo Inference Runtime

To start with, we will write a _dummy_ runtime which just prints the input, the _decoded_ input and returns it.
This will serve as a testbed to showcase how the `content_type` support works.

Later on, we will extend this runtime by adding custom _codecs_ that will decode our V2 payload to custom types.
-->

# ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã®ãƒ‡ã‚³ãƒ¼ãƒ‰

MLServerã¯ã€`content_type` ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€V2 æ¨è«–ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ‹¡å¼µã—ã¾ã™ã€‚
ã“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã® `parameters` ã¾ãŸã¯å…¥åŠ›ã® `parameters` ã‚’é€šã˜ã¦æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
`content_type` ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€MLServerã«å¿…è¦ãªæƒ…å ±ã‚’æä¾›ã—ã€å…¥åŠ›ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ "wire" V2 ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«/ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ„å‘³ã®ã‚ã‚‹ã‚‚ã®ï¼ˆä¾‹ï¼šNumPy é…åˆ—ï¼‰ã« _ãƒ‡ã‚³ãƒ¼ãƒ‰_ ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ã“ã®ä¾‹ã§ã¯ã€ã“ã®ä»•çµ„ã¿ã®å‹•ä½œã¨æ‹¡å¼µæ–¹æ³•ã‚’ç¤ºã™ã„ãã¤ã‹ã®ä¾‹ã‚’é€šã˜ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ã‚¨ã‚³ãƒ¼æ¨è«–ãƒ©ãƒ³ã‚¿ã‚¤ãƒ 

ã¾ãšã€å…¥åŠ›ã¨ _ãƒ‡ã‚³ãƒ¼ãƒ‰_ ã•ã‚ŒãŸå…¥åŠ›ã‚’å˜ã«è¡¨ç¤ºã—ã€ãã‚Œã‚’è¿”ã™ _ãƒ€ãƒŸãƒ¼_ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½œæˆã—ã¾ã™ã€‚
ã“ã‚Œã¯ã€`content_type` ã‚µãƒãƒ¼ãƒˆã®å‹•ä½œã‚’ç¤ºã™ãŸã‚ã®ãƒ†ã‚¹ãƒˆãƒ™ãƒƒãƒ‰ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚

ãã®å¾Œã€ã“ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’æ‹¡å¼µã—ã¦ã€V2 ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¤ãƒ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ  _ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯_ ã‚’è¿½åŠ ã—ã¾ã™ã€‚


```python
%%writefile runtime.py
import json

from mlserver import MLModel
from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput
from mlserver.codecs import DecodedParameterName

_to_exclude = {
    "parameters": {DecodedParameterName, "headers"},
    'inputs': {"__all__": {"parameters": {DecodedParameterName, "headers"}}}
}

class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        outputs = []
        for request_input in payload.inputs:
            decoded_input = self.decode(request_input)
            print(f"------ Encoded Input ({request_input.name}) ------")
            as_dict = request_input.dict(exclude=_to_exclude)  # type: ignore
            print(json.dumps(as_dict, indent=2))
            print(f"------ Decoded input ({request_input.name}) ------")
            print(decoded_input)
            
            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data
                )
            )
        
        return InferenceResponse(model_name=self.name, outputs=outputs)
        
```

<!--
As you can see above, this runtime will decode the incoming payloads by calling the `self.decode()` helper method.
This method will check what's the right content type for each input in the following order:

1. Is there any content type defined in the `inputs[].parameters.content_type` field within the **request payload**?
2. Is there any content type defined in the `inputs[].parameters.content_type` field within the **model metadata**?
3. Is there any default content type that should be assumed?
-->

ä¸Šè¨˜ã®ã‚ˆã†ã«ã€ã“ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã¯ `self.decode()` ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€å—ä¿¡ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€æ¬¡ã®é †åºã§å„å…¥åŠ›ã®é©åˆ‡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèªã—ã¾ã™ï¼š

1. **ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰** å†…ã® `inputs[].parameters.content_type` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
2. **ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿** å†…ã® `inputs[].parameters.content_type` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
3. æƒ³å®šã•ã‚Œã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ

<!--
### Model Settings

In order to enable this runtime, we will also create a `model-settings.json` file.
This file should be present (or accessible from) in the folder where we run `mlserver start .`.
-->


### ãƒ¢ãƒ‡ãƒ«è¨­å®š

ã“ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã«ã€`model-settings.json` ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆã—ã¾ã™ã€‚
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€`mlserver start .` ã‚’å®Ÿè¡Œã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã«å­˜åœ¨ã™ã‚‹ã‹ã€ãã“ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚


```python
%%writefile model-settings.json

{
    "name": "content-type-example",
    "implementation": "runtime.EchoRuntime"
}
```

<!--
## Request Inputs

Our initial step will be to decide the content type based on the incoming `inputs[].parameters` field.
For this, we will start our MLServer in the background (e.g. running `mlserver start .`)
-->

## ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…¥åŠ›

æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€å—ä¿¡ã™ã‚‹ `inputs[].parameters` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«åŸºã¥ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šã™ã‚‹ã“ã¨ã§ã™ã€‚
ã“ã‚Œã®ãŸã‚ã«ã€MLServer ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•ã—ã¾ã™ï¼ˆä¾‹ï¼š`mlserver start .` ã‚’å®Ÿè¡Œã™ã‚‹ï¼‰ã€‚


```python
import requests

payload = {
    "inputs": [
        {
            "name": "parameters-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "parameters-str",
            "datatype": "BYTES",
            "shape": [1],
            "data": "hello world ğŸ˜",
            "parameters": {
                "content_type": "str"
            }
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

<!--
### Codecs

As you've probably already noticed, writing request payloads compliant with both the V2 Inference Protocol requires a certain knowledge about both the V2 spec and the structure expected by each content type.
To account for this and simplify usage, the MLServer package exposes a set of utilities which will help you interact with your models via the V2 protocol.

These helpers are mainly shaped as _"codecs"_.
That is, abstractions which know how to _"encode"_ and _"decode"_ arbitrary Python datatypes to and from the V2 Inference Protocol.

Generally, we recommend using the existing set of codecs to generate your V2 payloads.
This will ensure that requests and responses follow the right structure, and should provide a more seamless experience.

Following with our previous example, the same code could be rewritten using codecs as:
-->

### ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯

ãŠãã‚‰ãã™ã§ã«æ°—ä»˜ã„ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€V2 æ¨è«–ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æº–æ‹ ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€V2 ã‚¹ãƒšãƒƒã‚¯ã¨å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ãŒæœŸå¾…ã™ã‚‹æ§‹é€ ã«é–¢ã™ã‚‹ä¸€å®šã®çŸ¥è­˜ãŒå¿…è¦ã§ã™ã€‚
ã“ã‚Œã‚’è€ƒæ…®ã—ã¦ã€ä½¿ç”¨ã‚’ç°¡ç´ åŒ–ã™ã‚‹ãŸã‚ã«ã€MLServer ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ V2 ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä»‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã¨ã‚„ã‚Šå–ã‚Šã™ã‚‹ã®ã‚’åŠ©ã‘ã‚‹ä¸€é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã¯ä¸»ã« _"ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯"_ ã¨ã—ã¦å½¢æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
ã¤ã¾ã‚Šã€ä»»æ„ã® Python ãƒ‡ãƒ¼ã‚¿å‹ã‚’ V2 æ¨è«–ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‹ã‚‰ _"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"_ ãŠã‚ˆã³ _"ãƒ‡ã‚³ãƒ¼ãƒ‰"_ ã™ã‚‹æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹æŠ½è±¡åŒ–ã§ã™ã€‚

ä¸€èˆ¬çš„ã«ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ V2 ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£ã—ã„æ§‹é€ ã«å¾“ã„ã€ã‚ˆã‚Šã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªä½“é¨“ãŒæä¾›ã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

å…ˆã®ä¾‹ã‚’å¼•ãç¶šãä½¿ç”¨ã™ã‚‹ã¨ã€åŒã˜ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦æ¬¡ã®ã‚ˆã†ã«æ›¸ãç›´ã™ã“ã¨ãŒã§ãã¾ã™ï¼š


```python
import requests
import numpy as np

from mlserver.types import InferenceRequest, InferenceResponse
from mlserver.codecs import NumpyCodec, StringCodec

parameters_np = np.array([[1, 2], [3, 4]])
parameters_str = ["hello world ğŸ˜"]

payload = InferenceRequest(
    inputs=[
        NumpyCodec.encode_input("parameters-np", parameters_np),
        # The `use_bytes=False` flag will ensure that the encoded payload is JSON-compatible
        StringCodec.encode_input("parameters-str", parameters_str, use_bytes=False),
    ]
)

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload.dict()
)

response_payload = InferenceResponse.parse_raw(response.text)
print(NumpyCodec.decode_output(response_payload.outputs[0]))
print(StringCodec.decode_output(response_payload.outputs[1]))
```

<!--
Note that the rewritten snippet now makes use of the built-in `InferenceRequest` class, which represents a V2 inference request.
On top of that, it also uses the `NumpyCodec` and `StringCodec` implementations, which know how to encode a Numpy array and a list of strings into V2-compatible request inputs.
-->

æ›¸ãç›´ã•ã‚ŒãŸã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã¯ã€V2 æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¡¨ã™çµ„ã¿è¾¼ã¿ã® `InferenceRequest` ã‚¯ãƒ©ã‚¹ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãã®ä¸Šã§ã€`NumpyCodec` ã¨ `StringCodec` ã®å®Ÿè£…ã‚‚ä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã‚‰ã¯ Numpy é…åˆ—ã¨æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’ V2 äº’æ›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…¥åŠ›ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã¾ã™ã€‚

<!--
### Model Metadata

Our next step will be to define the expected content type through the model metadata.
This can be done by extending the `model-settings.json` file, and adding a section on inputs.
-->

### ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é€šã˜ã¦æœŸå¾…ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã™ã€‚
ã“ã‚Œã¯ `model-settings.json` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‹¡å¼µã—ã€å…¥åŠ›ã«é–¢ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚


```python
%%writefile model-settings.json

{
    "name": "content-type-example",
    "implementation": "runtime.EchoRuntime",
    "inputs": [
        {
            "name": "metadata-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "metadata-str",
            "datatype": "BYTES",
            "shape": [11],
            "parameters": {
                "content_type": "str"
            }
        }
    ]
}
```

<!--
After adding this metadata, we will re-start MLServer (e.g. `mlserver start .`) and we will send a new request without any explicit `parameters`.
-->

ã“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ãŸå¾Œã€MLServer ã‚’å†èµ·å‹•ã—ã¾ã™ï¼ˆä¾‹ï¼š`mlserver start .`ï¼‰ã—ã€æ˜ç¤ºçš„ãª `parameters` ãªã—ã§æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¾ã™ã€‚

```python
import requests

payload = {
    "inputs": [
        {
            "name": "metadata-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
        },
        {
            "name": "metadata-str",
            "datatype": "BYTES",
            "shape": [11],
            "data": "hello world ğŸ˜",
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

<!--
As you should be able to see in the server logs, MLServer will cross-reference the input names against the model metadata to find the right content type.
-->

ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã€MLServer ã¯å…¥åŠ›åã¨ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã—ã¦ã€æ­£ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚

<!--
### Custom Codecs

There may be cases where a custom inference runtime may need to encode / decode to custom datatypes.
As an example, we can think of computer vision models which may only operate with `pillow` image objects.

In these scenarios, it's possible to extend the `Codec` interface to write our custom encoding logic.
A `Codec`, is simply an object which defines a `decode()` and `encode()` methods.
To illustrate how this would work, we will extend our custom runtime to add a custom `PillowCodec`.
-->

### ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯

ã‚«ã‚¹ã‚¿ãƒ æ¨è«–ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŒã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿å‹ã¸ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰/ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
ä¾‹ã¨ã—ã¦ã€`pillow` ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã§æ“ä½œã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã‚Œã‚‰ã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€`Codec` ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ‹¡å¼µã—ã¦ã‚«ã‚¹ã‚¿ãƒ ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
`Codec` ã¯ã€`decode()` ãƒ¡ã‚½ãƒƒãƒ‰ã¨ `encode()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®šç¾©ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
ã“ã‚ŒãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ã‚«ã‚¹ã‚¿ãƒ  `PillowCodec` ã‚’è¿½åŠ ã—ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’æ‹¡å¼µã—ã¾ã™ã€‚


```python
%%writefile runtime.py
import io
import json

from PIL import Image

from mlserver import MLModel
from mlserver.types import (
    InferenceRequest,
    InferenceResponse,
    RequestInput,
    ResponseOutput,
)
from mlserver.codecs import NumpyCodec, register_input_codec, DecodedParameterName
from mlserver.codecs.utils import InputOrOutput


_to_exclude = {
    "parameters": {DecodedParameterName},
    "inputs": {"__all__": {"parameters": {DecodedParameterName}}},
}


@register_input_codec
class PillowCodec(NumpyCodec):
    ContentType = "img"
    DefaultMode = "L"

    @classmethod
    def can_encode(cls, payload: Image) -> bool:
        return isinstance(payload, Image)

    @classmethod
    def _decode(cls, input_or_output: InputOrOutput) -> Image:
        if input_or_output.datatype != "BYTES":
            # If not bytes, assume it's an array
            image_array = super().decode_input(input_or_output)  # type: ignore
            return Image.fromarray(image_array, mode=cls.DefaultMode)

        encoded = input_or_output.data.__root__
        if isinstance(encoded, str):
            encoded = encoded.encode()

        return Image.frombytes(
            mode=cls.DefaultMode, size=input_or_output.shape, data=encoded
        )

    @classmethod
    def encode_output(cls, name: str, payload: Image) -> ResponseOutput:  # type: ignore
        byte_array = io.BytesIO()
        payload.save(byte_array, mode=cls.DefaultMode)

        return ResponseOutput(
            name=name, shape=payload.size, datatype="BYTES", data=byte_array.getvalue()
        )

    @classmethod
    def decode_output(cls, response_output: ResponseOutput) -> Image:
        return cls._decode(response_output)

    @classmethod
    def encode_input(cls, name: str, payload: Image) -> RequestInput:  # type: ignore
        output = cls.encode_output(name, payload)
        return RequestInput(
            name=output.name,
            shape=output.shape,
            datatype=output.datatype,
            data=output.data,
        )

    @classmethod
    def decode_input(cls, request_input: RequestInput) -> Image:
        return cls._decode(request_input)


class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        outputs = []
        for request_input in payload.inputs:
            decoded_input = self.decode(request_input)
            print(f"------ Encoded Input ({request_input.name}) ------")
            as_dict = request_input.dict(exclude=_to_exclude)  # type: ignore
            print(json.dumps(as_dict, indent=2))
            print(f"------ Decoded input ({request_input.name}) ------")
            print(decoded_input)

            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data,
                )
            )

        return InferenceResponse(model_name=self.name, outputs=outputs)
```

<!--
We should now be able to restart our instance of MLServer (i.e. with the `mlserver start .` command), to send a few test requests.
-->
ã“ã‚Œã§ã€MLServer ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†èµ·å‹•ï¼ˆã¤ã¾ã‚Š `mlserver start .` ã‚³ãƒãƒ³ãƒ‰ã§ï¼‰ã—ã€ã„ãã¤ã‹ã®ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚


```python
import requests

payload = {
    "inputs": [
        {
            "name": "image-int32",
            "datatype": "INT32",
            "shape": [8, 8],
            "data": [
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0
            ],
            "parameters": {
                "content_type": "img"
            }
        },
        {
            "name": "image-bytes",
            "datatype": "BYTES",
            "shape": [8, 8],
            "data": (
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
            ),
            "parameters": {
                "content_type": "img"
            }
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

<!--
As you should be able to see in the MLServer logs, the server is now able to decode the payload into a Pillow image.
This example also illustrates how `Codec` objects can be compatible with multiple `datatype` values (e.g. tensor and `BYTES` in this case).
-->

MLServer ã®ãƒ­ã‚°ã§ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã€ã‚µãƒ¼ãƒãƒ¼ã¯ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ Pillow ã‚¤ãƒ¡ãƒ¼ã‚¸ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
ã“ã®ä¾‹ã¯ã€`Codec` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¤‡æ•°ã® `datatype` å€¤ï¼ˆã“ã®å ´åˆã¯ãƒ†ãƒ³ã‚½ãƒ«ã¨ `BYTES`ï¼‰ã¨äº’æ›æ€§ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<!--
## Request Codecs

So far, we've seen how you can specify codecs so that they get applied at the input level.
However, it is also possible to use request-wide codecs that aggregate multiple inputs to decode the payload.
This is usually relevant for cases where the models expect a multi-column input type, like a Pandas DataFrame.

To illustrate this, we will first tweak our `EchoRuntime` so that it prints the decoded contents at the request level.
-->

## ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ãƒ¼ãƒ‡ãƒƒã‚¯

ã“ã‚Œã¾ã§ã«è¦‹ã¦ããŸã‚ˆã†ã«ã€å…¥åŠ›ãƒ¬ãƒ™ãƒ«ã§é©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã‚’æŒ‡å®šã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã—ãŸã€‚
ã—ã‹ã—ã€ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«è¤‡æ•°ã®å…¥åŠ›ã‚’é›†ç´„ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
ã“ã‚Œã¯é€šå¸¸ã€Pandas DataFrame ã®ã‚ˆã†ãªãƒãƒ«ãƒã‚«ãƒ©ãƒ å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’æœŸå¾…ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ã™ã‚‹ã‚±ãƒ¼ã‚¹ã§é‡è¦ã§ã™ã€‚

ã“ã‚Œã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€ã¾ãš `EchoRuntime` ã‚’èª¿æ•´ã—ã¦ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå†…å®¹ã‚’è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
%%writefile runtime.py
import json

from mlserver import MLModel
from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput
from mlserver.codecs import DecodedParameterName

_to_exclude = {
    "parameters": {DecodedParameterName},
    'inputs': {"__all__": {"parameters": {DecodedParameterName}}}
}

class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        print("------ Encoded Input (request) ------")
        as_dict = payload.dict(exclude=_to_exclude)  # type: ignore
        print(json.dumps(as_dict, indent=2))
        print("------ Decoded input (request) ------")
        decoded_request = None
        if payload.parameters:
            decoded_request = getattr(payload.parameters, DecodedParameterName)
        print(decoded_request)
            
        outputs = []
        for request_input in payload.inputs:
            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data
                )
            )
        
        return InferenceResponse(model_name=self.name, outputs=outputs)
        
```

<!--
We should now be able to restart our instance of MLServer (i.e. with the `mlserver start .` command), to send a few test requests.
-->
ã“ã‚Œã§ã€MLServer ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†èµ·å‹•ï¼ˆã¤ã¾ã‚Š `mlserver start .` ã‚³ãƒãƒ³ãƒ‰ã§ï¼‰ã—ã€ã„ãã¤ã‹ã®ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
import requests

payload = {
    "inputs": [
        {
            "name": "parameters-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "parameters-str",
            "datatype": "BYTES",
            "shape": [2, 11],
            "data": ["hello world ğŸ˜", "bye bye ğŸ˜"],
            "parameters": {
                "content_type": "str"
            }
        }
    ],
    "parameters": {
        "content_type": "pd"
    }
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```


```python

```
