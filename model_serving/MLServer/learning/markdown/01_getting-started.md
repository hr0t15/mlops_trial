
# MLServer ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

MLServerã«ã¤ã„ã¦ã€[è¨€èªãƒ¢ãƒ‡ãƒ«](https://spacy.io/models/en#en_core_web_lg) `spacy`ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€MLServerã®ä½¿ã„æ–¹ã‚’å­¦ã¶ã€‚

åŒã˜ç«¯æœ«ä¸Šã§ã®æ“ä½œã«ãªã‚‹ãŒã€ã‚µãƒ¼ãƒå´ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®2ã¤ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰æ“ä½œã™ã‚‹ã€‚


## 00 MLServerã¨ã¯ï¼Ÿ

MLServerã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®æœ¬ç•ªç’°å¢ƒå‘ã‘ã®éåŒæœŸAPIã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚


## 01 ç’°å¢ƒæ§‹ç¯‰

ã¾ãšã¯ç’°å¢ƒæ§‹ç¯‰ã‚’è¡Œã†ãŸã‚ã€ã‚µãƒ¼ãƒå´ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ç”¨ã„ã‚‹ã€‚

### Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æœ€åˆã«ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã«ã‚ãŸã£ã¦å¿…è¦ã¨ãªã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä»¥ä¸‹ã®é€šã‚Šå°å…¥ã™ã‚‹ã€‚

```
pip install mlserver spacy wikipedia-api
```

ãªãŠã€`requests`ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ä¾å­˜ã—ãŸã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã“ã¨ã‹ã‚‰ã€ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚‚åŒæ™‚ã«è¡Œã£ãŸã€‚

```
pip install -U requests
```

`mlserver`ã€`spacy`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ãŠã‚ˆã³ç§ãŸã¡ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¿…è¦ãª[è¨€èªãƒ¢ãƒ‡ãƒ«](https://spacy.io/models/en#en_core_web_lg) `spacy`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã§ã™ã€‚ã¾ãŸã€ã„ãã¤ã‹ã®æ¥½ã—ã„è¦ç´„ã§ç§ãŸã¡ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«`wikipedia-api`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

ã‚‚ã—[spaCy](https://spacy.io/)ã‚’èã„ãŸã“ã¨ãŒãªã‘ã‚Œã°ã€ãã‚Œã¯å¤§è¦æ¨¡ãªæƒ…å ±æŠ½å‡ºã‚„æ¤œç´¢ã‚¿ã‚¹ã‚¯ãªã©ã«å„ªã‚ŒãŸã€é«˜åº¦ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚’è¡Œã†ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ã“ã“ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¦ã‚§ãƒ–ä¸Šã®è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ç§ãŸã¡ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’ã‚¼ãƒ­ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã‚ˆã‚Šã‚‚è¿…é€Ÿã«å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚


### è¨€èªãƒ¢ãƒ‡ãƒ«ã®å–å¾—

ã¾ãŸã€ä»®æƒ³ç’°å¢ƒå†…ã«spaCyã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥é€”ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```
python -m spacy download en_core_web_lg
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ç¢ºèªã—ã¦ã„ã‚‹å ´åˆã€ä¸Šè¨˜ã®2ã¤ã®ã‚³ãƒãƒ³ãƒ‰ã®å‰ã«æ„Ÿå˜†ç¬¦`!`ã‚’å¿˜ã‚Œãšã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚VSCodeã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã®ã¾ã¾ã«ã—ã¦ã‚»ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’bashã«å¤‰æ›´ã§ãã¾ã™ã€‚


### ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã‚‚ã®ã®ç¢ºèª

2ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–“ã®é¡ä¼¼æ€§ã‚’æ¯”è¼ƒã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆã™ã‚‹å‰ã«ã€ç‰¹ã«äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€æœ€åˆã«ç§ãŸã¡ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ©Ÿèƒ½ã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ãŒè‰¯ã„ç¿’æ…£ã§ã™ã€‚

ã“ã“ã§ã¯Python REPLã‚ˆã‚Šç¢ºèªã™ã‚‹ã€‚

```
python
```

Python REPLãŒèµ·å‹•ã—ãŸã‚‰ã€ä»¥ä¸‹ã®é€šã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚’è¡Œã†ã€‚

```python
# Python REPL
import spacy
nlp = spacy.load("en_core_web_lg")
```

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã€`wikipedia-api` Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€[Barbieheimer](https://en.wikipedia.org/wiki/Barbenheimer)ã®è¦ç´„ã®é¡ä¼¼æ€§ã‚’èª¿ã¹ã¾ã™ã€‚

ã“ã®APIã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ä¸»è¦ãªè¦ä»¶ã¯ã€`Wikipedia()`ã‚¯ãƒ©ã‚¹ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€ãŠã‚ˆã³æƒ…å ±ã‚’å–å¾—ã—ãŸã„è¨€èªã‚’æ¸¡ã™ã“ã¨ã§ã™ã€‚

```python
# Python REPL
import wikipediaapi
wiki_wiki = wikipediaapi.Wikipedia('MyMovieEval (example@example.com)', 'en')
```

ãã®å¾Œã€`.page()`ãƒ¡ã‚½ãƒƒãƒ‰ã«æ˜ ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¸¡ã—ã¦ã€ãã®è¦ç´„ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

```python
# Python REPL
barbie = wiki_wiki.page('Barbie_(film)').summary
print(barbie)
```
```
Barbie is a 2023 American fantasy comedy film directed by Greta Gerwig and written by Gerwig and Noah Baumbach. Based on the Barbie fashion dolls by Mattel, it is the first live-action Barbie film after numerous computer-animated direct-to-video and streaming television films. The film stars Margot Robbie as Barbie and Ryan Gosling as Ken, and follows the two on a journey of self-discovery following an existential crisis. The film also features an ensemble cast that includes America Ferrera, Kate McKinnon, Issa Rae, Rhea Perlman, and Will Ferrell...
```

```python
# Python REPL
oppenheimer = wiki_wiki.page('Oppenheimer_(film)').summary
print(oppenheimer)
```
```
Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer's wife Katherine "Kitty" Oppenheimer; Matt Damon as General Leslie Groves, director of the Manhattan Project; and Robert Downey Jr. as Lewis Strauss, a senior member of the United States Atomic Energy Commission. The ensemble supporting cast includes Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, Gary Oldman and Kenneth Branagh...
```

ã“ã‚Œã§2ã¤ã®è¦ç´„ãŒæ‰‹ã«å…¥ã‚Šã¾ã—ãŸã®ã§ã€spacyã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚‰ã‚’æ¯”è¼ƒã—ã¾ã—ã‚‡ã†ã€‚

```python
# Python REPL
doc1 = nlp(barbie)
doc2 = nlp(oppenheimer)
doc1.similarity(doc2)
```

ä»¥ä¸‹ã®ã‚ˆã†ãªçµæœãŒå‡ºåŠ›ã•ã‚Œã¾ã™ï¼ˆç´°ã‹ã„å€¤ã¯çŠ¶æ³ã«ã‚ˆã‚Šç•°ãªã‚‹ç‚¹ã«æ³¨æ„ï¼‰

```
0.9866910567224084
```



<!--
Notice that both summaries have information about the other movie, about "films" in general, 
and about the dates each aired on (which is the same). The reality is that, the model hasn't seen 
any of these movies so it might be generalizing to the context of each article, "movies," 
rather than their content, "dolls as humans and the atomic bomb."

You should, of course, play around with different pages and see if what you get back is coherent with 
what you would expect.

Time to create a machine learning API for our use-case. ğŸ˜
-->

ä¸¡æ–¹ã®è¦ç´„ã«ã¯ã€ã‚‚ã†ä¸€ã¤ã®æ˜ ç”»ã«é–¢ã™ã‚‹æƒ…å ±ã€ä¸€èˆ¬çš„ãªã€Œæ˜ ç”»ã€ã«ã¤ã„ã¦ã®æƒ…å ±ã€ãã—ã¦ãã‚Œãã‚ŒãŒæ”¾æ˜ ã•ã‚ŒãŸæ—¥ä»˜ï¼ˆåŒã˜æ—¥ä»˜ã§ã™ï¼‰ã«ã¤ã„ã¦ã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ç¾å®Ÿã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ã“ã‚Œã‚‰ã®æ˜ ç”»ã‚’è¦‹ãŸã“ã¨ãŒãªã„ãŸã‚ã€å„è¨˜äº‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€Œæ˜ ç”»ã€ã«ä¸€èˆ¬åŒ–ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ãã®å†…å®¹"dolls as humans and the atomic bomb."ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

ã‚‚ã¡ã‚ã‚“ã€ç•°ãªã‚‹ãƒšãƒ¼ã‚¸ã§éŠã‚“ã§ã¿ã¦ã€è¿”ã£ã¦ãã‚‹çµæœãŒã‚ãªãŸã®æœŸå¾…ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

ç§ãŸã¡ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã®ãŸã‚ã®æ©Ÿæ¢°å­¦ç¿’APIã‚’ä½œæˆã™ã‚‹æ™‚ãŒæ¥ã¾ã—ãŸã€‚ğŸ˜

ã‚µãƒ¼ãƒå´ã®Python REPLã‚’æŠœã‘ã‚‹ã€‚

```python
# Python REPL
quit()
```


## 02 è¨­å®š

MLServerã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã¯ã€åŸºæœ¬çš„ã«3ç¨®é¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚

* `model-settings.json`: ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹æƒ…å ±ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
* `settings.json`: è¨­å®šã™ã‚‹ã‚µãƒ¼ãƒãƒ¼ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
* `.py`: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨äºˆæ¸¬ã®ãƒ¬ã‚·ãƒ”ã‚’è¨˜è¿°ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«

![setup](../assets/mlserver_setup.png)


ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã™ã‚‹ã€‚

```bash
mkdir -p similarity_model
```

<!--
## 03 Building a Service

MLServer allows us to wrap machine learning models into APIs and build microservices with 
replicas of a single model, or different models all together.

To create a service with MLServer, we will define a class with two asynchronous functions, one that 
loads the model and another one to run inference (or predict) with. The former will load the 
`spacy` model we tested in the last section, and the latter will take in a list with the two 
documents we want to compare. Lastly, our function will return a `numpy` array with a single 
value, our similarity score. We'll write the file to our `similarity_model` directory and call 
it `my_model.py`. 
-->

## 03 ã‚µãƒ¼ãƒ“ã‚¹ã®æ§‹ç¯‰

### ã‚µãƒ¼ãƒå´ã®è¨­å®š

MLServerã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’APIã«ãƒ©ãƒƒãƒ—ã—ã¦ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ§‹ç¯‰ã—ã€å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ãƒ—ãƒªã‚«ã‚„ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

MLServerã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éåŒæœŸé–¢æ•°ã¨æ¨è«–ï¼ˆã¾ãŸã¯äºˆæ¸¬ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã‚‚ã†ä¸€ã¤ã®éåŒæœŸé–¢æ•°ã‚’æŒã¤ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã—ã¾ã™ã€‚å‰è€…ã¯å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ãƒ†ã‚¹ãƒˆã—ãŸ`spacy`ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å¾Œè€…ã¯æ¯”è¼ƒã—ãŸã„2ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã¾ã™ã€‚æœ€å¾Œã«ã€ã“ã®é–¢æ•°ã¯é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦å˜ä¸€ã®å€¤ã‚’æŒã¤`numpy`é…åˆ—ã‚’è¿”ã—ã¾ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’`similarity_model`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã¿ã€`my_model.py`ã¨å‘¼ã³ã¾ã™ã€‚


```python
# similarity_model/my_model.py

from mlserver.codecs import decode_args
from mlserver import MLModel
from typing import List
import numpy as np
import spacy

class MyKulModel(MLModel):

    async def load(self):
        self.model = spacy.load("en_core_web_lg")
    
    @decode_args
    async def predict(self, docs: List[str]) -> np.ndarray:

        doc1 = self.model(docs[0])
        doc2 = self.model(docs[1])

        return np.array(doc1.similarity(doc2))
```

<!--
Now that we have our model file ready to go, the last piece of the puzzle is to tell MLServer a bit of info 
about it. In particular, it wants (or needs) to know the name of the model and how to implement 
it. The former can be anything you want (and it will be part of the URL of your API), and the latter will 
follow the recipe of `name_of_py_file_with_your_model.class_with_your_model`.

Let's create the `model-settings.json` file MLServer is expecting inside our `similarity_model` directory 
and add the name and the implementation of our model to it.
-->

ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ãŒæ•´ã£ãŸã®ã§ã€æ®‹ã‚Šã®ãƒ”ãƒ¼ã‚¹ã¯MLServerã«ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦å°‘ã—æƒ…å ±ã‚’æ•™ãˆã‚‹ã“ã¨ã§ã™ã€‚ç‰¹ã«ã€ãƒ¢ãƒ‡ãƒ«ã®åå‰ã¨å®Ÿè£…æ–¹æ³•ã‚’çŸ¥ã‚ŠãŸãŒã£ã¦ã„ã¾ã™ã€‚å‰è€…ã¯ã‚ãªãŸãŒæœ›ã‚€ã‚‚ã®ã§ä½•ã§ã‚‚ã‚ˆãï¼ˆAPIã®URLã®ä¸€éƒ¨ã«ãªã‚Šã¾ã™ï¼‰ã€å¾Œè€…ã¯`name_of_py_file_with_your_model.class_with_your_model`ã®ãƒ¬ã‚·ãƒ”ã«å¾“ã„ã¾ã™ã€‚

`similarity_model`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«MLServerãŒæœŸå¾…ã™ã‚‹`model-settings.json`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ãã“ã«ãƒ¢ãƒ‡ãƒ«ã®åå‰ã¨å®Ÿè£…ã‚’è¿½åŠ ã—ã¾ã—ã‚‡ã†ã€‚

```json
# similarity_model/model-settings.json

{
    "name": "doc-sim-model",
    "implementation": "my_model.MyKulModel"
}
```

<!--
Now that everything is in place, we can start serving predictions locally to test how things would play 
out for our future users. We'll initiate our server via the command line, and later on we'll see how to 
do the same via Python files. Here's where we are at right now in the process of developing microservices 
with MLServer.

![startassets/start_service.png)

As you can see in the image, our server will be initialized with three entry points, one for HTTP requests, 
another for gRPC, and another for the metrics. To learn more about the powerful metrics feature of MLServer, 
please visit the relevant docs page [here](https://mlserver.readthedocs.io/en/latest/user-guide/metrics.html). 
To learn more about gRPC, please see this tutorial [here](https://realpython.com/python-microservices-grpc/).

To start our service, open up a terminal and run the following command.
-->

å…¨ã¦ãŒæ•´ã„ã¾ã—ãŸã®ã§ã€å°†æ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã©ã®ã‚ˆã†ãªçµæœãŒå‡ºã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§äºˆæ¸¬ã‚’æä¾›ã—å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚µãƒ¼ãƒãƒ¼ã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’é€šã˜ã¦åˆæœŸåŒ–ã•ã‚Œã€å¾Œã«Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦åŒæ§˜ã®ã“ã¨ã‚’è¡Œã†æ–¹æ³•ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚ç¾åœ¨ã€MLServerã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã®é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ã¯ä»¥ä¸‹ã®ã¨ã“ã‚ã¾ã§é€²ã‚“ã§ã„ã¾ã™ã€‚

![start](../assets/start_service.png)

ç”»åƒã§è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã€ç§ãŸã¡ã®ã‚µãƒ¼ãƒãƒ¼ã¯HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã€gRPCç”¨ã€ãŠã‚ˆã³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨ã®3ã¤ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚MLServerã®å¼·åŠ›ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ©Ÿèƒ½ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸[ã“ã¡ã‚‰](https://mlserver.readthedocs.io/en/latest/user-guide/metrics.html)ã‚’ã”è¦§ãã ã•ã„ã€‚gRPCã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚‹ã«ã¯ã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«[ã“ã¡ã‚‰](https://realpython.com/python-microservices-grpc/)ã‚’ã”è¦§ãã ã•ã„ã€‚

ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ã„ã¦æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚


```bash
mlserver start similarity_model/
```

<!--
Note: If this is a fresh terminal, make sure you activate your environment before you run the command above. 
If you run the command above from your notebook (e.g. `!mlserver start similarity_model/`), 
you will have to send the request below from another notebook or terminal since the cell will continue to run 
until you turn it off.
-->

æ³¨æ„ï¼šæ–°ã—ã„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ã—ã¦ãã ã•ã„ã€‚
ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰ä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆï¼ˆä¾‹ï¼š`!mlserver start similarity_model/`ï¼‰ã€ã‚»ãƒ«ãŒã‚ªãƒ•ã«ãªã‚‹ã¾ã§å®Ÿè¡ŒãŒç¶šããŸã‚ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯åˆ¥ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¾ãŸã¯ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰é€ä¿¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

<!--
## 04 Testing our Service

Time to become a client of our service and test it. For this, we'll set up the payload we'll send 
to our service and use the `requests` library to [POST](https://www.baeldung.com/cs/http-get-vs-post) our request.
-->


### ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ

ç§ãŸã¡ã®ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãªã‚Šã€ãƒ†ã‚¹ãƒˆã‚’è¡Œã†æ™‚ãŒæ¥ã¾ã—ãŸã€‚

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚ˆã‚Šã€Python REPLãŒèµ·å‹•ã‚’è¡Œã†ã€‚

```bash
python
```

ã•ãã»ã©ã‚µãƒ¼ãƒã§Barbieã¨Oppenheimerã®è¦ç´„ã‚’ä½œæˆã—ãŸãŒã€ã“ã“ã§ã‚‚ç”¨ã„ã‚‹ã“ã¨ã‹ã‚‰ã€ãã‚Œã‚’ä½œæˆã™ã‚‹ã€‚

```python
# Python REPL
import wikipediaapi
wiki_wiki = wikipediaapi.Wikipedia('MyMovieEval (example@example.com)', 'en')
barbie = wiki_wiki.page('Barbie_(film)').summary
oppenheimer = wiki_wiki.page('Oppenheimer_(film)').summary
```

ã‚µãƒ¼ãƒ“ã‚¹ã«é€ã‚‹ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã€`requests`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’[POST](https://www.baeldung.com/cs/http-get-vs-post)ã—ã¾ã™ã€‚


```python
# Python REPL
from mlserver.codecs import StringCodec
import requests
```

ä»¥ä¸‹ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€ä»¥å‰ã«ä½œæˆã—ãŸBarbieã¨Oppenheimerã®è¦ç´„ã§ä½¿ç”¨ã—ãŸå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã“ã®POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹å ´åˆã¯ã€ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰è¡Œã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
# Python REPL
inference_request = {
    "inputs": [
        StringCodec.encode_input(name='docs', payload=[barbie, oppenheimer], use_bytes=False).dict()
    ]
}
print(inference_request)
```

```
{'inputs': [{'name': 'docs', 'shape': [2, 1], 'datatype': 'BYTES', 'parameters': {'content_type': 'str'}, 'data': ['Barbie is a 2023 fantasy comedy film directed by Greta Gerwig from a screenplay she wrote with Noah Baumbach. Based on the eponymous fashion dolls by Mattel, it is the first live-action Barbie film after numerous animated films and specials. It stars Margot Robbie as the title character and Ryan Gosling as Ken, and follows them on a journey of self-discovery through both Barbieland and the real world following an existential crisis. It is also a commentary regarding patriarchy and the effects of feminism. The supporting cast includes America Ferrera, Michael Cera, Kate McKinnon, Issa Rae, Rhea Perlman, and Will Ferrell.\nA live-action Barbie film was announced in September 2009 by Universal Pictures with Laurence Mark producing. Development began in April 2014, when Sony Pictures acquired the film rights. Following multiple writer and director changes and the casting of Amy Schumer and later Anne Hathaway as Barbie, the rights were transferred to Warner Bros. Pictures in October 2018. Robbie was cast in 2019, after Gal Gadot turned down the role due to scheduling conflicts, and Gerwig was announced as director and co-writer with Baumbach in 2020. The rest of the cast was announced in early 2022. Principal photography occurred primarily at Warner Bros. Studios, Leavesden, England, and at the Venice Beach Skatepark in Los Angeles from March to July 2022.\n\nBarbie premiered at the Shrine Auditorium in Los Angeles on July 9, 2023, and was released in the United States on July 21. Its concurrent release with Universal Pictures\' Oppenheimer was the catalyst of the "Barbenheimer" phenomenon, encouraging audiences to see both films as a double feature. The film grossed $1.446 billion and achieved several milestones, including the highest-grossing film of 2023 and the 14th highest-grossing film of all time. Named one of the top 10 films of 2023 by the National Board of Review and the American Film Institute, it received critical acclaim and other accolades, including eight Academy Award nominations (among them Best Picture), winning Best Original Song for "What Was I Made For?"; the song also won Golden Globe Award for Best Original Song while Barbie was awarded Golden Globe Award for Cinematic and Box Office Achievement.', 'Oppenheimer is a 2023 epic biographical thriller drama film written, directed, and produced by Christopher Nolan. It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles Oppenheimer\'s studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Cillian Murphy stars as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek and Kenneth Branagh.\nOppenheimer was announced in September 2021. It is Nolan\'s first film not distributed by Warner Bros. Pictures since Memento (2000), due to his conflicts regarding the studio\'s simultaneous theatrical and HBO Max release schedule. Murphy was the first cast member to sign on the following month, with the rest joining between November 2021 and April 2022. Pre-production began by January 2022, and filming took place from February to May. The cinematographer, Hoyte van Hoytema, used a combination of IMAX 65 mm and 65 mm large-format film, including, for the first time, scenes in IMAX black-and-white film photography. As with many of his previous films, Nolan used extensive practical effects, with minimal compositing.\nOppenheimer premiered at Le Grand Rex in Paris on July 11, 2023, and was theatrically released in the US and the UK ten days later by Universal. Its concurrent release with Warner Bros.\'s Barbie was the catalyst of the "Barbenheimer" phenomenon, encouraging audiences to see both films as a double feature. Oppenheimer grossed over $974 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film.\nAmong its many accolades, Oppenheimer won seven Academy Awards, including Best Picture, Best Director, Best Actor for Murphy and Best Supporting Actor for Downey. It also won five Golden Globe Awards (including Best Motion Picture â€“ Drama) and seven British Academy Film Awards (including Best Film), and was named one of the top ten films of 2023 by the National Board of Review and the American Film Institute.']}]}
```

```python
# Python REPL
r = requests.post('http://0.0.0.0:8080/v2/models/doc-sim-model/infer', json=inference_request)
r.json()
```
```
{'model_name': 'doc-sim-model', 'id': '338c44a8-7044-4912-9f25-b6c1fb241250', 'parameters': {}, 'outputs': [{'name': 'output-0', 'shape': [1], 'datatype': 'FP64', 'parameters': {'content_type': 'np'}, 'data': [0.9844775471634595]}]}
```

```python
# Python REPL
print(f"Our movies are {round(r.json()['outputs'][0]['data'][0] * 100, 4)}% similar!")
```
```
Our movies are 98.4478% similar!
```


<!--
Let's decompose what just happened.

The `URL` for our service might seem a bit odd if you've never heard of the 
[V2/Open Inference Protocol (OIP)](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html). 
This protocol is a set of specifications that allows machine learning models to be shared and deployed in a 
standardized way. This protocol enables the use of machine learning models on a variety of platforms and 
devices without requiring changes to the model or its code. The OIP is useful because it allows us
to integrate machine learning into a wide range of applications in a standard way.

All URLs you create with MLServer will have the following structure.

![v2assets/urlv2.png)

This kind of protocol is a standard adopted by different companies like NVIDIA, Tensorflow Serving, 
KServe, and others, to keep everyone on the same page. If you think about driving cars globally, 
your country has to apply a standard for driving on a particular side of the road, and this ensures 
you and everyone else stays on the left (or the right depending on where you are at). Adopting this 
means that you won't have to wonder where the next driver is going to come out of when you are driving 
and are about to take a turn, instead, you can focus on getting to where you're going to without much worrying.

Let's describe what each of the components of our `inference_request` does.
- `name`: this maps one-to-one to the name of the parameter in your `predict()` function.
- `shape`: represents the shape of the elements in our `data`. In our case, it is a list with `[2]` strings.
- `datatype`: the different data types expected by the server, e.g., str, numpy array, pandas dataframe, bytes, etc.
- `parameters`: allows us to specify the `content_type` beyond the data types 
- `data`: the inputs to our predict function.

To learn more about the OIP and how MLServer content types work, please have a looks at their 
[docs page here](https://mlserver.readthedocs.io/en/latest/user-guide/content-type.html).
-->
èµ·ã“ã£ãŸã“ã¨ã‚’è©³ã—ãè§£æã—ã¾ã—ã‚‡ã†ã€‚

ç§ãŸã¡ã®ã‚µãƒ¼ãƒ“ã‚¹ã®`URL`ã¯ã€[V2/Open Inference Protocol (OIP)](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html)ã«ã¤ã„ã¦èã„ãŸã“ã¨ãŒãªã‘ã‚Œã°å°‘ã—å¥‡å¦™ã«è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ¨™æº–åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§å…±æœ‰ãŠã‚ˆã³ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ä»•æ§˜ã®ã‚»ãƒƒãƒˆã§ã™ã€‚ã“ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚„ãã®ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãªãã€ã•ã¾ã–ã¾ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚„ãƒ‡ãƒã‚¤ã‚¹ã§æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚OIPã¯æ¨™æº–çš„ãªæ–¹æ³•ã§å¤šå²ã«ã‚ãŸã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«æ©Ÿæ¢°å­¦ç¿’ã‚’çµ±åˆã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

MLServerã§ä½œæˆã™ã‚‹ã™ã¹ã¦ã®URLã¯ã€æ¬¡ã®æ§‹é€ ã‚’æŒã¡ã¾ã™ã€‚

![v2](../assets/urlv2.png)

ã“ã®ç¨®ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¯ã€NVIDIAã€Tensorflow Servingã€KServeãªã©ã®ç•°ãªã‚‹ä¼æ¥­ã«ã‚ˆã£ã¦æ¡ç”¨ã•ã‚Œã¦ãŠã‚Šã€ã™ã¹ã¦ã®äººãŒåŒã˜åŸºæº–ã«æ²¿ã£ã¦å‹•ãã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚è»Šã®é‹è»¢ã«ã¤ã„ã¦è€ƒãˆã‚‹ã¨ã€ã‚ãªãŸã®å›½ã¯ç‰¹å®šã®é“è·¯ã®å´ã‚’èµ°è¡Œã™ã‚‹ãŸã‚ã®æ¨™æº–ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šã‚ãªãŸã¨ä»–ã®ã™ã¹ã¦ã®äººãŒå·¦ï¼ˆã¾ãŸã¯ã‚ãªãŸãŒã„ã‚‹å ´æ‰€ã«ã‚ˆã£ã¦ã¯å³ï¼‰ã«ç•™ã¾ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚ã“ã‚Œã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã§ã€é‹è»¢ä¸­ã«æ¬¡ã«ã©ã®ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒå‡ºã¦ãã‚‹ã‹ã‚’å¿ƒé…ã™ã‚‹ã“ã¨ãªãã€ç›®çš„åœ°ã«å‘ã‹ã†ã“ã¨ã«é›†ä¸­ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ç§ãŸã¡ã®`inference_request`ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒä½•ã‚’ã™ã‚‹ã‹ã‚’èª¬æ˜ã—ã¾ã—ã‚‡ã†ã€‚
- `name`: ã“ã‚Œã¯ã‚ãªãŸã®`predict()`é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã¨ä¸€å¯¾ä¸€ã§å¯¾å¿œã—ã¾ã™ã€‚
- `shape`: ç§ãŸã¡ã®`data`ã®è¦ç´ ã®å½¢ã‚’è¡¨ã—ã¾ã™ã€‚ç§ãŸã¡ã®å ´åˆã€ã“ã‚Œã¯`[2]`å€‹ã®æ–‡å­—åˆ—ã‚’æŒã¤ãƒªã‚¹ãƒˆã§ã™ã€‚
- `datatype`: ã‚µãƒ¼ãƒãƒ¼ãŒæœŸå¾…ã™ã‚‹ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è¡¨ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€strã€numpyé…åˆ—ã€pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ãƒã‚¤ãƒˆãªã©ã€‚
- `parameters`: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è¶…ãˆã¦`content_type`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- `data`: ç§ãŸã¡ã®äºˆæ¸¬é–¢æ•°ã¸ã®å…¥åŠ›ã§ã™ã€‚

OIPã¨MLServerã®ã‚³ãƒ³ãƒ†ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®å‹•ä½œã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚‹ã«ã¯ã€[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸](https://mlserver.readthedocs.io/en/latest/user-guide/content-type.html)ã‚’ã”è¦§ãã ã•ã„ã€‚



<!--
## 05 Creating Model Replicas

Say you need to meet the demand of a high number of users and one model might not be enough, or is not using 
all of the resources of the virtual machine instance it was allocated to. What we can do in this case is 
to create multiple replicas of our model to increase the throughput of the requests that come in. This 
can be particularly useful at the peak times of our server. To do this, we need to tweak the settings of 
our server via the `settings.json` file. In it, we'll add the number of independent models we want to 
have to the parameter `"parallel_workers": 3`.

Let's stop our server, change the settings of it, start it again, and test it.
-->
## 05 ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ—ãƒªã‚«ã®ä½œæˆ

å¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éœ€è¦ã«å¿œãˆã‚‹ãŸã‚ã«ã€1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§ã¯ä¸ååˆ†ã§ã‚ã‚‹å ´åˆã‚„ã€å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸä»®æƒ³ãƒã‚·ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ã™ã¹ã¦ä½¿ç”¨ã—ã¦ã„ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã®å ´åˆã«ã§ãã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¤‡æ•°ã®ãƒ¬ãƒ—ãƒªã‚«ã‚’ä½œæˆã—ã¦ã€å…¥ã£ã¦ãã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å¢—åŠ ã•ã›ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ã‚µãƒ¼ãƒãƒ¼ã®ãƒ”ãƒ¼ã‚¯æ™‚ã«ç‰¹ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã‚Œã‚’è¡Œã†ãŸã‚ã«ã¯ã€`settings.json`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä»‹ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã€ç‹¬ç«‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ•°ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿`"parallel_workers": 3`ã«è¿½åŠ ã—ã¾ã™ã€‚

ã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã—ã€ãã®è¨­å®šã‚’å¤‰æ›´ã—ã¦å†ã³èµ·å‹•ã—ã€ãƒ†ã‚¹ãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚


```json
# similarity_model/settings.json

{
    "parallel_workers": 3
}
```


```bash
mlserver start similarity_model
```

<!--
![multiplemodelsassets/multiple_models.png)

As you can see in the output of the terminal in the picture above, we now have 3 models running in 
parallel. The reason you might see 4 is because, by default, MLServer will print the name of the 
initialized model if it is one or more, and it will also print one for each of the replicas 
specified in the settings.

Let's get a few more [twin films examples](https://en.wikipedia.org/wiki/Twin_films) to test our 
server. Get as creative as you'd like. ğŸ’¡
-->
![multiplemodels](../assets/multiple_models.png)

ä¸Šã®ç”»åƒã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®å‡ºåŠ›ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã€ç¾åœ¨3ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒä¸¦è¡Œã—ã¦å‹•ä½œã—ã¦ã„ã¾ã™ã€‚4ã¤è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã“ã‚Œã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§MLServerãŒ1ã¤ä»¥ä¸Šã®åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’å‡ºåŠ›ã—ã€è¨­å®šã§æŒ‡å®šã•ã‚ŒãŸå„ãƒ¬ãƒ—ãƒªã‚«ã«ã¤ã„ã¦ã‚‚å‡ºåŠ›ã™ã‚‹ãŸã‚ã§ã™ã€‚

ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ã€ã‚‚ã£ã¨å¤šãã®[ãƒ„ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ãƒ ã®ä¾‹](https://en.wikipedia.org/wiki/Twin_films)ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚å¥½ããªã ã‘å‰µé€ çš„ã«ãªã£ã¦ãã ã•ã„ã€‚ğŸ’¡


```python
# Python REPL
deep_impact    = wiki_wiki.page('Deep_Impact_(film)').summary
armageddon     = wiki_wiki.page('Armageddon_(1998_film)').summary

antz           = wiki_wiki.page('Antz').summary
a_bugs_life    = wiki_wiki.page("A_Bug's_Life").summary

the_dark_night = wiki_wiki.page('The_Dark_Knight').summary
mamma_mia      = wiki_wiki.page('Mamma_Mia!_(film)').summary
```


```python
# Python REPL
def get_sim_score(movie1, movie2):
    response = requests.post(
        'http://0.0.0.0:8080/v2/models/doc-sim-model/infer', 
        json={
            "inputs": [
                StringCodec.encode_input(name='docs', payload=[movie1, movie2], use_bytes=False).dict()
            ]
        })
    return response.json()['outputs'][0]['data'][0]
```

<!--
Let's first test that the function works as intended.
-->
ã¾ãšã¯ã€é–¢æ•°ãŒæ„å›³ã—ãŸã¨ãŠã‚Šã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã—ã‚‡ã†ã€‚

```python
# Python REPL
get_sim_score(deep_impact, armageddon)
```
```
0.9569279450151813
```

<!--
Now let's map three POST requests at the same time.
-->
æ¬¡ã«ã€åŒæ™‚ã«3ã¤ã®POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã—ã‚‡ã†ã€‚

```python
# Python REPL
results = list(
    map(get_sim_score, (deep_impact, antz, the_dark_night), (armageddon, a_bugs_life, mamma_mia))
)
results
```
```
[0.9569279450151813, 0.9725374771538605, 0.9626173937217876]
```


<!--
We can also test it one by one.
-->
ã¾ãŸã€ä¸€ã¤ãšã¤ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚


```python
# Python REPL
for movie1, movie2 in zip((deep_impact, antz, the_dark_night), (armageddon, a_bugs_life, mamma_mia)):
    print(get_sim_score(movie1, movie2))
```
```
0.9569279450151813
0.9725374771538605
0.9626173937217876
```


<!--
## 06 Packaging our Service

![serving3assets/serving_2.png)

For the last step of this guide, we are going to package our model and service into a 
docker image that we can reuse in another project or share with colleagues immediately. This step 
requires that we have docker installed and configured in our PCs, so if you need to set up docker, 
you can do so by following the instructions in the documentation [here](https://docs.docker.com/get-docker/).

The first step is to create a `requirements.txt` file with all of our dependencies and add it to 
the directory we've been using for our service (`similarity_model`).
-->


## 06 ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åŒ–

![serving3](../assets/serving_2.png)

ã“ã®ã‚¬ã‚¤ãƒ‰ã®æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åŒ–ã—ã€åˆ¥ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å†åˆ©ç”¨ã—ãŸã‚Šã€ã™ãã«åŒåƒšã¨å…±æœ‰ã—ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€PCã«DockerãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€Dockerã®è¨­å®šãŒå¿…è¦ãªå ´åˆã¯ã€[ã“ã“](https://docs.docker.com/get-docker/)ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æŒ‡ç¤ºã«å¾“ã£ã¦è¨­å®šã§ãã¾ã™ã€‚

æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ã™ã¹ã¦ã®ä¾å­˜é–¢ä¿‚ã‚’å«ã‚€`requirements.txt`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ã‚µãƒ¼ãƒ“ã‚¹ã«ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ`similarity_model`ï¼‰ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã™ã€‚


```
# similarity_model/requirements.txt

mlserver
spacy==3.6.0
https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl
```

<!--
The next step is to build a docker image with our model, its dependencies and our server. If you've never heard 
of **docker images** before, here's a short description.

> A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including code, libraries, dependencies, and settings. It's like a carry-on bag for your application, containing everything it needs to travel safely and run smoothly in different environments. Just as a carry-on bag allows you to bring your essentials with you on a trip, a Docker image enables you to transport your application and its requirements across various computing environments, ensuring consistent and reliable deployment.

MLServer has a convenient function that lets us create docker images with our services. Let's use it.
-->
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãƒ¢ãƒ‡ãƒ«ã€ãã®ä¾å­˜é–¢ä¿‚ã€ãŠã‚ˆã³ã‚µãƒ¼ãƒãƒ¼ã‚’å«ã‚€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã™ã€‚ã‚‚ã—**Dockerã‚¤ãƒ¡ãƒ¼ã‚¸**ã«ã¤ã„ã¦èã„ãŸã“ã¨ãŒãªã‘ã‚Œã°ã€ã“ã¡ã‚‰ãŒç°¡å˜ãªèª¬æ˜ã§ã™ã€‚

> Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã¯ã€è»½é‡ã§ç‹¬ç«‹ã—ã¦ãŠã‚Šã€å®Ÿè¡Œå¯èƒ½ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€ã‚³ãƒ¼ãƒ‰ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ä¾å­˜é–¢ä¿‚ã€è¨­å®šãŒå«ã¾ã‚Œã¾ã™ã€‚ãã‚Œã¯ã‚ãªãŸã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æ©Ÿå†…æŒã¡è¾¼ã¿è·ç‰©ã®ã‚ˆã†ãªã‚‚ã®ã§ã€ç•°ãªã‚‹ç’°å¢ƒã§å®‰å…¨ã«ç§»å‹•ã—ã€ã‚¹ãƒ ãƒ¼ã‚ºã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚æ©Ÿå†…æŒã¡è¾¼ã¿è·ç‰©ãŒæ—…è¡Œã«å¿…è¦ãªã‚‚ã®ã‚’æŒã£ã¦ã„ãã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†ã«ã€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã¯ã‚ãªãŸã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãã®è¦ä»¶ã‚’ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç’°å¢ƒã«è¼¸é€ã—ã€ä¸€è²«ã—ãŸä¿¡é ¼æ€§ã®ã‚ã‚‹ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ä¿è¨¼ã—ã¾ã™ã€‚

MLServerã«ã¯ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚’å«ã‚€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆã™ã‚‹ä¾¿åˆ©ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã‚’ä½¿ç”¨ã—ã¾ã—ã‚‡ã†ã€‚

```python
mlserver build similarity_model/ -t 'fancy_ml_service'
```

<!--
We can check that our image was successfully build not only by looking at the logs of the previous 
command but also with the `docker images` command.
-->
å‰ã®ã‚³ãƒãƒ³ãƒ‰ã®ãƒ­ã‚°ã‚’è¦‹ã‚‹ã ã‘ã§ãªãã€`docker images`ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ã€ç§ãŸã¡ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ­£å¸¸ã«æ§‹ç¯‰ã•ã‚ŒãŸã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```bash
docker images
```

<!--
Let's test that our image works as intended with the following command. Make sure you have closed your 
previous server by using `CTRL + C` in your terminal.
-->
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ç§ãŸã¡ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ„å›³ã—ãŸé€šã‚Šã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã—ã‚‡ã†ã€‚ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§`CTRL + C`ã‚’ä½¿ç”¨ã—ã¦ã€å‰ã®ã‚µãƒ¼ãƒãƒ¼ã‚’é–‰ã˜ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


```bash
docker run -it --rm -p 8080:8080 fancy_ml_service
```

<!--
Now that you have a packaged and fully-functioning microservice with our model, we could deploy our container 
to a production serving platform like [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/#), 
or via different offerings available through the many cloud providers out there (e.g. AWS Lambda, Google 
Cloud Run, etc.). You could also run this image on KServe, a Kubernetes native tool for model serving, or 
anywhere else where you can bring your docker image with you.

To learn more about MLServer and the different ways in which you can use it, head over to the 
[examples](https://mlserver.readthedocs.io/en/latest/examples/index.html) section 
or the [user guide](https://mlserver.readthedocs.io/en/latest/user-guide/index.html). To learn about 
some of the deployment options available, head over to the docs [here](https://mlserver.readthedocs.io/en/stable/user-guide/deployment/index.html).

To keep up to date with what we are up to at Seldon, make sure you join our 
[Slack community](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg).
-->
ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åŒ–ã•ã‚ŒãŸå®Œå…¨ã«æ©Ÿèƒ½ã™ã‚‹ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ãŒç”¨æ„ã•ã‚Œã¾ã—ãŸã®ã§ã€[Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/#)ã®ã‚ˆã†ãªæœ¬ç•ªç’°å¢ƒç”¨ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«ã‚³ãƒ³ãƒ†ãƒŠã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã¯ã€AWS Lambdaã€Google Cloud Runãªã©ã€å¤šãã®ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæä¾›ã™ã‚‹ã•ã¾ã–ã¾ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’KServeï¼ˆãƒ¢ãƒ‡ãƒ«æä¾›ã®ãŸã‚ã®Kubernetesãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ„ãƒ¼ãƒ«ï¼‰ã‚„ã€ãƒ‰ãƒƒã‚«ãƒ¼ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æŒã¡è¾¼ã‚ã‚‹ä»–ã®å ´æ‰€ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

MLServerãŠã‚ˆã³ãã®ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã•ã‚‰ã«å­¦ã¶ã«ã¯ã€[examples](https://mlserver.readthedocs.io/en/latest/examples/index.html)ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚„[user guide](https://mlserver.readthedocs.io/en/latest/user-guide/index.html)ã‚’ã”è¦§ãã ã•ã„ã€‚åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦å­¦ã¶ã«ã¯ã€[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://mlserver.readthedocs.io/en/stable/user-guide/deployment/index.html)ã‚’ã”è¦§ãã ã•ã„ã€‚

Seldonã§è¡Œã£ã¦ã„ã‚‹ã“ã¨ã«æœ€æ–°ã®æƒ…å ±ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€ç§ãŸã¡ã®[Slack community](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg)ã«ãœã²å‚åŠ ã—ã¦ãã ã•ã„ã€‚

