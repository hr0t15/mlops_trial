<!--
# Logging our first runs with MLflow

* [https://mlflow.org/docs/latest/getting-started/logging-first-model/step6-logging-a-run.html](https://mlflow.org/docs/latest/getting-started/logging-first-model/step6-logging-a-run.html)

In our previous segments, we worked through setting up our first MLflow Experiment and equipped it with custom tags. These tags, as we’ll soon discover, are instrumental in seamlessly retrieving related experiments that belong to a broader project.

In the last section, we created a dataset that we’ll be using to train a series of models.

As we advance in this section, we’ll delve deeper into the core features of MLflow Tracking:

* Making use of the `start_run` context for creating and efficiently managing runs.
* An introduction to logging, covering tags, parameters, and metrics.
* Understanding the role and formation of a model signature.
* Logging a trained model, solidifying its presence in our MLflow run.

But first, a foundational step awaits us. For our upcoming tasks, we need a dataset, specifically focused on apple sales. While it’s tempting to scour the internet for one, crafting our own dataset will ensure it aligns perfectly with our objectives.
-->

# MLflowで最初の実行をログする

* [https://mlflow.org/docs/latest/getting-started/logging-first-model/step6-logging-a-run.html](https://mlflow.org/docs/latest/getting-started/logging-first-model/step6-logging-a-run.html)

これまでのセグメントで、私たちは最初のMLflow実験を設定し、カスタムタグを装備しました。これらのタグは、広範なプロジェクトに属する関連する実験をシームレスに取得するのに役立つことがすぐにわかるでしょう。

前のセクションでは、一連のモデルを学習するために使用するデータセットを作成しました。

このセクションでは、MLflow Tracking のコア機能にさらに深く潜ります：

* 実行を作成し効率的に管理するための`start_run`コンテキストの使用。
* タグ、パラメータ、メトリックのログについての紹介。
* モデルシグネチャの役割と形成についての理解。
* 学習されたモデルのログ、私たちのMLflow実行におけるその存在の固定化。

しかし、まず、基本的なステップが待っています。これからのタスクには、特にリンゴの販売に焦点を当てたデータセットが必要です。インターネットで探すのも魅力的ですが、自分たちでデータセットを作成することで、完璧に私たちの目的に合致するようにすることができます。


<!--
## Crafting the Apple Sales Dataset

Let’s roll up our sleeves and construct this dataset.

We need a data set that defines the dynamics of apple sales influenced by various factors like weekends, promotions, and fluctuating prices. This dataset will serve as the bedrock upon which our predictive models will be built and tested.

Before we get to that, though, let’s take a look at what we’ve learned so far and how these principles were used when crafting this data set for the purposes of this tutorial.
-->

## リンゴの販売データセットの作成

袖をまくり上げて、このデータセットを構築しましょう。

私たちには、週末、プロモーション、変動する価格などのさまざまな要因の影響を受けるリンゴの販売のダイナミクスを定義するデータセットが必要です。このデータセットは、予測モデルが構築され、テストされる基盤として機能します。

それに取り掛かる前に、これまでに学んだことと、このチュートリアルのためにこのデータセットを作成する際に使用された原則を見てみましょう。


<!--
### Using Experiments in early-stage project development

As the diagram below shows, I tried taking a series of shortcuts. In order to record what I was trying, I created a new MLflow Experiment to record the state of what I tried. Since I was using different data sets and models, each subsequent modification that I was trying necessitated a new Experiment.

![Using MLflow Tracking for building this demo](https://mlflow.org/docs/latest/_images/dogfood-diagram.svg)
Using Experiments in MLflow Tracking to keep track of building this tutorial

After finding a workable approach for the dataset generator, the results can be seen in the MLflow UI.


![Checking the results of the test](https://mlflow.org/docs/latest/_images/dogfood.gif)
Validating the results of a training run in the MLflow UI

Once I found something that actually worked, I cleaned everything up (deleted them).

![Tidying up](https://mlflow.org/docs/latest/_images/cleanup-experiments.gif)
Removing experiments that were filled with failed attempts

Note

If you’re precisely following along to this tutorial and you delete your `Apple_Models` Experiment, recreate it before proceeding to the next step in the tutorial.
-->

### 初期段階のプロジェクト開発での実験の使用

以下の図が示すように、いくつかのショートカットを試みました。試していることを記録するために、新しいMLflow実験を作成して試した内容の状態を記録しました。異なるデータセットやモデルを使用していたため、試みていた各修正には新しい実験が必要でした。

![このデモの構築にMLflow Tracking を使用する](https://mlflow.org/docs/latest/_images/dogfood-diagram.svg)
このチュートリアルを構築する過程を追跡するためにMLflow Tracking で実験を使用する

データセットジェネレーターに対して実用的なアプローチを見つけた後、結果はMLflow UIで確認できます。

![テストの結果の確認](https://mlflow.org/docs/latest/_images/dogfood.gif)
MLflow UIで学習実行の結果を検証する

実際に機能するものを見つけた後、すべてをクリーンアップしました（削除しました）。

![片付け](https://mlflow.org/docs/latest/_images/cleanup-experiments.gif)
失敗した試みでいっぱいの実験を削除する

注記

このチュートリアルに正確に従っていて、`Apple_Models`実験を削除した場合は、次のステップに進む前にそれを再作成してください。


<!--
## Using MLflow Tracking to keep track of training

Now that we have our data set and have seen a little bit of how runs are recorded, let’s dive in to using MLflow to tracking a training iteration.

To start with, we will need to import our required modules.
-->


## 学習を追跡するためのMLflow Tracking の使用

これでデータセットが用意され、実行がどのように記録されるかを少し見たので、MLflowを使用して学習イテレーションを追跡することに焦点を当てましょう。

始めるにあたり、必要なモジュールをインポートする必要があります。


```Python
import mlflow
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
```

<!--
Notice that here we aren’t importing the `MlflowClient` directly. For this portion, we’re going to be using the `fluent` API. The fluent APIs use a globally referenced state of the MLflow tracking server’s uri. This global instance allows for us to use these ‘higher-level’ (simpler) APIs to perform every action that we can otherwise do with the `MlflowClient`, with the addition of some other useful syntax (such as context handlers that we’ll be using very shortly) to make integrating MLflow to ML workloads as simple as possible.

In order to use the `fluent` API, we’ll need to set the global reference to the Tracking server’s address. We do this via the following command:
-->

ここでは`MlflowClient`を直接インポートしていません。この部分では`fluent` APIを使用します。fluent APIはMLflow Tracking サーバのURIのグローバルに参照される状態を使用します。このグローバルインスタンスにより、`MlflowClient`を使用して行うことができるすべてのアクションを実行でき、コンテキストハンドラーなどのいくつかの便利な構文（まもなく使用する予定です）を追加して、MLワークロードへのMLflowの統合を可能な限り簡単にします。

`fluent` APIを使用するためには、トラッキングサーバのアドレスへのグローバル参照を設定する必要があります。これは次のコマンドを通じて行います：


```Python
mlflow.set_tracking_uri("http://127.0.0.1:8080")
```

<!--
Once this is set, we can define a few more constants that we’re going to be using when logging our training events to MLflow in the form of runs. We’ll start by defining an Experiment that will be used to log runs to. The parent-child relationship of Experiments to Runs and its utility will become very clear once we start iterating over some ideas and need to compare the results of our tests.
-->

この設定が完了すると、MLflowに学習イベントを実行としてログする際に使用するいくつかの定数を定義できます。まず、実行をログするために使用される実験を定義します。実験と実行の親子関係およびその有用性は、いくつかのアイデアを反復処理し始めてテストの結果を比較する必要があるときに非常に明確になります。


```Python
# Sets the current active experiment to the "Apple_Models" experiment and
# returns the Experiment metadata
apple_experiment = mlflow.set_experiment("Apple_Models")

# Define a run name for this iteration of training.
# If this is not set, a unique name will be auto-generated for your run.
run_name = "apples_rf_test"

# Define an artifact path that the model will be saved to.
artifact_path = "rf_apples"
```

<!--
With these variables defined, we can commence with actually training a model.

Firstly, let’s look at what we’re going to be running. Following the code display, we’ll look at an annotated version of the code.
-->
これらの変数を定義したので、実際にモデルの学習を開始できます。

まず、実行する内容を見てみましょう。コードの表示に続いて、コードの注釈付きバージョンを見ていきます。


```Python
# Split the data into features and target and drop irrelevant date field and target field
X = data.drop(columns=["date", "demand"])
y = data["demand"]

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

params = {
    "n_estimators": 100,
    "max_depth": 6,
    "min_samples_split": 10,
    "min_samples_leaf": 4,
    "bootstrap": True,
    "oob_score": False,
    "random_state": 888,
}

# Train the RandomForestRegressor
rf = RandomForestRegressor(**params)

# Fit the model on the training data
rf.fit(X_train, y_train)

# Predict on the validation set
y_pred = rf.predict(X_val)

# Calculate error metrics
mae = mean_absolute_error(y_val, y_pred)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred)

# Assemble the metrics we're going to write into a collection
metrics = {"mae": mae, "mse": mse, "rmse": rmse, "r2": r2}

# Initiate the MLflow run context
with mlflow.start_run(run_name=run_name) as run:
    # Log the parameters used for the model fit
    mlflow.log_params(params)

    # Log the error metrics that were calculated during validation
    mlflow.log_metrics(metrics)

    # Log an instance of the trained model for later use
    mlflow.sklearn.log_model(
        sk_model=rf, input_example=X_val, artifact_path=artifact_path
    )
```

<!--
To aid in visualizing how MLflow tracking API calls add in to an ML training code base, see the figure below.

![Explanation of MLflow integration into ML training code](https://mlflow.org/docs/latest/_images/training-annotation.png)
-->

MLflow Tracking APIの呼び出しが機械学習学習のコードベースにどのように組み込まれるかを可視化するために、以下の図を参照してください。

![ML学習コードへのMLflow統合の説明](https://mlflow.org/docs/latest/_images/training-annotation.png)

<!--
## Putting it all together

Let’s see what this looks like when we run our model training code and navigate to the MLflow UI.

![Log the model to MLflow](https://mlflow.org/docs/latest/_images/logging-first-model.gif) 
-->


## すべてをまとめる

モデル学習コードを実行し、MLflow UIにナビゲートしたときの様子を見てみましょう。

![モデルをMLflowにログする](https://mlflow.org/docs/latest/_images/logging-first-model.gif)

