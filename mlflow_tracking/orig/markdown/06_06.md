<!--
# Understanding Parent and Child Runs in MLflow

* [https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html)


## Introduction

Machine learning projects often involve intricate relationships. These connections can emerge at various stages, be it the project’s conception, during data preprocessing, in the model’s architecture, or even during the model’s tuning process. MLflow provides tools to efficiently capture and represent these relationships.

## Core Concepts of MLflow: Tags, Experiments, and Runs

In our foundational MLflow tutorial, we highlighted a fundamental relationship: the association between tags, experiments, and runs. This association is crucial when dealing with complex ML projects, such as forecasting models for individual products in a supermarket, as presented in our example. The diagram below offers a visual representation:

![Tags, experiments, and runs relationships](https://mlflow.org/docs/latest/_images/tag-exp-run-relationship.svg)
A model grouping hierarchy

### Key Aspects

* Tags: These are instrumental in defining business-level filtering keys. They aid in retrieving relevant experiments and their runs.
* Experiments: They set boundaries, both from a business perspective and data-wise. For instance, sales data for carrots wouldn’t be used to predict sales of apples without prior validation.
* Runs: Each run captures a specific hypothesis or iteration of training, nestled within the context of the experiment.
-->

# MLflowにおける親と子のランの理解

* [MLflowドキュメント](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html)

## 導入

機械学習プロジェクトにはしばしば複雑な関係が関与します。これらの接続は、プロジェクトの構想時、データ前処理中、モデルのアーキテクチャの段階、あるいはモデルのチューニングプロセス中に現れることがあります。MLflowはこれらの関係を効率的に捉えて表現するためのツールを提供します。

## MLflowの基本概念: タグ、実験、およびラン

私たちの基礎的なMLflowチュートリアルで、我々は基本的な関係を強調しました: タグ、実験、およびランの間の連携です。この連携は、例として提示されたスーパーマーケットの個々の製品の予測モデルのような複雑なMLプロジェクトを扱う際に不可欠です。以下の図は、これを視覚的に表しています：

![タグ、実験、およびランの関係](https://mlflow.org/docs/latest/_images/tag-exp-run-relationship.svg)
モデルグルーピング階層

### 主な側面

* タグ: これらはビジネスレベルのフィルタリングキーを定義するのに役立ちます。関連する実験とそのランを取得するのに役立ちます。
* 実験: これらはビジネスの観点から、またデータの観点からも境界を設定します。例えば、ニンジンの販売データは、事前の検証なしにリンゴの販売予測には使用されません。
* ラン: 各ランは、実験の文脈の中で、特定の仮説や学習の反復をキャプチャします。


<!--
## The Real-world Challenge: Hyperparameter Tuning

While the above model suffices for introductory purposes, real-world scenarios introduce complexities. One such complexity arises when tuning models.

Model tuning is paramount. Methods range from grid search (though typically not recommended due to inefficiencies) to random searches, and more advanced approaches like automated hyperparameter tuning. The objective remains the same: to optimally traverse the model’s parameter space.

### Benefits of Hyperparameter Tuning

* Loss Metric Relationship: By analyzing the relationship between hyperparameters and optimization loss metrics, we can discern potentially irrelevant parameters.
* Parameter Space Analysis: Monitoring the range of tested values can indicate if we need to constrict or expand our search space.
* Model Sensitivity Analysis: Estimating how a model reacts to specific parameters can pinpoint potential feature set issues.

But here lies the challenge: How do we systematically store the extensive data produced during hyperparameter tuning?

![Challenges with hyperparameter data storage](https://mlflow.org/docs/latest/_images/what-to-do-with-hyperparam-runs.svg)
The quandary of storing hyperparameter data

In the upcoming sections, we’ll delve deeper, exploring MLflow’s capabilities to address this challenge, focusing on the concepts of Parent and Child Runs.

### What are Parent and Child Runs?

At its core, MLflow allows users to track experiments, which are essentially named groups of runs. A “run” in this context refers to a single execution of a model training event, where you can log parameters, metrics, tags, and artifacts associated with the training process. The concept of Parent and Child Runs introduces a hierarchical structure to these runs.

Imagine a scenario where you’re testing a deep learning model with different architectures. Each architecture can be considered a parent run, and every iteration of hyperparameter tuning for that architecture becomes a child run nested under its respective parent.
-->

## 実際の課題：ハイパーパラメータチューニング

上述のモデルは入門用としては十分ですが、実際のシナリオでは複雑さが介入します。そのような複雑さの一つが、モデルのチューニング時に生じます。

モデルチューニングは極めて重要です。方法は、効率が低いと一般に推奨されないグリッドサーチから、ランダムサーチ、そして自動ハイパーパラメータチューニングのようなより進んだアプローチまで様々です。目標は同じです：モデルのパラメータ空間を最適に探索すること。

### ハイパーパラメータチューニングの利点

* 損失メトリックとの関係: ハイパーパラメータと最適化損失メトリクスとの関係を分析することで、おそらく無関係なパラメータを見極めることができます。
* パラメータ空間の分析: テストされた値の範囲を監視することで、検索空間を縮小するか拡大するかを示すことができます。
* モデルの感度分析: 特定のパラメータに対するモデルの反応を推定することで、潜在的な特徴量セットの問題を特定することができます。

しかし、ここに課題があります：ハイパーパラメータチューニング中に生成された広範なデータをどのように体系的に保存するか？

![ハイパーパラメータデータの保存の課題](https://mlflow.org/docs/latest/_images/what-to-do-with-hyperparam-runs.svg)
ハイパーパラメータデータを保存するジレンマ

次のセクションでは、この課題に対処するMLflowの能力をより深く探求し、親ランと子ランの概念に焦点を当てます。

### 親ランと子ランとは何か？

その核心において、MLflowは実験を追跡することを可能にします。これは本質的にはランの名前付きグループです。「ラン」とはこの文脈で、モデル学習イベントの一回の実行を指し、学習プロセスに関連するパラメータ、メトリクス、タグ、アーティファクトをログすることができます。親ランと子ランの概念は、これらのランに階層構造を導入します。

異なるアーキテクチャでディープラーニングモデルをテストしているシナリオを想像してみてください。各アーキテクチャは親ランとみなすことができ、そのアーキテクチャに対するハイパーパラメータチューニングの各反復は、それぞれの親の下にネストされた子ランとなります。

<!--
### Benefits

1. Organizational Clarity: By using Parent and Child Runs, you can easily group related runs together. For instance, if you’re running a hyperparameter search using a Bayesian approach on a particular model architecture, every iteration can be logged as a child run, while the overarching Bayesian optimization process can be the parent run.
2. Enhanced Traceability: When working on large projects with a broad product hierarchy, child runs can represent individual products or variants, making it straightforward to trace back results, metrics, or artifacts to their specific run.
3. Scalability: As your experiments grow in number and complexity, having a nested structure ensures that your tracking remains scalable. It’s much easier to navigate through a structured hierarchy than a flat list of hundreds or thousands of runs.
4. Improved Collaboration: For teams, this approach ensures that members can easily understand the structure and flow of experiments conducted by their peers, promoting collaboration and knowledge sharing.

### Relationship between Experiments, Parent Runs, and Child Runs

* Experiments: Consider experiments as the topmost layer. They are named entities under which all related runs reside. For instance, an experiment named “Deep Learning Architectures” might contain runs related to various architectures you’re testing.
* Parent Runs: Within an experiment, a parent run represents a significant segment or phase of your workflow. Taking the earlier example, each specific architecture (like CNN, RNN, or Transformer) can be a parent run.
* Child Runs: Nested within parent runs are child runs. These are iterations or variations within the scope of their parent. For a CNN parent run, different sets of hyperparameters or slight architectural tweaks can each be a child run.

### Practical Example

For this example, let’s image that we’re working through a fine-tuning exercise for a particular modeling solution. We’re going through the tuning phase of rough adjustments initially, attempting to determine which parameter ranges and categorical selection values that we might want to consider for a full hyperparameter tuning run with a much higher iteration count.

#### Naive Approach with no child runs

In this first phase, we will be trying relatively small batches of different combinations of parameters and evaluating them within the MLflow UI to determine whether we should include or exempt certain values based on the relatively performance amongst our iterative trials.

If we were to use each iteration as its own MLflow run, our code might look something like this:
-->

### 利点

1. **組織的な明確さ**: 親ランと子ランを使用することで、関連するランを簡単にグループ化できます。例えば、特定のモデルアーキテクチャに対してベイジアンアプローチを使用してハイパーパラメータ検索を実行する場合、各反復は子ランとしてログされ、全体のベイジアン最適化プロセスは親ランになります。
2. **トレーサビリティの強化**: 広範な製品階層を持つ大規模プロジェクトで作業する場合、子ランは個々の製品やバリアントを表すことができ、特定のランに対して結果、メトリクス、またはアーティファクトを簡単に追跡できます。
3. **スケーラビリティ**: 実験の数と複雑さが増すにつれて、ネストされた構造を持つことで追跡がスケーラブルに保たれます。数百または数千のランのフラットリストを通過するよりも、構造化された階層を通じてナビゲートする方がはるかに簡単です。
4. **コラボレーションの改善**: チームにとって、このアプローチはメンバーが同僚が行った実験の構造と流れを簡単に理解できるようにし、コラボレーションと知識共有を促進します。

### 実験、親ラン、および子ラン間の関係

* **実験**: 実験を最上層と考えます。これはすべての関連ランが属する名前付きエンティティです。例えば、「Deep Learning Architectures」と名付けられた実験には、テスト中のさまざまなアーキテクチャに関連するランが含まれるかもしれません。
* **親ラン**: 実験内では、親ランはワークフローの重要なセグメントまたはフェーズを表します。前の例を引き続き使用すると、特定のアーキテクチャ（CNN、RNN、またはトランスフォーマーなど）が親ランになります。
* **子ラン**: 親ランの中にネストされた子ランがあります。これらは親の範囲内での反復またはバリエーションです。CNN親ランの場合、異なるハイパーパラメータセットやわずかなアーキテクチャの調整がそれぞれ子ランになります。

### 実用例

この例では、特定のモデリングソリューションの微調整を行っていると想像しましょう。最初に、大まかな調整のチューニングフェーズを経て、はるかに多くの反復回数で実行する完全なハイパーパラメータチューニングのために検討すべきパラメータ範囲とカテゴリ選択値を決定しようとしています。

#### 子ランなしの素朴なアプローチ

この最初のフェーズでは、異なるパラメータの組み合わせの比較的小さなバッチを試し、MLflow UI内で評価して、繰り返し試行の中での相対的なパフォーマンスに基づいて特定の値を含めるか除外するかを決定します。

各反復を独自のMLflowランとして使用した場合、コードは次のようになるかもしれません：

```Python
import random
import mlflow
from functools import partial
from itertools import starmap
from more_itertools import consume


# Define a function to log parameters and metrics
def log_run(run_name, test_no):
    with mlflow.start_run(run_name=run_name):
        mlflow.log_param("param1", random.choice(["a", "b", "c"]))
        mlflow.log_param("param2", random.choice(["d", "e", "f"]))
        mlflow.log_metric("metric1", random.uniform(0, 1))
        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))


# Generate run names
def generate_run_names(test_no, num_runs=5):
    return (f"run_{i}_test_{test_no}" for i in range(num_runs))


# Execute tuning function
def execute_tuning(test_no):
    # Partial application of the log_run function
    log_current_run = partial(log_run, test_no=test_no)
    # Generate run names and apply log_current_run function to each run name
    runs = starmap(
        log_current_run, ((run_name,) for run_name in generate_run_names(test_no))
    )
    # Consume the iterator to execute the runs
    consume(runs)


# Set the tracking uri and experiment
mlflow.set_tracking_uri("http://localhost:8080")
mlflow.set_experiment("No Child Runs")

# Execute 5 hyperparameter tuning runs
consume(starmap(execute_tuning, ((x,) for x in range(5))))
```

<!--
After executing this, we can navigate to the MLflow UI to see the results of the iterations and compare each run’s error metrics to the parameters that were selected.

![Hyperparameter tuning no child runs](https://mlflow.org/docs/latest/_images/no-child-first.gif)
Initial Hyperparameter tuning execution

What happens when we need to run this again with some slight modifications?

Our code might change in-place with the values being tested:
-->

実行後、MLflow UIに移動して各反復の結果を確認し、選択されたパラメータに対する各ランのエラーメトリクスを比較することができます。

![ハイパーパラメータチューニング 子ランなし](https://mlflow.org/docs/latest/_images/no-child-first.gif)
初期ハイパーパラメータチューニング実行

もしわずかな修正を加えて再度実行する必要がある場合はどうなるでしょうか？

テストされる値に対してコードがその場で変更される可能性があります：

```Python
def log_run(run_name, test_no):
    with mlflow.start_run(run_name=run_name):
        mlflow.log_param("param1", random.choice(["a", "c"]))  # remove 'b'
        # remainder of code ...
```

<!--
When we execute this and navigate back to the UI, it is now significantly more difficult to determine which run results are associated with a particular parameter grouping. For this example, it isn’t particularly problematic since the features are identical and the parameter search space is a subset of the original hyperparameter test.

This may become a serious problem for analysis if we:

* Add terms to the original hyperparameter search space
* Modify the feature data (add or remove features)
* Change the underlying model architecture (test 1 is a Random Forest model, while test 2 is a Gradient Boosted Trees model)

Let’s take a look at the UI and see if it is clear which iteration a particular run is a member of.

![Adding more runs](https://mlflow.org/docs/latest/_images/no-child-more.gif)
Challenges with iterative tuning without child run encapsulation

It’s not too hard to imagine how complicated this can become if there are thousands of runs in this experiment.

There is a solution for this, though. We can setup the exact same testing scenario with few small modifications to make it easy to find related runs, declutter the UI, and greatly simplify the overall process of evaluating hyperparameter ranges and parameter inclusions during the process of tuning. Only a few modification are needed:

* Use child runs by adding a nested `start_run()` context within a parent run’s context.
* Add disambiguation information to the runs in the form of modifying the run_name of the parent run
* Add tag information to the parent and child runs to enable searching on keys that identify a family of runs

### Adapting for Parent and Child Runs

The code below demonstrates these modifications to our original hyperparameter tuning example.
-->

実行後にUIに戻ると、特定のパラメータグルーピングに関連付けられたランの結果を特定するのがかなり困難になります。この例では、特徴量が同一であり、パラメータ検索空間が元のハイパーパラメータテストのサブセットであるため、特に問題はありません。

しかし、以下の場合には分析で深刻な問題が発生する可能性があります:

* 元のハイパーパラメータ検索空間に項目を追加する
* 特徴量データを変更する（特徴量を追加または削除する）
* 基盤となるモデルアーキテクチャを変更する（テスト1はランダムフォレストモデル、テスト2は勾配ブースティングツリーモデル）

UIを見て、特定のランがどの反復のメンバーであるかが明確かどうかを確認しましょう。

![さらなるランの追加](https://mlflow.org/docs/latest/_images/no-child-more.gif)
子ランのカプセル化なしでの反復的チューニングの課題

この実験に何千ものランがある場合、どれだけ複雑になるかを想像するのは難しくありません。

ただし、これに対する解決策があります。関連するランを簡単に見つけるために、UIをすっきりさせ、チューニングプロセス中にハイパーパラメータ範囲とパラメータの組み込みを全体的に簡単に評価するために、いくつかの小さな変更を加えて同じテストシナリオを設定することができます。必要な変更はわずかです：

* 親ランのコンテキスト内にネストされた `start_run()` コンテキストを追加して子ランを使用する。
* 親ランのrun_nameを変更する形でランに識別情報を追加する
* ランのファミリーを識別するキーで検索できるように親ランと子ランにタグ情報を追加する

### 親ランと子ランへの適応

以下のコードは、私たちの元のハイパーパラメータチューニング例にこれらの変更を加えたものを示しています。


```Python
import random
import mlflow
from functools import partial
from itertools import starmap
from more_itertools import consume


# Define a function to log parameters and metrics and add tag
# logging for search_runs functionality
def log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):
    with mlflow.start_run(run_name=run_name, nested=True):
        mlflow.log_param("param1", random.choice(param1_choices))
        mlflow.log_param("param2", random.choice(param2_choices))
        mlflow.log_metric("metric1", random.uniform(0, 1))
        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))
        mlflow.set_tag("test_identifier", tag_ident)


# Generate run names
def generate_run_names(test_no, num_runs=5):
    return (f"run_{i}_test_{test_no}" for i in range(num_runs))


# Execute tuning function, allowing for param overrides,
# run_name disambiguation, and tagging support
def execute_tuning(
    test_no,
    param1_choices=["a", "b", "c"],
    param2_choices=["d", "e", "f"],
    test_identifier="",
):
    ident = "default" if not test_identifier else test_identifier
    # Use a parent run to encapsulate the child runs
    with mlflow.start_run(run_name=f"parent_run_test_{ident}_{test_no}"):
        # Partial application of the log_run function
        log_current_run = partial(
            log_run,
            test_no=test_no,
            param1_choices=param1_choices,
            param2_choices=param2_choices,
            tag_ident=ident,
        )
        mlflow.set_tag("test_identifier", ident)
        # Generate run names and apply log_current_run function to each run name
        runs = starmap(
            log_current_run, ((run_name,) for run_name in generate_run_names(test_no))
        )
        # Consume the iterator to execute the runs
        consume(runs)


# Set the tracking uri and experiment
mlflow.set_tracking_uri("http://localhost:8080")
mlflow.set_experiment("Nested Child Association")

# Define custom parameters
param_1_values = ["x", "y", "z"]
param_2_values = ["u", "v", "w"]

# Execute hyperparameter tuning runs with custom parameter choices
consume(
    starmap(execute_tuning, ((x, param_1_values, param_2_values) for x in range(5)))
)
```

<!--
We can view the results of executing this in the UI:

The real benefit of this nested architecture becomes much more apparent when we add additional runs with different conditions of hyperparameter selection criteria.
-->
UIでこの実行の結果を見ることができます：

異なる条件のハイパーパラメータ選択基準を持つ追加のランを加えると、このネストされたアーキテクチャの実際の利点がはるかに明確になります。

```Python
# Execute modified hyperparameter tuning runs with custom parameter choices
param_1_values = ["a", "b"]
param_2_values = ["u", "v", "w"]
ident = "params_test_2"
consume(
    starmap(
        execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))
    )
)
```

… and even more runs …

```Python
param_1_values = ["b", "c"]
param_2_values = ["d", "f"]
ident = "params_test_3"
consume(
    starmap(
        execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))
    )
)
```

<!--
Once we execute these three tuning run tests, we can view the results in the UI:

![Using child runs](https://mlflow.org/docs/latest/_images/child-runs.gif)
Encapsulating tests with child runs

In the above video, you can see that we purposefully avoided including the parent run in the run comparison. This is due to the fact that no metrics or parameters were actually written to these parent runs; rather, they were used purely for organizational purposes to limit the volume of runs visible within the UI.

In practice, it is best to store the best conditions found with a hyperparamter execution of child runs within the parent’s run data.

## Challenge

As an exercise, if you are interested, you may download the notebook with these two examples and modify the code within in order to achieve this.

[Download the notebook](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb)

The notebook contains an example implementation of this, but it is recommended to develop your own implementation that fulfills the following requirements:

* Record the lowest metric1 value amongst the children and the associated parameters with that child run in the parent run’s information.
* Add the ability to specify an iteration count to the number of children created from the calling entry point.

The results in the UI for this challenge are shown below.

![Challenge](https://mlflow.org/docs/latest/_images/parent-child-challenge.gif)
Adding best child run data to parent run

## Conclusion

The usage of parent and child runs associations can greatly simplify iterative model development. With repetitive and high-data-volume tasks such as hyperparameter tuning, encapsulating a training run’s parameter search space or feature engineering evaluation runs can help to ensure that you’re comparing exactly what you intend to compare, all with minimal effort.
-->
これらの三つのチューニング実行テストを実行すると、結果をUIで確認できます：

![子ランの使用](https://mlflow.org/docs/latest/_images/child-runs.gif)
テストを子ランでカプセル化する

上記のビデオでは、意図的に親ランをラン比較に含めていないことがわかります。これは、実際には親ランにメトリクスやパラメータが書き込まれていないためであり、UI内のランの数を制限するために純粋に組織的な目的で使用されたためです。

実際には、子ランのハイパーパラメータ実行で見つかった最良の条件を親のランデータ内に保存することが最善です。

## 課題

興味がある場合は、この二つの例を含むノートブックをダウンロードし、これを達成するためにコードを変更してみてください。

[ノートブックのダウンロード](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb)

このノートブックには例の実装が含まれていますが、以下の要件を満たす独自の実装を開発することをお勧めします：

* 子ランの中で最も低いmetric1値とその子ランに関連するパラメータを親ランの情報に記録する。
* 呼び出しエントリポイントから生成される子の数に対して反復カウントを指定する機能を追加する。

この課題のUIでの結果は以下のように表示されます。

![課題](https://mlflow.org/docs/latest/_images/parent-child-challenge.gif)
親ランに最良の子ランのデータを追加する

## 結論

親ランと子ランの関連付けの使用は、反復的なモデル開発を大幅に簡素化することができます。ハイパーパラメータチューニングのような反復的で高データボリュームのタスクでは、学習ランのパラメータ検索空間や特徴量エンジニアリング評価ランをカプセル化することで、比較したい内容を正確に比較できるようにし、労力を最小限に抑えることができます。


