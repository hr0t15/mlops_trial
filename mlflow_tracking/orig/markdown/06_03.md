<!--
# MLflow with Optuna: Hyperparameter Optimization and Tracking

* [https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html)

A critical part of building production-grade models is ensuring that a given model’s parameters are selected to create the best inference set possible. However, the sheer number of combinations and their resultant metrics can become overwhelming to track manually. That’s where tools like MLflow and Optuna come into play.

## Objective:

In this notebook, you’ll learn how to integrate MLflow with Optuna for hyperparameter optimization. We’ll guide you through the process of:

* Setting up your environment with MLflow tracking.
* Generating our training and evaluation data sets.
* Defining a partial function that fits a machine learning model.
* Using Optuna for hyperparameter tuning.
* Leveraging child runs within MLflow to keep track of each iteration during the hyperparameter tuning process.


## Why Optuna?

Optuna is an open-source hyperparameter optimization framework in Python. It provides an efficient approach to searching over hyperparameters, incorporating the latest research and techniques. With its integration into MLflow, every trial can be systematically recorded.

## Child Runs in MLflow:

One of the core features we will be emphasizing is the concept of ‘child runs’ in MLflow. When performing hyperparameter tuning, each iteration (or trial) in Optuna can be considered a ‘child run’. This allows us to group all the runs under one primary ‘parent run’, ensuring that the MLflow UI remains organized and interpretable. Each child run will track the specific hyperparameters used and the resulting metrics, providing a consolidated view of the entire optimization process.

## What’s Ahead?

* Data Preparation: We’ll start by loading and preprocessing our dataset.
* Model Definition: Defining a machine learning model that we aim to optimize.
* Optuna Study: Setting up an Optuna study to find the best hyperparameters for our model.
* MLflow Integration: Tracking each Optuna trial as a child run in MLflow.
* Analysis: Reviewing the tracked results in the MLflow UI.

By the end of this notebook, you’ll have hands-on experience in setting up an advanced hyperparameter tuning workflow, emphasizing best practices and clean organization using MLflow and Optuna.
-->

# MLflowとOptuna: ハイパーパラメータ最適化とトラッキング

* [https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html)

本番級モデルを構築する際の重要な部分は、与えられたモデルのパラメーターが最適な推論セットを作成するように選択されていることを確認することです。しかし、組み合わせの数とそれに伴うメトリクスが手動で追跡するには圧倒されることがあります。そこで、MLflowとOptunaのようなツールが登場します。

## 目的：

このノートブックでは、MLflowとOptunaを統合してハイパーパラメータ最適化を行う方法を学びます。プロセスを通じてガイドします：

* MLflow Tracking を使用した環境の設定。
* 学習および評価データセットの生成。
* 機械学習モデルをフィットする部分関数の定義。
* ハイパーパラメータチューニングのためのOptunaの使用。
* MLflow内での子実行を活用して、ハイパーパラメータチューニングプロセス中の各反復を追跡します。

## なぜOptunaか？

OptunaはPythonでのオープンソースのハイパーパラメータ最適化フレームワークです。最新の研究と技術を取り入れた効率的なハイパーパラメータ探索方法を提供します。MLflowとの統合により、すべての試行が体系的に記録されます。

## MLflowの子実行：

強調する主要な機能の一つは、MLflowの「子実行」の概念です。ハイパーパラメータチューニングを行うとき、Optunaの各反復（または試行）は「子実行」と見なすことができます。これにより、すべての実行を一つの主要な「親実行」の下にグループ化し、MLflow UIが整理され解釈可能なままであることを保証します。各子実行は使用された具体的なハイパーパラメータとその結果のメトリクスを追跡し、最適化プロセス全体の統合されたビューを提供します。

## これからの内容：

* データ準備：まず、データセットのロードと前処理から始めます。
* モデル定義：最適化を目指す機械学習モデルを定義します。
* Optunaスタディ：モデルに最適なハイパーパラメータを見つけるためのOptunaスタディの設定。
* MLflow統合：各Optuna試行をMLflowの子実行として追跡。
* 分析：MLflow UIで追跡結果をレビュー。

このノートブックを終えるころには、MLflowとOptunaを使用した高度なハイパーパラメータチューニングワークフローの設定方法について、実践的な経験とベストプラクティス、そして整理された組織を強調する経験が得られます。

Let’s dive in!

```python
import math
from datetime import datetime, timedelta

import numpy as np
import optuna
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

import mlflow
```

<!--
### Configure the tracking server uri

Depending on where you are running this notebook, your configuration may vary for how you initialize the interface with the MLflow Tracking Server.

For this example, we’re using a locally running tracking server, but other options are available (The easiest is to use the free managed service within Databricks Community Edition).

Please see the guide to running notebooks here for more information on setting the tracking server uri and configuring access to either managed or self-managed MLflow tracking servers.
-->

### トラッキングサーバURIの設定

このノートブックを実行している場所によって、MLflow Tracking サーバとのインターフェースの初期化方法が異なる場合があります。

この例ではローカルで実行されているトラッキングサーバを使用していますが、他のオプションも利用可能です（最も簡単なのはDatabricks Community Edition内の無料管理サービスを使用することです）。

トラッキングサーバURIの設定と、管理されたまたは自己管理されたMLflow Tracking サーバへのアクセスの設定についての詳細は、ノートブックの実行に関するガイドを参照してください。


```python
# NOTE: review the links mentioned above for guidance on connecting to a managed tracking server, such as the free Databricks Community Edition

mlflow.set_tracking_uri("http://localhost:8080")
```

<!--
## Generate our synthetic training data

If you’ve followed along with the introductory tutorial “Logging your first model with MLflow”, then you’re familiar with the apples sales data generator that we created for that tutorial.

Here, we’re expanding upon the data to create a slightly more complex dataset that should have improved correlation effects between the features and the target variable (the demand).
-->

### 合成学習データの生成

もし「Logging your first model with MLflow」という初心者向けのチュートリアルに従っているなら、そのチュートリアルのために作成したリンゴの販売データジェネレーターに馴染みがあるでしょう。

ここでは、データを拡張して、特徴量と目標変数（需要）との間に改善された相関効果が得られるように、少し複雑なデータセットを作成します。


```python
def generate_apple_sales_data_with_promo_adjustment(
    base_demand: int = 1000,
    n_rows: int = 5000,
    competitor_price_effect: float = -50.0,
):
    """
    Generates a synthetic dataset for predicting apple sales demand with multiple
    influencing factors.

    This function creates a pandas DataFrame with features relevant to apple sales.
    The features include date, average_temperature, rainfall, weekend flag, holiday flag,
    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,
    and the previous day's demand. The target variable, 'demand', is generated based on a
    combination of these features with some added noise.

    Args:
        base_demand (int, optional): Base demand for apples. Defaults to 1000.
        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.
        competitor_price_effect (float, optional): Effect of competitor's price being lower
                                                   on our sales. Defaults to -50.
    Returns:
        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.

    Example:
        >>> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)
        >>> df.head()
    """

    # Set seed for reproducibility
    np.random.seed(9999)

    # Create date range
    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]
    dates.reverse()

    # Generate features
    df = pd.DataFrame(
        {
            "date": dates,
            "average_temperature": np.random.uniform(10, 35, n_rows),
            "rainfall": np.random.exponential(5, n_rows),
            "weekend": [(date.weekday() >= 5) * 1 for date in dates],
            "holiday": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),
            "price_per_kg": np.random.uniform(0.5, 3, n_rows),
            "month": [date.month for date in dates],
        }
    )

    # Introduce inflation over time (years)
    df["inflation_multiplier"] = 1 + (df["date"].dt.year - df["date"].dt.year.min()) * 0.03

    # Incorporate seasonality due to apple harvests
    df["harvest_effect"] = np.sin(2 * np.pi * (df["month"] - 3) / 12) + np.sin(
        2 * np.pi * (df["month"] - 9) / 12
    )

    # Modify the price_per_kg based on harvest effect
    df["price_per_kg"] = df["price_per_kg"] - df["harvest_effect"] * 0.5

    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month
    peak_months = [4, 10]  # months following the peak availability
    df["promo"] = np.where(
        df["month"].isin(peak_months),
        1,
        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),
    )

    # Generate target variable based on features
    base_price_effect = -df["price_per_kg"] * 50
    seasonality_effect = df["harvest_effect"] * 50
    promo_effect = df["promo"] * 200

    df["demand"] = (
        base_demand
        + base_price_effect
        + seasonality_effect
        + promo_effect
        + df["weekend"] * 300
        + np.random.normal(0, 50, n_rows)
    ) * df["inflation_multiplier"]  # adding random noise

    # Add previous day's demand
    df["previous_days_demand"] = df["demand"].shift(1)
    df["previous_days_demand"].fillna(method="bfill", inplace=True)  # fill the first row

    # Introduce competitor pricing
    df["competitor_price_per_kg"] = np.random.uniform(0.5, 3, n_rows)
    df["competitor_price_effect"] = (
        df["competitor_price_per_kg"] < df["price_per_kg"]
    ) * competitor_price_effect

    # Stock availability based on past sales price (3 days lag with logarithmic decay)
    log_decay = -np.log(df["price_per_kg"].shift(3) + 1) + 2
    df["stock_available"] = np.clip(log_decay, 0.7, 1)

    # Marketing intensity based on stock availability
    # Identify where stock is above threshold
    high_stock_indices = df[df["stock_available"] > 0.95].index

    # For each high stock day, increase marketing intensity for the next week
    for idx in high_stock_indices:
        df.loc[idx : min(idx + 7, n_rows - 1), "marketing_intensity"] = np.random.uniform(0.7, 1)

    # If the marketing_intensity column already has values, this will preserve them;
    #  if not, it sets default values
    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)
    df["marketing_intensity"].fillna(fill_values, inplace=True)

    # Adjust demand with new factors
    df["demand"] = df["demand"] + df["competitor_price_effect"] + df["marketing_intensity"]

    # Drop temporary columns
    df.drop(
        columns=[
            "inflation_multiplier",
            "harvest_effect",
            "month",
            "competitor_price_effect",
            "stock_available",
        ],
        inplace=True,
    )

    return df
```

```python
df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)
df
```
```
date	average_temperature	rainfall	weekend	holiday	price_per_kg	promo	demand	previous_days_demand	competitor_price_per_kg	marketing_intensity
0	2010-01-14 11:52:20.662955	30.584727	1.199291	0	0	1.726258	0	851.375336	851.276659	1.935346	0.098677
1	2010-01-15 11:52:20.662954	15.465069	1.037626	0	0	0.576471	0	906.855943	851.276659	2.344720	0.019318
2	2010-01-16 11:52:20.662954	10.786525	5.656089	1	0	2.513328	0	1108.304909	906.836626	0.998803	0.409485
3	2010-01-17 11:52:20.662953	23.648154	12.030937	1	0	1.839225	0	1099.833810	1157.895424	0.761740	0.872803
4	2010-01-18 11:52:20.662952	13.861391	4.303812	0	0	1.531772	0	983.949061	1148.961007	2.123436	0.820779
...	...	...	...	...	...	...	...	...	...	...	...
4995	2023-09-18 11:52:20.659592	21.643051	3.821656	0	0	2.391010	0	1140.210762	1563.064082	1.504432	0.756489
4996	2023-09-19 11:52:20.659591	13.808813	1.080603	0	1	0.898693	0	1285.149505	1189.454273	1.343586	0.742145
4997	2023-09-20 11:52:20.659590	11.698227	1.911000	0	0	2.839860	0	965.171368	1284.407359	2.771896	0.742145
4998	2023-09-21 11:52:20.659589	18.052081	1.000521	0	0	1.188440	0	1368.369501	1014.429223	2.564075	0.742145
4999	2023-09-22 11:52:20.659584	17.017294	0.650213	0	0	2.131694	0	1261.301286	1367.627356	0.785727	0.833140
5000 rows × 11 columns
```

<!--
## Examining Feature-Target Correlations

Before delving into the model building process, it’s essential to understand the relationships between our features and the target variable. The upcoming function will display a plot indicating the correlation coefficient for each feature in relation to the target. Here’s why this step is crucial:

1. Avoiding Data Leakage: We must ensure that no feature perfectly correlates with the target (a correlation coefficient of 1.0). If such a correlation exists, it’s a sign that our dataset might be “leaking” information about the target. Using such data for hyperparameter tuning would mislead the model, as it could easily achieve a perfect score without genuinely learning the underlying patterns.
2. Ensuring Meaningful Relationships: Ideally, our features should have some degree of correlation with the target. If all features have correlation coefficients close to zero, it suggests a weak linear relationship. Although this doesn’t automatically render the features useless, it does introduce challenges:
  * Predictive Power: The model might struggle to make accurate predictions.
  * Overfitting Risk: With weak correlations, there’s a heightened risk of the model fitting to noise rather than genuine patterns, leading to overfitting.
  * Complexity: Demonstrating non-linear relationships or interactions between features would necessitate more intricate visualizations and evaluations.
3. Auditing and Traceability: Logging this correlation visualization with our main MLflow run ensures traceability. It provides a snapshot of the data characteristics at the time of the model training, which is invaluable for auditing and replicability purposes.

As we proceed, remember that while understanding correlations is a powerful tool, it’s just one piece of the puzzle. Let’s visualize these relationships to gain more insights!
-->

## 特徴量と目標変数の相関を調べる

モデル構築プロセスに入る前に、特徴量と目標変数との関係を理解することが重要です。次の関数は、目標変数との関連で各特徴量の相関係数を示すプロットを表示します。このステップが重要な理由を以下に説明します：

1. データリークの回避: 目標と完全に相関する特徴量（相関係数が1.0）がないことを確認する必要があります。そのような相関が存在する場合、それはデータセットが目標についての情報を「リーク」している可能性があるという兆候です。そのようなデータをハイパーパラメータチューニングに使用すると、モデルが誤って完璧なスコアを簡単に達成できるため、実際のパターンを真に学習していないことになります。
2. 意味のある関係の確保: 理想的には、私たちの特徴量は目標変数とある程度の相関を持つべきです。すべての特徴量が相関係数がゼロに近い場合、それは弱い線形関係を示唆しています。これが自動的に特徴量を無意味にするわけではありませんが、次のような課題を提起します：
  * 予測力：モデルは正確な予測を行うのに苦労する可能性があります。
  * オーバーフィッティングリスク：弱い相関では、モデルが本物のパターンではなくノイズにフィットするリスクが高まり、過学習につながる可能性があります。
  * 複雑さ：特徴量間の非線形関係や相互作用を示すには、より複雑な可視化や評価が必要になる可能性があります。
3. 監査と追跡可能性: この相関可視化を主要なMLflow実行とともにログすることで、追跡可能性が保証されます。これは、モデル学習時のデータの特性のスナップショットを提供し、監査および再現性の目的にとって非常に貴重です。

相関関係を理解することは強力なツールではあるが、それはパズルの1ピースに過ぎないことを忘れないでほしい。より多くのインサイトを得るために、これらの関係を可視化してみよう！

```python
import matplotlib.pyplot as plt
import seaborn as sns


def plot_correlation_with_demand(df, save_path=None):
    """
    Plots the correlation of each variable in the dataframe with the 'demand' column.

    Args:
    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.
    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.

    Returns:
    - None (Displays the plot on a Jupyter window)
    """

    # Compute correlations between all variables and 'demand'
    correlations = df.corr()["demand"].drop("demand").sort_values()

    # Generate a color palette from red to green
    colors = sns.diverging_palette(10, 130, as_cmap=True)
    color_mapped = correlations.map(colors)

    # Set Seaborn style
    sns.set_style(
        "whitegrid", {"axes.facecolor": "#c2c4c2", "grid.linewidth": 1.5}
    )  # Light grey background and thicker grid lines

    # Create bar plot
    fig = plt.figure(figsize=(12, 8))
    plt.barh(correlations.index, correlations.values, color=color_mapped)

    # Set labels and title with increased font size
    plt.title("Correlation with Demand", fontsize=18)
    plt.xlabel("Correlation Coefficient", fontsize=16)
    plt.ylabel("Variable", fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.grid(axis="x")

    plt.tight_layout()

    # Save the plot if save_path is specified
    if save_path:
        plt.savefig(save_path, format="png", dpi=600)

    # prevent matplotlib from displaying the chart every time we call this function
    plt.close(fig)

    return fig


# Test the function
correlation_plot = plot_correlation_with_demand(df, save_path="correlation_plot.png")
```
```
<Figure size 432x288 with 0 Axes>
```

<!--
## Investigating Feature Correlation with the Target Variable

In the code above, we’ve intentionally disabled the automatic display of the plot generated by Matplotlib. During machine learning experimentation, it’s often not useful to display figures directly within the notebook for several reasons. Instead, we aim to associate this plot with the results of an iterative experiment run. To achieve this, we’ll save the plot to our MLflow tracking system. This provides us with a detailed record, linking the state of the dataset to the logged model, its parameters, and performance metrics.

### Why Not Display Plots Directly in the Notebook?

Choosing not to display plots within the notebook is a deliberate decision, and the reasons are multiple. Some of the key points include:

* Ephemerality of Notebooks: Notebooks are inherently transient; they are not designed to be a permanent record of your work.
* Risk of Obsolescence: If you rerun portions of your notebook, the plot displayed could become outdated or misleading, posing a risk when interpreting results.
* Loss of Previous State: If you happen to rerun the entire notebook, the plot will be lost. While some plugins can recover previous cell states, setting this up can be cumbersome and time-consuming.

In contrast, logging the plot to MLflow ensures that we have a permanent, easily accessible record that correlates directly with other experiment data. This is invaluable for maintaining the integrity and reproducibility of your machine learning experiments.

### Displaying the Plot for This Guide

For the purposes of this guide, we’ll still take a moment to examine the plot. We can do this by explicitly calling the figure object we’ve returned from our function.
-->

### 目的変数との特徴量相関の調査

上記のコードでは、Matplotlibによって生成されたプロットの自動表示を意図的に無効にしています。機械学習の実験中、ノートブック内に図を直接表示することは、いくつかの理由で役に立たないことがよくあります。代わりに、このプロットを反復的な実験の結果と関連付けることを目指しています。これを達成するために、プロットをMLflow Tracking システムに保存します。これにより、データセットの状態、ログされたモデル、そのパラメータ、およびパフォーマンス指標と直接関連する詳細な記録が提供されます。

#### なぜノートブック内にプロットを直接表示しないのですか？

ノートブック内にプロットを表示しないことは意図的な決定であり、その理由は複数あります。主な点をいくつか挙げます：

* ノートブックの儚さ：ノートブックは本質的に一時的なものであり、作業の永続的な記録として設計されていません。
* 陳腐化のリスク：ノートブックの一部を再実行すると、表示されているプロットが古くなったり誤解を招く可能性があり、結果の解釈時にリスクとなります。
* 以前の状態の損失：もしノートブック全体を再実行すると、プロットは失われます。一部のプラグインでは以前のセルの状態を回復できますが、これを設定するのは手間がかかり時間もかかります。

これに対して、プロットをMLflowにログすると、他の実験データと直接相関する永続的で簡単にアクセスできる記録が得られます。これは、機械学習実験の完全性と再現性を維持するために非常に貴重です。

#### このガイドのためにプロットを表示

このガイドの目的で、プロットを見る時間を取ります。これは、関数から返された図オブジェクトを明示的に呼び出すことによって行います。


```
correlation_plot
```
![../../../_images/traditional-ml_hyperparameter-tuning-with-child-runs_notebooks_hyperparameter-tuning-with-child-runs_10_0.png](https://mlflow.org/docs/latest/_images/traditional-ml_hyperparameter-tuning-with-child-runs_notebooks_hyperparameter-tuning-with-child-runs_10_0.png)

<!--
### Visualizing Model Residuals for Diagnostic Insights

The `plot_residuals` function serves to visualize the residuals—the differences between the model’s predictions and the actual values in the validation set. Residual plots are crucial diagnostic tools in machine learning, as they can reveal patterns that suggest our model is either failing to capture some aspect of the data or that there’s a systematic issue with the model itself.

### Why Residual Plots?

Residual plots offer several advantages:

* Identifying Bias: If residuals show a trend (not centered around zero), it might indicate that your model is systematically over- or under-predicting the target variable.
* Heteroskedasticity: Varying spread of residuals across the range of the predicted values can indicate ‘Heteroskedasticity,’ which can violate assumptions in some modeling techniques.
* Outliers: Points far away from the zero line can be considered as outliers and might warrant further investigation.

### Auto-saving the Plot

Just like with the correlation plot, this function allows you to save the residual plot to a specific path. This feature aligns with our broader strategy of logging important figures to MLflow for more effective model tracking and auditing.

### Plot Structure

In the scatter plot, each point represents the residual for a specific observation in the validation set. The red horizontal line at zero serves as a reference, indicating where residuals would lie if the model’s predictions were perfect.

For the sake of this guide, we will be generating this plot, but not examining it until later when we see it within the MLflow UI.
-->

### モデル残差の可視化による診断インサイト

`plot_residuals` 関数は、モデルの予測値と検証セットの実際の値との差である残差を可視化するために使用されます。残差プロットは機械学習において重要な診断ツールであり、モデルがデータのある側面を捉えられていないか、モデル自体に系統的な問題があることを示唆するパターンを明らかにすることができます。

#### 残差プロットの利点

残差プロットはいくつかの利点を提供します：

* バイアスの特定：残差がトレンドを示す場合（ゼロの周囲に集中していない場合）、モデルが目標変数を系統的に過大または過小評価している可能性があります。
* 異分散性：予測値の範囲にわたる残差のばらつきが異なる場合、「異分散性」を示すことがあり、これは一部のモデリング技術での前提条件に違反する可能性があります。
* 外れ値：ゼロラインから遠く離れた点は外れ値と見なすことができ、さらなる調査が必要かもしれません。

#### プロットの自動保存

相関プロットと同様に、この関数を使用すると、特定のパスに残差プロットを保存できます。この機能は、より効果的なモデルトラッキングと監査のために重要な図をMLflowにログするという私たちの広範な戦略と一致しています。

#### プロット構造

散布図では、各点が検証セットの特定の観測値の残差を表します。ゼロの赤い水平線は参照として機能し、モデルの予測が完璧であれば残差がどこにあるかを示します。

このガイドのために、このプロットを生成しますが、MLflow UI内で見るまで詳しく調べることはありません。

```python
def plot_residuals(model, dvalid, valid_y, save_path=None):
    """
    Plots the residuals of the model predictions against the true values.

    Args:
    - model: The trained XGBoost model.
    - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.
    - valid_y (pd.Series): The true values for the validation set.
    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.

    Returns:
    - None (Displays the residuals plot on a Jupyter window)
    """

    # Predict using the model
    preds = model.predict(dvalid)

    # Calculate residuals
    residuals = valid_y - preds

    # Set Seaborn style
    sns.set_style("whitegrid", {"axes.facecolor": "#c2c4c2", "grid.linewidth": 1.5})

    # Create scatter plot
    fig = plt.figure(figsize=(12, 8))
    plt.scatter(valid_y, residuals, color="blue", alpha=0.5)
    plt.axhline(y=0, color="r", linestyle="-")

    # Set labels, title and other plot properties
    plt.title("Residuals vs True Values", fontsize=18)
    plt.xlabel("True Values", fontsize=16)
    plt.ylabel("Residuals", fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.grid(axis="y")

    plt.tight_layout()

    # Save the plot if save_path is specified
    if save_path:
        plt.savefig(save_path, format="png", dpi=600)

    # Show the plot
    plt.close(fig)

    return fig
```

<!--
## Understanding Feature Importance with XGBoost

The `plot_feature_importance` function is designed to visualize the importance of each feature used in our XGBoost model. Understanding feature importance can offer critical insights into the model’s decision-making process and can aid in feature selection, engineering, and interpretation.

### Types of Feature Importance

XGBoost offers multiple ways to interpret feature importance. This function supports:

* Weight: Number of times a feature appears in a tree across the ensemble of trees (for `gblinear` booster).
* Gain: Average gain (or improvement to the model) of the feature when it is used in trees (for other booster types).

We automatically select the appropriate importance type based on the booster used in the XGBoost model.

### Why Feature Importance Matters

Understanding feature importance offers several advantages:

* Interpretability: Knowing which features are most influential helps us understand the model better.
* Feature Selection: Unimportant features can potentially be dropped to simplify the model.
* Domain Understanding: Aligns the model’s importance scale with domain-specific knowledge or intuition.


### Saving and Accessing the Plot

This function returns a Matplotlib figure object that you can further manipulate or save. Like the previous plots, it is advisable to log this plot in MLflow for an immutable record of your model’s interpretive characteristics.

### Navigating the Plot

In the resulting plot, each bar represents a feature used in the model. The length of the bar corresponds to the feature’s importance, as calculated by the selected importance type.

We need a model to be trained in order to generate this plot. As such, we’ll be generating, but not displaying the plot when we train the model. The resulting figure will be logged to MLflow and visible within the UI.
-->

## XGBoostを使用した特徴量の重要性の理解

`plot_feature_importance` 関数は、XGBoostモデルで使用される各特徴量の重要性を可視化するように設計されています。特徴量の重要性を理解することは、モデルの意思決定プロセスに重要なインサイトを提供し、特徴量選択、エンジニアリング、および解釈に役立ちます。

### 特徴量の重要性の種類

XGBoostは、特徴量の重要性を解釈するための複数の方法を提供します。この機能は以下をサポートしています：

* 重み（Weight）：ある特徴量が木のアンサンブル全体でどれだけの回数出現するかを示します（`gblinear` ブースター用）。
* ゲイン（Gain）：その特徴量が木に使用されたときの平均ゲイン（またはモデルの改善）を示します（他のブースタータイプ用）。

使用するXGBoostモデルのブースターに基づいて、適切な重要性のタイプを自動的に選択します。

### 特徴量の重要性が重要な理由

特徴量の重要性を理解することは、いくつかの利点を提供します：

* 解釈可能性：最も影響力のある特徴量がどれかを知ることで、モデルをよりよく理解できます。
* 特徴量選択：重要でない特徴量はモデルを単純化するために削除することができます。
* ドメイン理解：モデルの重要性スケールをドメイン固有の知識や直感と一致させます。

### プロットの保存とアクセス

この関数は操作または保存できるMatplotlib図オブジェクトを返します。前のプロットと同様に、モデルの解釈特性の不変の記録としてこのプロットをMLflowにログすることをお勧めします。

### プロットのナビゲート

結果のプロットでは、各バーがモデルで使用される特徴量を表しています。バーの長さは、選択された重要性のタイプによって計算された特徴量の重要性に対応します。

このプロットを生成するにはモデルの学習が必要です。そのため、モデルを学習するときにプロットを表示せずに生成します。生成された図はMLflowにログされ、UI内で表示されます。


```python
def plot_feature_importance(model, booster):
    """
    Plots feature importance for an XGBoost model.

    Args:
    - model: A trained XGBoost model

    Returns:
    - fig: The matplotlib figure object
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    importance_type = "weight" if booster == "gblinear" else "gain"
    xgb.plot_importance(
        model,
        importance_type=importance_type,
        ax=ax,
        title=f"Feature Importance based on {importance_type}",
    )
    plt.tight_layout()
    plt.close(fig)

    return fig
```

<!--
## Setting Up the MLflow Experiment

Before we start our hyperparameter tuning process, we need to designate a specific “experiment” within MLflow to track and log our results. An experiment in MLflow is essentially a named set of runs. Each run within an experiment tracks its own parameters, metrics, tags, and artifacts.

### Why create a new experiment?

1. Organization: It helps in keeping our runs organized under a specific task or project, making it easier to compare and analyze results.
2. Isolation: By isolating different tasks or projects into separate experiments, we prevent accidental overwrites or misinterpretations of results.

The `get_or_create_experiment` function we’ve defined below aids in this process. It checks if an experiment with the specified name already exists. If yes, it retrieves its ID. If not, it creates a new experiment and returns its ID.

### How will we use the experiment_id?

The retrieved or created experiment_id becomes crucial when we initiate our hyperparameter tuning. As we start the parent run for tuning, the experiment_id ensures that the run, along with its nested child runs, gets logged under the correct experiment. It provides a structured way to navigate, compare, and analyze our tuning results within the MLflow UI.

When we want to try additional parameter ranges, different parameters, or a slightly modified dataset, we can use this Experiment to log all parent runs to keep our MLflow Tracking UI clean and easy to navigate.

Let’s proceed and set up our experiment!
-->

## MLflow実験の設定

ハイパーパラメータチューニングプロセスを開始する前に、結果を追跡して記録するためにMLflow内で特定の「実験」を指定する必要があります。MLflowの実験は、基本的には名前付きの実行セットです。実験内の各実行は、独自のパラメータ、メトリクス、タグ、およびアーティファクトを追跡します。

### 新しい実験を作成する理由は？

1. 組織化：特定のタスクやプロジェクトの下で実行を整理するのに役立ち、結果の比較と分析が容易になります。
2. 分離：異なるタスクやプロジェクトを別々の実験に分離することで、結果の誤って上書きされたり、誤解されたりするのを防ぎます。

以下で定義した `get_or_create_experiment` 関数は、このプロセスを支援します。指定された名前の実験がすでに存在するかどうかを確認します。存在する場合は、そのIDを取得します。存在しない場合は、新しい実験を作成してそのIDを返します。

### 実験IDはどのように使用しますか？

取得または作成された実験IDは、ハイパーパラメータチューニングを開始する際に重要となります。チューニングのための親実行を開始すると、実験IDは、その実行およびそのネストされた子実行が正しい実験の下で記録されることを保証します。これにより、MLflow UI内でチューニング結果をナビゲート、比較、分析するための構造化された方法が提供されます。

追加のパラメータ範囲、異なるパラメータ、またはわずかに変更されたデータセットを試したい場合、この実験を使用してすべての親実行をログに記録し、MLflow Tracking UIを清潔でナビゲートしやすい状態に保つことができます。

それでは進んで実験を設定しましょう！

```python
def get_or_create_experiment(experiment_name):
    """
    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.

    This function checks if an experiment with the given name exists within MLflow.
    If it does, the function returns its ID. If not, it creates a new experiment
    with the provided name and returns its ID.

    Parameters:
    - experiment_name (str): Name of the MLflow experiment.

    Returns:
    - str: ID of the existing or newly created MLflow experiment.
    """

    if experiment := mlflow.get_experiment_by_name(experiment_name):
        return experiment.experiment_id
    else:
        return mlflow.create_experiment(experiment_name)
```

<!--
### Create an experiment for our hyperparameter tuning runs
-->
### ハイパーパラメータチューニング実行用の実験を作成

```python
experiment_id = get_or_create_experiment("Apples Demand")
```
<!--
We can view the `experiment_id` that was either generated or fetched to see how this unique reference key looks. The value generated here is also visible within the MLflow UI.
-->
生成された `experiment_id` を確認して、このユニークな参照キーがどのように見えるかを見てみましょう。ここで生成された値は、MLflow UI内でも確認できます。

```python
experiment_id
```
```
'908436739760555869'
```

<!--
### Setting Up MLflow and Data Preprocessing for Model Training

This section of the code accomplishes two major tasks: initializing an MLflow experiment for usage in run tracking and preparing the dataset for model training and validation.

#### MLflow Initialization

We start by setting the MLflow experiment using the set_experiment function. The experiment_id serves as a unique identifier for the experiment, allowing us to segregate and manage different runs and their associated data efficiently.

#### Data Preprocessing

The next steps involve preparing the dataset for model training:

1. Feature Selection: We drop the columns ‘date’ and ‘demand’ from our DataFrame, retaining only the feature columns in `X`.
2. Target Variable: The ‘demand’ column is designated as our target variable `y`.
3. Data Splitting: We split the dataset into training `(train_x, train_y)` and validation `(valid_x, valid_y)` sets using a 75-25 split.
4. XGBoost Data Format: Finally, we convert the training and validation datasets into XGBoost’s DMatrix format. This optimized data structure speeds up the training process and is required for using XGBoost’s advanced functionalities.


#### Why These Steps Matter

* MLflow Tracking: Initializing the MLflow experiment ensures that all subsequent model runs, metrics, and artifacts are logged under the same experiment, making it easier to compare and analyze different models. While we are using the fluent API to do this here, you can also specify the experiment_id within a start_run() context.
* Data Preparation: Properly preparing your data ensures that the model training process will proceed without issues and that the results will be as accurate as possible.

In the next steps, we’ll proceed to model training and evaluation, and all these preparation steps will come into play.
-->

### MLflowの設定とモデル学習のためのデータ前処理

このコードのセクションでは、2つの主要なタスクを実行します：実行追跡用のMLflow実験の初期化と、モデル学習および検証のためのデータセットの準備。

#### MLflowの初期化

まず、set_experiment 関数を使用してMLflow実験を設定します。experiment_id は実験のユニークな識別子として機能し、異なる実行とそれらの関連データを効率的に区別し、管理することを可能にします。

#### データ前処理

次に、モデル学習用のデータセットを準備するステップが続きます：

1. 特徴量選択：データフレームから「date」と「demand」の列を削除し、`X` には特徴量列のみを保持します。
2. 目標変数：「demand」列を目標変数 `y` として指定します。
3. データ分割：データセットを学習（`train_x, train_y`）と検証（`valid_x, valid_y`）セットに75-25の割合で分割します。
4. XGBoostデータ形式：最後に、学習と検証のデータセットをXGBoostのDMatrix形式に変換します。この最適化されたデータ構造は学習プロセスを加速し、XGBoostの高度な機能を使用するために必要です。

#### これらのステップが重要な理由

* MLflow Tracking ：MLflow実験を初期化することで、後続のすべてのモデル実行、メトリックス、アーティファクトが同じ実験の下にログされ、異なるモデルを比較・分析しやすくなります。ここではfluent APIを使用していますが、start_run() コンテキスト内で experiment_id を指定することもできます。
* データ準備：データを適切に準備することで、モデル学習プロセスが問題なく進行し、結果が可能な限り正確になることを保証します。

次のステップでは、モデルの学習と評価に進み、これらの準備ステップが活用されます。

```python
# Set the current active MLflow experiment
mlflow.set_experiment(experiment_id=experiment_id)

# Preprocess the dataset
X = df.drop(columns=["date", "demand"])
y = df["demand"]
train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)
dtrain = xgb.DMatrix(train_x, label=train_y)
dvalid = xgb.DMatrix(valid_x, label=valid_y)
```

<!--
### Hyperparameter Tuning and Model Training using Optuna and MLflow

The `objective` function serves as the core of our hyperparameter tuning process using Optuna. Additionally, it trains an XGBoost model using the selected hyperparameters and logs metrics and parameters to MLflow.

#### MLflow Nested Runs

The function starts a new nested run in MLflow. Nested runs are useful for organizing hyperparameter tuning experiments as they allow you to group individual runs under a parent run.

#### Defining Hyperparameters

Optuna’s `trial.suggest_*` methods are used to define a range of possible values for hyperparameters. Here’s what each hyperparameter does:

* `objective` and `eval_metric`: Define the loss function and evaluation metric.
* `booster`: Type of boosting to be used (`gbtree`, `gblinear`, or `dart`).
* `lambda` and `alpha`: Regularization parameters.
* Additional parameters like `max_depth`, `eta`, and `gamma` are specific to tree-based models (`gbtree` and `dart`).

#### Model Training

An XGBoost model is trained using the chosen hyperparameters and the preprocessed training dataset (`dtrain`). Predictions are made on the validation set (`dvalid`), and the mean squared error (`mse`) is calculated.

#### Logging with MLflow

All the selected hyperparameters and metrics (`mse` and `rmse`) are logged to MLflow for later analysis and comparison.

* `mlflow.log_params`: Logs the hyperparameters.
* `mlflow.log_metric`: Logs the metrics.

### Why This Function is Important

* Automated Tuning: Optuna automates the process of finding the best hyperparameters.
* Experiment Tracking: MLflow allows us to keep track of each run’s hyperparameters and performance metrics, making it easier to analyze, compare, and reproduce experiments later.

In the next step, this objective function will be used by Optuna to find the optimal set of hyperparameters for our XGBoost model.

## Housekeeping: Streamlining Logging for Optuna Trials

As we embark on our hyperparameter tuning journey with Optuna, it’s essential to understand that the process can generate a multitude of runs. In fact, so many that the standard output (stdout) from the default logger can quickly become inundated, producing pages upon pages of log reports.

While the verbosity of the default logging configuration is undeniably valuable during the code development phase, initiating a full-scale trial can result in an overwhelming amount of information. Considering this, logging every single detail to stdout becomes less practical, especially when we have dedicated tools like MLflow to meticulously track our experiments.

To strike a balance, we’ll utilize callbacks to tailor our logging behavior.
-->

### OptunaとMLflowを使用したハイパーパラメータチューニングとモデル学習

`objective` 関数は、Optunaを使用したハイパーパラメータチューニングプロセスの中核として機能し、選択されたハイパーパラメータを使用してXGBoostモデルを学習し、MLflowにメトリクスとパラメータをログします。

#### MLflowのネストされた実行

この関数は、MLflowで新しいネストされた実行を開始します。ネストされた実行は、個々の実行を親実行の下にグループ化することができるため、ハイパーパラメータチューニング実験を整理するのに役立ちます。

#### ハイパーパラメータの定義

Optunaの `trial.suggest_*` メソッドは、ハイパーパラメータの可能な値の範囲を定義するために使用されます。以下は各ハイパーパラメータの機能です：

* `objective` と `eval_metric`：損失関数と評価メトリックを定義します。
* `booster`：使用されるブースティングのタイプ（`gbtree`、`gblinear`、または `dart`）。
* `lambda` と `alpha`：正則化パラメータ。
* `max_depth`、`eta`、`gamma` などの追加パラメータは、木ベースのモデル（`gbtree` および `dart`）に特有です。

#### モデル学習

選択されたハイパーパラメータと前処理された学習データセット（`dtrain`）を使用してXGBoostモデルが学習されます。検証セット（`dvalid`）で予測が行われ、平均二乗誤差（`mse`）が計算されます。

#### MLflowでのログ記録

選択されたハイパーパラメータとメトリクス（`mse` および `rmse`）は、後で分析および比較するためにMLflowにログされます。

* `mlflow.log_params`：ハイパーパラメータをログします。
* `mlflow.log_metric`：メトリクスをログします。

### この関数が重要な理由

* 自動チューニング：Optunaは最適なハイパーパラメータを見つけるプロセスを自動化します。
* 実験トラッキング：MLflowを使用すると、各実行のハイパーパラメータとパフォーマンスメトリクスを追跡できるため、後で実験を分析、比較、再現するのが簡単になります。

次のステップでは、この目的関数を使用してOptunaでXGBoostモデルの最適なハイパーパラメータセットを見つけます。

## お手入れ：Optuna試験のログを合理化

Optunaでハイパーパラメータチューニングの旅を始めるにあたり、このプロセスが多数の実行を生成する可能性があることを理解することが重要です。実際には、デフォルトのロガーからの標準出力（stdout）がすぐにログのレポートで溢れかえる可能性があります。

デフォルトのログ設定の詳細さはコード開発フェーズ中には疑いなく価値がありますが、本格的な試験を開始すると、大量の情報によって圧倒される可能性があります。これを考慮して、標準出力への詳細なログをすべて記録することは、MLflowのような専用ツールで実験を厳密に追跡できる場合、実用的ではありません。

バランスを取るために、コールバックを使用してログ行動をカスタマイズします。



<!--
## Implementing a Logging Callback:

The callback we’re about to introduce will modify the default reporting behavior. Instead of logging every trial, we’ll only receive updates when a new hyperparameter combination yields an improvement over the best metric value recorded thus far.

This approach offers two salient benefits:

1. Enhanced Readability: By filtering out the extensive log details and focusing only on the trials that show improvement, we can gauge the efficacy of our hyperparameter search. For instance, if we observe a diminishing frequency of ‘best result’ reports early on, it might suggest that fewer iterations would suffice to pinpoint an optimal hyperparameter set. On the other hand, a consistent rate of improvement might indicate that our feature set requires further refinement.
2. Progress Indicators: Especially pertinent for extensive trials that span hours or even days, receiving periodic updates provides assurance that the process is still in motion. These ‘heartbeat’ notifications affirm that our system is diligently at work, even if it’s not flooding stdout with every minute detail.

Moreover, MLflow’s user interface (UI) complements this strategy. As each trial concludes, MLflow logs the child run, making it accessible under the umbrella of the parent run.

In the ensuing code, we:

1. Adjust Optuna’s logging level to report only errors, ensuring a decluttered stdout.
2. Define a `champion_callback` function, tailored to log only when a trial surpasses the previously recorded best metric.

Let’s dive into the implementation:
-->

## ロギングコールバックの実装:

これから紹介するコールバックは、デフォルトの報告動作を変更します。すべての試行を記録する代わりに、これまでに記録された最良のメトリック値を上回る新しいハイパーパラメータの組み合わせが得られたときにのみ、更新情報を受け取ります。

このアプローチは、次の2つの顕著な利点を提供します：

1. **向上した可読性**：広範なログ詳細をフィルタリングし、改善を示す試行のみに焦点を当てることで、ハイパーパラメータ検索の効果を測定することができます。たとえば、早い段階で「最良の結果」の報告の頻度が減少することが観察された場合、最適なハイパーパラメータセットを特定するために必要な反復が少なくて済むかもしれません。一方、改善の割合が一貫している場合は、機能セットがさらなる洗練を必要としていることを示している可能性があります。
2. **進捗指標**：特に数時間または数日にわたる広範な試行にとって、定期的な更新を受け取ることは、プロセスがまだ進行中であることを保証します。これらの「ハートビート」通知は、システムが詳細のすべてを標準出力に溢れさせることなく、一生懸命に動作していることを確認します。

さらに、MLflowのユーザーインターフェース（UI）は、この戦略を補完します。各試行が終了すると、MLflowはその子実行を記録し、親実行の傘下でアクセス可能にします。

以下のコードでは、私たちは：

1. Optunaのログレベルをエラーのみを報告するように調整し、標準出力を整理します。
2. 以前に記録された最良のメトリックを超えた試行のみをログするようにカスタマイズされた `champion_callback` 関数を定義します。

実装に入りましょう：


```python
# override Optuna's default logging to ERROR only
optuna.logging.set_verbosity(optuna.logging.ERROR)

# define a logging callback that will report on only new challenger parameter configurations if a
# trial has usurped the state of 'best conditions'


def champion_callback(study, frozen_trial):
    """
    Logging callback that will report when a new trial iteration improves upon existing
    best trial values.

    Note: This callback is not intended for use in distributed computing systems such as Spark
    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's
    workers or agents.
    The race conditions with file system state management for distributed trials will render
    inconsistent values with this callback.
    """

    winner = study.user_attrs.get("winner", None)

    if study.best_value and winner != study.best_value:
        study.set_user_attr("winner", study.best_value)
        if winner:
            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100
            print(
                f"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with "
                f"{improvement_percent: .4f}% improvement"
            )
        else:
            print(f"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}")
```

```python
def objective(trial):
    with mlflow.start_run(nested=True):
        # Define hyperparameters
        params = {
            "objective": "reg:squarederror",
            "eval_metric": "rmse",
            "booster": trial.suggest_categorical("booster", ["gbtree", "gblinear", "dart"]),
            "lambda": trial.suggest_float("lambda", 1e-8, 1.0, log=True),
            "alpha": trial.suggest_float("alpha", 1e-8, 1.0, log=True),
        }

        if params["booster"] == "gbtree" or params["booster"] == "dart":
            params["max_depth"] = trial.suggest_int("max_depth", 1, 9)
            params["eta"] = trial.suggest_float("eta", 1e-8, 1.0, log=True)
            params["gamma"] = trial.suggest_float("gamma", 1e-8, 1.0, log=True)
            params["grow_policy"] = trial.suggest_categorical(
                "grow_policy", ["depthwise", "lossguide"]
            )

        # Train XGBoost model
        bst = xgb.train(params, dtrain)
        preds = bst.predict(dvalid)
        error = mean_squared_error(valid_y, preds)

        # Log to MLflow
        mlflow.log_params(params)
        mlflow.log_metric("mse", error)
        mlflow.log_metric("rmse", math.sqrt(error))

    return error
```

<!--
## Orchestrating Hyperparameter Tuning, Model Training, and Logging with MLflow

This section of the code serves as the orchestration layer, bringing together Optuna for hyperparameter tuning and MLflow for experiment tracking.

### Initiating Parent Run

We begin by starting a parent MLflow run with the name “Best Run”. All subsequent operations, including Optuna’s trials, are nested under this parent run, providing a structured way to organize our experiments.

### Hyperparameter Tuning with Optuna

* `study = optuna.create_study(direction='minimize')`: We create an Optuna study object aiming to minimize our objective function.
* `study.optimize(objective, n_trials=10)`: The objective function is optimized over 10 trials.

### Logging Best Parameters and Metrics

After Optuna finds the best hyperparameters, we log these, along with the best mean squared error (mse) and root mean squared error (`rmse`), to MLflow.

### Logging Additional Metadata

Using `mlflow.set_tags`, we log additional metadata like the project name, optimization engine, model family, and feature set version. This helps in better categorizing and understanding the context of the model run.

### Model Training and Artifact Logging

* We train an XGBoost model using the best hyperparameters.
* Various plots—correlation with demand, feature importance, and residuals—are generated and logged as artifacts in MLflow.

### Model Serialization and Logging

Finally, the trained model is logged to MLflow using `mlflow.xgboost.log_model`, along with an example input and additional metadata. The model is stored in a specified artifact path and its URI is retrieved.


### Why This Block is Crucial

* End-to-End Workflow: This code block represents an end-to-end machine learning workflow, from hyperparameter tuning to model evaluation and logging.
* Reproducibility: All details about the model, including hyperparameters, metrics, and visual diagnostics, are logged, ensuring that the experiment is fully reproducible.
* Analysis and Comparison: With all data logged in MLflow, it becomes easier to analyze the performance of various runs and choose the best model for deployment.

In the next steps, we’ll explore how to retrieve and use the logged model for inference.

### Setting a Descriptive Name for the Model Run

Before proceeding with model training and hyperparameter tuning, it’s beneficial to assign a descriptive name to our MLflow run. This name serves as a human-readable identifier, making it easier to track, compare, and analyze different runs.

### The Importance of Naming Runs:

* Reference by Name: While MLflow provides unique identifying keys like run_id for each run, having a descriptive name allows for more intuitive referencing, especially when using particular APIs and navigating the MLflow UI.
* Clarity and Context: A well-chosen run name can provide context about the hypothesis being tested or the specific modifications made, aiding in understanding the purpose and rationale of a particular run.
* Automatic Naming: If you don’t specify a run name, MLflow will generate a unique fun name for you. However, this might lack the context and clarity of a manually chosen name.

### Best Practices:

When naming your runs, consider the following:

1. Relevance to Code Changes: The name should reflect any code or parameter modifications made for that run.
2. Iterative Runs: If you’re executing multiple runs iteratively, it’s a good idea to update the run name for each iteration to avoid confusion.

In the subsequent steps, we will set a name for our parent run. Remember, if you execute the model training multiple times, consider updating the run name for clarity.
-->

## ハイパーパラメータチューニング、モデル学習、MLflowによるログ記録の統合

このコードセクションは、オーケストレーションレイヤーとして機能し、ハイパーパラメータチューニングのためのOptunaと実験トラッキングのためのMLflowを組み合わせます。

### 親ランの開始

「Best Run」という名前で親MLflowランを開始します。Optunaの試行を含む、すべての後続の操作は、この親ランの下にネストされ、実験を整理する構造化された方法を提供します。

### Optunaによるハイパーパラメータチューニング

* `study = optuna.create_study(direction='minimize')`: 目的関数を最小化することを目指してOptunaのスタディオブジェクトを作成します。
* `study.optimize(objective, n_trials=10)`: 目的関数を10回の試行で最適化します。

### 最適なパラメータとメトリクスのログ記録

Optunaが最適なハイパーパラメータを見つけた後、これらをMLflowにログ記録します。また、最良の平均二乗誤差（mse）および平方根平均二乗誤差（`rmse`）もログ記録します。

### 追加のメタデータのログ記録

`mlflow.set_tags`を使用して、プロジェクト名、最適化エンジン、モデルファミリー、特徴量セットバージョンなどの追加メタデータをログ記録します。これにより、モデルランの文脈をよりよく分類して理解するのに役立ちます。

### モデル学習とアーティファクトのログ記録

* 最適なハイパーパラメータを使用してXGBoostモデルを学習します。
* 需要予測の相関、特徴量の重要性、残差などのさまざまなプロットが生成され、MLflowにアーティファクトとしてログ記録されます。

### モデルのシリアライゼーションとログ記録

最後に、学習されたモデルは`mlflow.xgboost.log_model`を使用してMLflowにログ記録され、例示入力と追加のメタデータとともに、指定されたアーティファクトパスに保存されます。モデルは指定されたアーティファクトパスに保存され、そのURIが取得されます。

### このブロックが重要な理由

* エンドツーエンドのワークフロー：このコードブロックは、ハイパーパラメータチューニングからモデル評価、ログ記録に至るまでのエンドツーエンドの機械学習ワークフローを表しています。
* 再現性：モデルに関するすべての詳細（ハイパーパラメータ、メトリクス、視覚的診断）がログ記録されるため、実験を完全に再現できます。
* 分析と比較：すべてのデータがMLflowにログ記録されているため、さまざまなランのパフォーマンスを分析し、デプロイ用の最適なモデルを選択することが容易になります。

次のステップでは、ログ記録されたモデルを取得して推論に使用する方法を探ります。

### モデルランに説明的な名前を設定する

モデル学習とハイパーパラメータチューニングを進める前に、MLflowランに説明的な名前を割り当てることが有益です。この名前は、異なるランを追跡、比較、分析しやすくするための人間が読める識別子として機能します。

### ランに名前を付ける重要性：

* 名前による参照：MLflowは各ランに一意の識別キー（run_idなど）を提供しますが、説明的な名前を持つことで、特定のAPIを使用したりMLflow UIをナビゲートする際に直感的に参照できます。
* 明確性と文脈：適切に選ばれたラン名は、特定のランのテストされた仮説や行われた特定の修正についての文脈と理解を助けます。
* 自動名付け：ラン名を指定しない場合、MLflowは独自の楽しい名前を生成します。ただし、これには文脈と明確性が欠ける可能性があります。

### ベストプラクティス：

ランに名前を付ける際は、以下を考慮してください：

1. コード変更との関連性：名前はそのランで行われたコードやパラメータの変更を反映するべきです。
2. 反復的なラン：複数のランを反復的に実行する場合、混乱を避けるために各反復でラン名を更新することが良い考えです。

次のステップでは、親ランに名前を設定します。モデル学習を複数回実行する場合は、明確性のためにラン名を更新することを考慮してください。


```python
run_name = "first_attempt"
```

```python
# Initiate the parent run and call the hyperparameter tuning child run logic
with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):
    # Initialize the Optuna study
    study = optuna.create_study(direction="minimize")

    # Execute the hyperparameter optimization trials.
    # Note the addition of the `champion_callback` inclusion to control our logging
    study.optimize(objective, n_trials=500, callbacks=[champion_callback])

    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_mse", study.best_value)
    mlflow.log_metric("best_rmse", math.sqrt(study.best_value))

    # Log tags
    mlflow.set_tags(
        tags={
            "project": "Apple Demand Project",
            "optimizer_engine": "optuna",
            "model_family": "xgboost",
            "feature_set_version": 1,
        }
    )

    # Log a fit model instance
    model = xgb.train(study.best_params, dtrain)

    # Log the correlation plot
    mlflow.log_figure(figure=correlation_plot, artifact_file="correlation_plot.png")

    # Log the feature importances plot
    importances = plot_feature_importance(model, booster=study.best_params.get("booster"))
    mlflow.log_figure(figure=importances, artifact_file="feature_importances.png")

    # Log the residuals plot
    residuals = plot_residuals(model, dvalid, valid_y)
    mlflow.log_figure(figure=residuals, artifact_file="residuals.png")

    artifact_path = "model"

    mlflow.xgboost.log_model(
        xgb_model=model,
        artifact_path=artifact_path,
        input_example=train_x.iloc[[0]],
        model_format="ubj",
        metadata={"model_data_version": 1},
    )

    # Get the logged model uri so that we can load it from the artifact store
    model_uri = mlflow.get_artifact_uri(artifact_path)
```
```
Initial trial 0 achieved value: 1593256.879424474
Trial 1 achieved value: 1593250.8071099266 with  0.0004% improvement
Trial 2 achieved value: 30990.735000917906 with  5041.0552% improvement
Trial 5 achieved value: 22804.947010998963 with  35.8948% improvement
Trial 7 achieved value: 18232.507769997483 with  25.0785% improvement
Trial 10 achieved value: 15670.64645523901 with  16.3482% improvement
Trial 11 achieved value: 15561.843005727616 with  0.6992% improvement
Trial 21 achieved value: 15144.954353687495 with  2.7527% improvement
Trial 23 achieved value: 14846.71981618512 with  2.0088% improvement
Trial 55 achieved value: 14570.287261018764 with  1.8972% improvement
/Users/benjamin.wilson/repos/mlflow-fork/mlflow/mlflow/models/signature.py:333: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.
  input_schema = _infer_schema(input_ex)
/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
```

<!--
## Understanding the Artifact URI in MLflow

The output ‘mlflow-artifacts:/908436739760555869/c8d64ce51f754eb698a3c09239bcdcee/artifacts/model’ represents a unique Uniform Resource Identifier (URI) for the trained model artifacts within MLflow. This URI is a crucial component of MLflow’s architecture, and here’s why:

### Simplified Access to Model Artifacts

The `model_uri` abstracts away the underlying storage details, providing a consistent and straightforward way to reference model artifacts, regardless of where they are stored. Whether your artifacts are on a local filesystem, in a cloud storage bucket, or on a network mount, the URI remains a consistent reference point.

### Abstraction of Storage Details

MLflow is designed to be storage-agnostic. This means that while you might switch the backend storage from, say, a local directory to an Amazon S3 bucket, the way you interact with MLflow remains consistent. The URI ensures that you don’t need to know the specifics of the storage backend; you only need to reference the model’s URI.

### Associated Information and Metadata

Beyond just the model files, the URI provides access to associated metadata, the model artifact, and other logged artifacts (files and images). This ensures that you have a comprehensive set of information about the model, aiding in reproducibility, analysis, and deployment.

### In Summary

The `model_uri` serves as a consistent, abstracted reference to your model and its associated data. It simplifies interactions with MLflow, ensuring that users don’t need to worry about the specifics of underlying storage mechanisms and can focus on the machine learning workflow.
-->

## MLflowのアーティファクトURIについて理解する

出力された「mlflow-artifacts:/908436739760555869/c8d64ce51f754eb698a3c09239bcdcee/artifacts/model」というURI（Uniform Resource Identifier）は、MLflow内で学習されたモデルアーティファクトの一意の識別子です。このURIはMLflowのアーキテクチャの重要な要素であり、以下の理由から重要です：

### モデルアーティファクトへの簡易アクセス

`model_uri`は、アーティファクトが保存されている場所に関係なく、モデルアーティファクトを参照する一貫したかつ簡単な方法を提供します。アーティファクトがローカルファイルシステムにある場合も、クラウドストレージバケットにある場合も、またはネットワークマウントにある場合も、URIは一貫した参照ポイントとなります。

### ストレージの詳細を抽象化

MLflowはストレージに依存しない設計となっています。つまり、バックエンドストレージをローカルディレクトリからAmazon S3バケットなどに切り替えたとしても、MLflowとの対話方法は一貫しています。URIは、ストレージバックエンドの詳細を知る必要がなく、モデルのURIを参照するだけでよいことを保証します。

### 関連情報およびメタデータ

URIは、モデルファイルだけでなく、関連するメタデータやモデルアーティファクト、その他のログされたアーティファクト（ファイルや画像）にもアクセスを提供します。これにより、モデルに関する包括的な情報セットを提供し、再現性、分析、展開を支援します。

### まとめ

`model_uri`は、モデルおよびそれに関連するデータへの一貫した、抽象化された参照を提供します。これにより、MLflowとのやり取りが簡素化され、ユーザーは基盤となるストレージメカニズムの詳細について心配することなく、機械学習ワークフローに集中できるようになります。

```python
model_uri
```
```
'mlflow-artifacts:/908436739760555869/c28196b19e1843bca7e22f07d796e740/artifacts/model'
```

## Loading the Trained Model with MLflow

With the line:

```python
loaded = mlflow.xgboost.load_model(model_uri)
```

<!--
we’re leveraging MLflow’s native model loader for XGBoost. Instead of using the generic pyfunc loader, which provides a universal Python function interface for models, we’re using the XGBoost-specific loader.

### Benefits of Native Loading:

* Fidelity: Loading the model using the native loader ensures that you’re working with the exact same model object as it was during training. This means all nuances, specifics, and intricacies of the original model are preserved.
* Functionality: With the native model object in hand, you can utilize all of its inherent methods and properties. This allows for more flexibility, especially when you need advanced features or fine-grained control during inference.
* Performance: Using the native model object might offer performance benefits, especially when performing batch inference or deploying the model in environments optimized for the specific machine learning framework.

In essence, by loading the model natively, we ensure maximum compatibility and functionality, allowing for a seamless transition from training to inference.
-->

## XGBoostのネイティブモデルローダーの活用

このセクションでは、汎用的なpyfuncローダーを使用する代わりに、XGBoost専用のローダーを使用してMLflowのネイティブモデルローダーを活用しています。pyfuncローダーは、モデルに対するユニバーサルなPython関数インターフェースを提供しますが、ここではより具体的なローダーを使用しています。

### ネイティブロードのメリット：

* 忠実性：ネイティブローダーを使用してモデルをロードすることで、学習時と全く同じモデルオブジェクトで作業していることが保証されます。これは、元のモデルのすべてのニュアンス、特性、細部が保存されていることを意味します。
* 機能性：ネイティブモデルオブジェクトを使用することで、その固有の方法とプロパティを利用できます。これにより、特に高度な機能が必要な場合や推論中に細かい制御を必要とする場合に、より柔軟性が増します。
* パフォーマンス：ネイティブモデルオブジェクトを使用することで、特にバッチ推論を行う場合や、特定の機械学習フレームワークに最適化された環境でモデルをデプロイする場合に、パフォーマンスの利点が得られる可能性があります。

本質的に、モデルをネイティブにロードすることで、最大の互換性と機能性を保証し、学習から推論へのシームレスな移行を可能にします。


```python
loaded = mlflow.xgboost.load_model(model_uri)
```

<!--
## Example: Batch Inference Using the Loaded Model

After loading the model natively, performing batch inference is straightforward.

In the following cell, we’re going to perform a prediction based on the entire source feature set. Although doing an inference action on the entire training and validation dataset features is of very limited utility in a real-world application, we’ll use our generated synthetic data here to illustrate using the native model for inference.

## Performing Batch Inference and Augmenting Data

In this section, we’re taking our entire dataset and performing batch inference using our loaded XGBoost model. We’ll then append these predictions back into our original dataset to compare, analyze, or further process.

### Steps Explained:

1. Creating a DMatrix: `batch_dmatrix = xgb.DMatrix(X)`: We first convert our features (`X`) into XGBoost’s optimized DMatrix format. This data structure is specifically designed for efficiency and speed in XGBoost.
2. Predictions: `inference = loaded.predict(batch_dmatrix)`: Using the previously loaded model (`loaded`), we perform batch inference on the entire dataset.
3. Creating a New DataFrame: `infer_df = df.copy()`: We create a copy of the original DataFrame to ensure that we’re not modifying our original data.
4. Appending Predictions: `infer_df["predicted_demand"] = inference`: The predictions are then added as a new column, predicted_demand, to this DataFrame.

### Best Practices:

* Always Copy Data: When augmenting or modifying datasets, it’s generally a good idea to work with a copy. This ensures that the original data remains unchanged, preserving data integrity.
* Batch Inference: When predicting on large datasets, using batch inference (as opposed to individual predictions) can offer significant speed improvements.
* DMatrix Conversion: While converting to DMatrix might seem like an extra step, it’s crucial for performance when working with XGBoost. It ensures that predictions are made as quickly as possible.

In the subsequent steps, we can further analyze the differences between the actual demand and our model’s predicted demand, potentially visualizing the results or calculating performance metrics.
-->

## バッチ推論の例：ロードされたモデルを使用したバッチ推論

ネイティブにモデルをロードした後、バッチ推論を行うことは非常に簡単です。

以下のセルでは、全ソース特徴量セットに基づいて予測を行います。実際のアプリケーションでは、学習および検証データセットの全特徴量で推論を行うことはほとんど意味がありませんが、ここでは生成された合成データを使用して、ネイティブモデルでの推論の例を示します。

### バッチ推論の実行とデータの拡張

このセクションでは、ロードされたXGBoostモデルを使用して、データセット全体でバッチ推論を行い、その予測を元のデータセットに再度追加して、比較、分析、またはさらなる処理を行います。

#### ステップの説明：

1. DMatrixの作成: `batch_dmatrix = xgb.DMatrix(X)`: まず、特徴量（`X`）をXGBoostの最適化されたDMatrix形式に変換します。このデータ構造は、XGBoostでの効率と速度を目的として特別に設計されています。
2. 予測の実行: `inference = loaded.predict(batch_dmatrix)`: 以前にロードされたモデル（`loaded`）を使用して、データセット全体に対してバッチ推論を実行します。
3. 新しいDataFrameの作成: `infer_df = df.copy()`: 元のDataFrameのコピーを作成し、元のデータを変更しないようにします。
4. 予測の追加: `infer_df["predicted_demand"] = inference`: 予測は新しい列predicted_demandとしてこのDataFrameに追加されます。

#### ベストプラクティス：

* データのコピーを常に行う: データセットを拡張または変更するときは、コピーで作業することが一般的に良い考えです。これにより、元のデータが変更されずに、データの整合性が保持されます。
* バッチ推論を使用する: 大規模なデータセットに対して予測を行う際には、個々の予測ではなくバッチ推論を使用することで、大幅な速度向上が見込めます。
* DMatrixへの変換: DMatrixへの変換は余分なステップのように思えるかもしれませんが、XGBoostを使用する際にはパフォーマンスに不可欠です。これにより、可能な限り迅速に予測が行われます。

次のステップでは、実際の需要とモデルの予測した需要の違いをさらに分析し、結果を可視化するか、パフォーマンスメトリックを計算することができます。


```python
batch_dmatrix = xgb.DMatrix(X)

inference = loaded.predict(batch_dmatrix)

infer_df = df.copy()

infer_df["predicted_demand"] = inference
```

<!--
## Visualizing the Augmented DataFrame

Below, we display the `infer_df` DataFrame. This augmented dataset now includes both the actual demand (`demand`) and the model’s predictions (`predicted_demand`). By examining this table, we can get a quick sense of how well our model’s predictions align with the actual demand values.
-->

#### 拡張されたDataFrameの可視化

以下では、`infer_df` DataFrameを表示しています。この拡張されたデータセットには、実際の需要（`demand`）とモデルの予測（`predicted_demand`）が含まれています。この表を見ることで、モデルの予測が実際の需要値とどれだけうまく一致しているかをすぐに把握することができます。


```python
infer_df
```
```
date	average_temperature	rainfall	weekend	holiday	price_per_kg	promo	demand	previous_days_demand	competitor_price_per_kg	marketing_intensity	predicted_demand
0	2010-01-14 11:52:20.662955	30.584727	1.199291	0	0	1.726258	0	851.375336	851.276659	1.935346	0.098677	953.708496
1	2010-01-15 11:52:20.662954	15.465069	1.037626	0	0	0.576471	0	906.855943	851.276659	2.344720	0.019318	1013.409973
2	2010-01-16 11:52:20.662954	10.786525	5.656089	1	0	2.513328	0	1108.304909	906.836626	0.998803	0.409485	1152.382446
3	2010-01-17 11:52:20.662953	23.648154	12.030937	1	0	1.839225	0	1099.833810	1157.895424	0.761740	0.872803	1352.879272
4	2010-01-18 11:52:20.662952	13.861391	4.303812	0	0	1.531772	0	983.949061	1148.961007	2.123436	0.820779	1121.233032
...	...	...	...	...	...	...	...	...	...	...	...	...
4995	2023-09-18 11:52:20.659592	21.643051	3.821656	0	0	2.391010	0	1140.210762	1563.064082	1.504432	0.756489	1070.676636
4996	2023-09-19 11:52:20.659591	13.808813	1.080603	0	1	0.898693	0	1285.149505	1189.454273	1.343586	0.742145	1156.580688
4997	2023-09-20 11:52:20.659590	11.698227	1.911000	0	0	2.839860	0	965.171368	1284.407359	2.771896	0.742145	1086.527710
4998	2023-09-21 11:52:20.659589	18.052081	1.000521	0	0	1.188440	0	1368.369501	1014.429223	2.564075	0.742145	1085.064087
4999	2023-09-22 11:52:20.659584	17.017294	0.650213	0	0	2.131694	0	1261.301286	1367.627356	0.785727	0.833140	1047.954102
5000 rows × 12 columns
```

<!--
## Wrapping Up: Reflecting on Our Comprehensive Machine Learning Workflow

Throughout this guide, we embarked on a detailed exploration of an end-to-end machine learning workflow. We began with data preprocessing, delved deeply into hyperparameter tuning with Optuna, leveraged MLflow for structured experiment tracking, and concluded with batch inference.

### Key Takeaways:

* Hyperparameter Tuning with Optuna: We harnessed the power of Optuna to systematically search for the best hyperparameters for our XGBoost model, aiming to optimize its performance.
* Structured Experiment Tracking with MLflow: MLflow’s capabilities shone through as we logged experiments, metrics, parameters, and artifacts. We also explored the benefits of nested child runs, allowing us to logically group and structure our experiment iterations.
* Model Interpretation: Various plots and metrics equipped us with insights into our model’s behavior. We learned to appreciate its strengths and identify potential areas for refinement.
* Batch Inference: The nuances of batch predictions on extensive datasets were explored, alongside methods to seamlessly integrate these predictions back into our primary data.
* Logging Visual Artifacts: A significant portion of our journey emphasized the importance of logging visual artifacts, like plots, to MLflow. These visuals serve as invaluable references, capturing the state of the model, its performance, and any alterations to the feature set that might sway the model’s performance metrics.

By the end of this guide, you should possess a robust understanding of a well-structured machine learning workflow. This foundation not only empowers you to craft effective models but also ensures that each step, from data wrangling to predictions, is transparent, reproducible, and efficient.

We’re grateful you accompanied us on this comprehensive journey. The practices and insights gleaned will undoubtedly be pivotal in all your future machine learning endeavors!
-->

## 総括: 包括的な機械学習ワークフローについての振り返り

このガイドを通じて、データの前処理から始まり、Optunaを用いたハイパーパラメータチューニング、MLflowを活用した体系的な実験追跡、バッチ推論に至るまで、エンドツーエンドの機械学習ワークフローを詳細に探求しました。

### 主な学び:

* **Optunaによるハイパーパラメータチューニング**: Optunaの力を利用して、XGBoostモデルの最適なハイパーパラメータを体系的に探索し、パフォーマンスを最大化しました。
* **MLflowによる体系的な実験追跡**: MLflowの機能が輝く中で、実験、メトリクス、パラメータ、そしてアーティファクトを記録しました。また、論理的にグループ化して実験イテレーションを構造化するために、ネストされた子実行の利点を探りました。
* **モデル解釈**: 様々なプロットとメトリクスによって、モデルの振る舞いについてのインサイトを得ました。その強みを理解し、改善が必要な領域を特定しました。
* **バッチ推論**: 大規模データセットに対するバッチ予測のニュアンスを探り、これらの予測を主要データにシームレスに統合する方法を並行して検討しました。
* **視覚的アーティファクトの記録**: プロットなどの視覚的アーティファクトをMLflowに記録する重要性を強調しました。これらの視覚資料は、モデルの状態、その性能、およびモデルの性能指標に影響を与える可能性のある特徴量セットの変更を記録する貴重な参照として機能します。

このガイドを終えた時点で、よく構造化された機械学習ワークフローを理解する堅牢な基盤を持つべきです。この基盤は、効果的なモデルを作成するだけでなく、データ処理から予測に至るまでの各ステップが透明で、再現可能で、効率的であることを保証します。

この包括的な旅に同行していただき、感謝します。この経験から得られた実践とインサイトは、あなたの将来の機械学習の取り組みにおいて間違いなく重要なものとなるでしょう！

