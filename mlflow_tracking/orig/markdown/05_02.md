<!--
# MLflow Tracking APIs

* [https://mlflow.org/docs/latest/tracking/tracking-api.html](https://mlflow.org/docs/latest/tracking/tracking-api.html)

MLflow Tracking provides Python, R, Java, or REST API to log your experiment data and models.

## Auto Logging

Auto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements but just a single mlflow.autolog() call at the top of your ML code. See auto logging documentation for supported libraries and how to use autolog APIs with each of them.
-->

# MLflow トラッキングAPI

* [https://mlflow.org/docs/latest/tracking/tracking-api.html](https://mlflow.org/docs/latest/tracking/tracking-api.html)

MLflow Tracking は、実験データやモデルをログするためのPython、R、Java、REST APIを提供しています。

## オートロギング

オートロギングは、明示的なログステートメントを必要とせずに、MLコードの先頭に単一の `mlflow.autolog()` 呼び出しを行うだけで、メトリクス、パラメータ、およびモデルをログできる強力な機能です。サポートされているライブラリと各ライブラリでのautolog APIの使用方法については、オートロギングのドキュメントを参照してください。


```python
import mlflow

mlflow.autolog()

# Your training code...
```

<!--
## Manual Logging

Alternatively, you log MLflow metrics by adding log methods in your ML code. For example:
-->

## マニュアルログ

代替手段として、MLコード内にログメソッドを追加することでMLflowメトリクスをログできます。例えば：

```Python
with mlflow.start_run():
    for epoch in range(0, 3):
        mlflow.log_metric(key="quality", value=2 * epoch, step=epoch)
```

<!--
## Logging functions

Here are the full list of logging functions provided by the Tracking API (Python). Note that Java and R APIs provide similar but limited set of logging functions.

`mlflow.set_tracking_uri()` connects to a tracking URI. You can also set the `MLFLOW_TRACKING_URI` environment variable to have MLflow find a URI from there. In both cases, the URI can either be a HTTP/HTTPS URI for a remote server, a database connection string, or a local path to log data to a directory. The URI defaults to mlruns.

`mlflow.get_tracking_uri()` returns the current tracking URI.

`mlflow.create_experiment()` creates a new experiment and returns the experiment ID. Runs can be launched under the experiment by passing the experiment ID to `mlflow.start_run` or by setting the active experiment with `mlflow.set_experiment()`, passing in the experiment ID of the created experiment.

`mlflow.set_experiment()` sets an experiment as active and returns the active experiment instance. If you do not specify an experiment in `mlflow.start_run()`, new runs are launched under this experiment.

Note

If the experiment being set by name does not exist, a new experiment will be created with the given name. After the experiment has been created, it will be set as the active experiment. On certain platforms, such as Databricks, the experiment name must be an absolute path, e.g. `"/Users/<username>/my-experiment"`.

`mlflow.start_run()` returns the currently active run (if one exists), or starts a new run and returns a mlflow.ActiveRun object usable as a context manager for the current run. You do not need to call `start_run` explicitly: calling one of the logging functions with no active run automatically starts a new one.

Note

If the argument run_name is not set within `mlflow.start_run()`, a unique run name will be generated for each run.

`mlflow.end_run() `ends the currently active run, if any, taking an optional run status.

`mlflow.active_run()` returns a `mlflow.entities.Run` object corresponding to the currently active run, if any. Note: You cannot access currently-active run attributes (parameters, metrics, etc.) through the run returned by `mlflow.active_run`. In order to access such attributes, use the MlflowClient as follows:
-->

## ロギング機能

以下は、トラッキングAPI（Python）によって提供されるロギング機能の完全なリストです。JavaおよびRのAPIも類似したが限定されたセットのロギング機能を提供していることに注意してください。

`mlflow.set_tracking_uri()` はトラッキングURIに接続します。`MLFLOW_TRACKING_URI` 環境変数を設定して、そこからURIを見つけるようにMLflowに指示することもできます。どちらの場合も、URIはリモートサーバ用のHTTP/HTTPS URI、データベース接続文字列、またはディレクトリにデータをログするためのローカルパスのいずれかにすることができます。URIのデフォルトはmlrunsです。

`mlflow.get_tracking_uri()` は現在のトラッキングURIを返します。

`mlflow.create_experiment()` は新しい実験を作成し、実験IDを返します。`mlflow.start_run`に実験IDを渡すか、`mlflow.set_experiment()`でアクティブな実験を設定し、作成した実験の実験IDを渡すことにより、その実験の下で実行を開始できます。

`mlflow.set_experiment()` は実験をアクティブに設定し、アクティブな実験インスタンスを返します。`mlflow.start_run()`で実験を指定しない場合、新しい実行はこの実験の下で開始されます。

注記

名前で設定される実験が存在しない場合、指定された名前で新しい実験が作成されます。実験が作成された後、それがアクティブな実験として設定されます。Databricksなどの特定のプラットフォームでは、実験名は絶対パスである必要があります（例：`"/Users/<username>/my-experiment"`）。

`mlflow.start_run()` は現在アクティブな実行（存在する場合）を返すか、新しい実行を開始して、現在の実行のためのコンテキストマネージャとして使用可能なmlflow.ActiveRunオブジェクトを返します。明示的に`start_run`を呼び出す必要はありません：アクティブな実行がない場合にロギング機能のいずれかを呼び出すと、新しい実行が自動的に開始されます。

注記

`mlflow.start_run()`内でrun_name引数が設定されていない場合、各実行に対して一意の実行名が生成されます。

`mlflow.end_run()` は現在アクティブな実行（存在する場合）を終了し、オプションで実行状態を取ります。

`mlflow.active_run()`は、現在アクティブな実行がある場合、その実行に対応する`mlflow.entities.Run`オブジェクトを返します。注記：`mlflow.active_run`で返される実行を通じて現在アクティブな実行の属性（パラメータ、メトリックなど）にアクセスすることはできません。そのような属性にアクセスするには、次のようにMlflowClientを使用します：



```python
client = mlflow.MlflowClient()
data = client.get_run(mlflow.active_run().info.run_id).data
```

<!--
`mlflow.last_active_run()` returns a `mlflow.entities.Run` object corresponding to the currently active run, if any. Otherwise, it returns a `mlflow.entities.Run` object corresponding the last run started from the current Python process that reached a terminal status (i.e. FINISHED, FAILED, or KILLED).

`mlflow.get_parent_run()` returns a mlflow.entities.Run object corresponding to the parent run for the given run id, if one exists. Otherwise, it returns None.

`mlflow.log_param()` logs a single key-value param in the currently active run. The key and value are both strings. Use `mlflow.log_params()` to log multiple params at once.

`mlflow.log_metric()` logs a single key-value metric. The value must always be a number. MLflow remembers the history of values for each metric. Use `mlflow.log_metrics()` to log multiple metrics at once.

`mlflow.log_input()` logs a single `mlflow.data.dataset.Dataset` object corresponding to the currently active run. You may also log a dataset context string and a dict of key-value tags.

`mlflow.set_tag()` sets a single key-value tag in the currently active run. The key and value are both strings. Use `mlflow.set_tags()` to set multiple tags at once.

`mlflow.log_artifact()` logs a local file or directory as an artifact, optionally taking an `artifact_path` to place it in within the run’s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way.

`mlflow.log_artifacts()` logs all the files in a given directory as artifacts, again taking an optional `artifact_path`.

`mlflow.get_artifact_uri()` returns the URI that artifacts from the current run should be logged to.
-->

`mlflow.last_active_run()`は、現在アクティブな実行がある場合はその実行に対応する`mlflow.entities.Run`オブジェクトを返します。そうでない場合は、現在のPythonプロセスから開始され最後に終了状態（FINISHED、FAILED、またはKILLED）に達した実行に対応する`mlflow.entities.Run`オブジェクトを返します。

`mlflow.get_parent_run()`は、与えられた実行IDの親実行に対応するmlflow.entities.Runオブジェクトを返します。親実行が存在する場合のみ、それ以外の場合はNoneを返します。

`mlflow.log_param()`は現在アクティブな実行に単一のキー値パラメータをログします。キーと値はどちらも文字列です。一度に複数のパラメータをログするには`mlflow.log_params()`を使用します。

`mlflow.log_metric()`は単一のキー値メトリックをログします。値は常に数値でなければなりません。MLflowは各メトリックの値の履歴を記憶します。一度に複数のメトリックをログするには`mlflow.log_metrics()`を使用します。

`mlflow.log_input()`は、現在アクティブな実行に対応する単一の`mlflow.data.dataset.Dataset`オブジェクトをログします。データセットのコンテキスト文字列とキー値タグの辞書もログすることができます。

`mlflow.set_tag()`は現在アクティブな実行に単一のキー値タグを設定します。キーと値はどちらも文字列です。一度に複数のタグを設定するには`mlflow.set_tags()`を使用します。

`mlflow.log_artifact()`は、ローカルファイルまたはディレクトリをアーティファクトとしてログし、必要に応じてアーティファクトを配置する`artifact_path`を指定することができます。実行アーティファクトはディレクトリに整理されるため、この方法でアーティファクトをディレクトリに配置できます。

`mlflow.log_artifacts()`は指定されたディレクトリ内のすべてのファイルをアーティファクトとしてログし、再びオプションで`artifact_path`を取ります。

`mlflow.get_artifact_uri()`は、現在の実行からログされるべきアーティファクトのURIを返します。


<!--
## Tracking Tips

### Logging with explicit step and timestamp

The `log` methods support two alternative methods for distinguishing metric values on the x-axis: `timestamp` and `step` (number of training iterations, number of epochs, and so on). By default, `timestamp` is set to the current time and `step` is set to 0. You can override these values by passing `timestamp` and `step` arguments to the `log` methods. For example:
-->

## トラッキングのヒント

### 明確なステップとタイムスタンプでのログ記録

`log`メソッドは、x軸上でメトリック値を区別するための2つの代替方法、`timestamp`（タイムスタンプ）と`step`（学習の反復回数、エポック数など）をサポートしています。デフォルトでは、`timestamp`は現在の時刻に、`step`は0に設定されています。これらの値を`log`メソッドに`timestamp`と`step`の引数を渡すことで上書きすることができます。例えば：

```python
mlflow.log_metric(key="train_loss", value=train_loss, step=epoch, timestamp=now)
```

<!--
`step` has the following requirements and properties:

Must be a valid 64-bit integer value.

* Can be negative.
* Can be out of order in successive write calls. For example, (1, 3, 2) is a valid sequence.
* Can have “gaps” in the sequence of values specified in successive write calls. For example, (1, 5, 75, -20) is a valid sequence.
* If you specify both a timestamp and a step, metrics are recorded against both axes independently.


## Organizing Runs in Experiments

MLflow allows you to group runs under experiments, which can be useful for comparing runs intended to tackle a particular task. You can create experiments via multiple way - MLflow UI, the Command-Line Interface (mlflow experiments), or the mlflow.create_experiment() Python API. You can pass the experiment name for an individual run using the CLI (for example, mlflow run ... --experiment-name [name]) or the MLFLOW_EXPERIMENT_NAME environment variable. Alternatively, you can use the experiment ID instead, via the --experiment-id CLI flag or the MLFLOW_EXPERIMENT_ID environment variable.
-->

`step`は以下の要件と特性を持っています：

64ビット整数の有効な値でなければなりません。

* 負の値でもかまいません。
* 連続する書き込み呼び出しで順序が前後しても構いません。例えば、(1, 3, 2)は有効なシーケンスです。
* 連続する書き込み呼び出しで指定された値のシーケンスに「ギャップ」があっても構いません。例えば、(1, 5, 75, -20)は有効なシーケンスです。
* タイムスタンプとステップの両方を指定した場合、メトリクスは両軸に対して独立して記録されます。

## 実験で実行を整理する

MLflowは実行を実験の下にグループ化することを許可しており、特定のタスクに取り組むことを意図した実行を比較するのに役立ちます。実験はMLflow UI、コマンドラインインターフェース（`mlflow experiments`）、または`mlflow.create_experiment()` Python APIを通じて複数の方法で作成できます。個々の実行に対して実験名をCLIを使って指定することができます（例：`mlflow run ... --experiment-name [name]`）、または`MLFLOW_EXPERIMENT_NAME`環境変数を使用します。代わりに、`--experiment-id` CLIフラグまたは`MLFLOW_EXPERIMENT_ID`環境変数を使用して実験IDを指定することもできます。


```python
# Set the experiment via environment variables
export MLFLOW_EXPERIMENT_NAME=fraud-detection

mlflow experiments create --experiment-name fraud-detection
```

```python
# Launch a run. The experiment is inferred from the MLFLOW_EXPERIMENT_NAME environment
# variable, or from the --experiment-name parameter passed to the MLflow CLI (the latter
# taking precedence)
with mlflow.start_run():
    mlflow.log_param("a", 1)
    mlflow.log_metric("b", 2)
```

<!--
## Creating Child Runs

You can also create multiple runs inside a single run. This is useful for scenario like hyperparameter tuning, cross-validation folds, where you need another level of organization within an experiment. You can create child runs by passing parent_run_id to mlflow.`start_run()` function. For example:
-->

## 子実行の作成

単一の実行内に複数の実行を作成することもできます。これは、ハイパーパラメータチューニングやクロスバリデーションのフォールドなど、実験内にさらなる整理レベルが必要なシナリオに役立ちます。子実行は、`mlflow.start_run()` 関数に `parent_run_id` を渡すことで作成できます。例えば：

```python
# Start parent run
with mlflow.start_run() as parent_run:
    param = [0.01, 0.02, 0.03]

    # Create a child run for each parameter setting
    for p in param:
        with mlflow.start_run(nested=True) as child_run:
            mlflow.log_param("p", p)
            ...
            mlflow.log_metric("val_loss", val_loss)
```

<!--
You can fetch all child runs under a parent run using tags.

## Launching Multiple Runs in One Program

Sometimes you want to launch multiple MLflow runs in the same program: for example, maybe you are performing a hyperparameter search locally or your experiments are just very fast to run. The way to do this depends on whether you want to run them sequentially or in parallel.

### Sequential Runs

Running multiple runs one-by-one is easy to do because the ActiveRun object returned by mlflow.`start_run()` is a Python context manager. You can “scope” each run to just one block of code as follows:
-->

子実行を親実行の下で全て取得するにはタグを使用します。

## 一つのプログラムで複数の実行を起動する

時には、同じプログラムで複数のMLflow実行を起動したい場合があります。例えば、ローカルでハイパーパラメータ探索を行っているか、実験が非常に迅速に実行される場合などです。これを実行する方法は、それらを順次実行するか並行して実行するかによります。

### 順次実行

複数の実行を一つずつ実行するのは簡単です。これは、`mlflow.start_run()`によって返される`ActiveRun`オブジェクトがPythonのコンテキストマネージャーだからです。次のように、各実行をコードの一つのブロックに「スコープ」することができます：

```python
# First run
with mlflow.start_run():
    mlflow.log_param("x", 1)
    mlflow.log_metric("y", 2)
    ...

# Another run
with mlflow.start_run():
    ...
```

<!--
The run remains open throughout the with statement, and is automatically closed when the statement exits, even if it exits due to an exception.

### Parallel Runs

MLflow also supports running multiple runs in parallel using multiprocessing or multi threading.

Multi-processing is straightforward: just call mlflow.`start_run()` in a separate process and it creates a new run for each. For example:
-->

`with`ステートメントの間、実行は開かれたまま保持され、ステートメントが終了すると自動的に閉じられます。これは例外による終了の場合でも同様です。

### 並行実行

MLflowは、マルチプロセッシングやマルチスレッディングを使用して複数の実行を並行して行うこともサポートしています。

マルチプロセッシングは直感的です：別のプロセスで`mlflow.start_run()`を呼び出すだけで、それぞれのプロセスで新しい実行が作成されます。例えば：


```python
import mlflow
import multiprocessing as mp


def train_model(params):
    with mlflow.start_run():
        mlflow.log_param("p", params)
        ...


if __name__ == "__main__":
    params = [0.01, 0.02, ...]
    pool = mp.Pool(processes=4)
    pool.map(train_model, params)
```


<!--
Multi-threading is a bit more complicated because MLflow uses a global state to keep track of the currently active run i.e. having multiple active runs in the same process may cause data corruption. However, you can avoid this issue and use multi threading by using Child Runs. You can start child runs in each thread by passing nested=True to mlflow.`start_run()` as described in the previous section. For example:
-->

マルチスレッディングはもう少し複雑です。MLflowはグローバル状態を使用して現在アクティブな実行を追跡するため、同じプロセス内で複数のアクティブな実行があるとデータの破損が発生する可能性があります。しかし、前のセクションで説明したように、`mlflow.start_run()`に`nested=True`を渡して子実行を各スレッドで開始することで、この問題を回避しマルチスレッディングを使用することができます。例えば：


```python
import mlflow
import threading


def train_model(params):
    # Create a child run by passing nested=True
    with mlflow.start_run(nested=True):
        mlflow.log_param("p", params)
        ...


if __name__ == "__main__":
    params = [0.01, 0.02, ...]
    threads = []
    for p in params:
        t = threading.Thread(target=train_model, args=(p,))
        threads.append(t)
        t.start()

    for t in threads:
        t.join()
```

<!--
Also here is the full example of hyperparameter tuning using child runs with multi threading.

## Adding Tags to Runs

You can annotate runs with arbitrary tags to organize them into groups. This allows you to easily filter and search Runs in Tracking UI by using filter expression.

### System Tags

Tag keys that start with mlflow. are reserved for internal use. The following tags are set automatically by MLflow, when appropriate:

Note

mlflow.note.content is an exceptional case where the tag is not set automatically and can be overridden by the user to include additional information about the run.

Key

Description

`mlflow.note.content`

A descriptive note about this run. This reserved tag is not set automatically and can be overridden by the user to include additional information about the run. The content is displayed on the run’s page under the Notes section.

`mlflow.parentRunId`

The ID of the parent run, if this is a nested run.

`mlflow.user`

Identifier of the user who created the run.

`mlflow.source.type`

Source type. Possible values: "NOTEBOOK", "JOB", "PROJECT", "LOCAL", and "UNKNOWN"

`mlflow.source.name`

Source identifier (e.g., GitHub URL, local Python filename, name of notebook)

`mlflow.source.git.commit`

Commit hash of the executed code, if in a git repository. This tag is only logged when the code is executed as a Python script like python train.py or as a project. If the code is executed in a notebook, this tag is not logged.

`mlflow.source.git.branch`

Name of the branch of the executed code, if in a git repository. This tag is only logged within the context of MLflow Projects and MLflow Recipe.

`mlflow.source.git.repoURL`

URL that the executed code was cloned from. This tag is only logged within the context of MLflow Projects and MLflow Recipe.

`mlflow.project.env`

The runtime context used by the MLflow project. Possible values: "docker" and "conda".

`mlflow.project.entryPoint`

Name of the project entry point associated with the current run, if any.

`mlflow.docker.image.name`

Name of the Docker image used to execute this run.

`mlflow.docker.image.id`

ID of the Docker image used to execute this run.

`mlflow.log-model.history`

Model metadata collected by log-model calls. Includes the serialized form of the MLModel model files logged to a run, although the exact format and information captured is subject to change.
-->

また、マルチスレッディングを使用して子実行でのハイパーパラメータチューニングの完全な例もここに示します。

## 実行にタグを追加する

任意のタグを実行に注釈付けしてグループ化することができます。これにより、フィルタ式を使用してトラッキングUIで実行を簡単にフィルタリングおよび検索できます。

### システムタグ

`mlflow.`で始まるタグキーは内部使用のために予約されています。以下のタグは、適切な場合にMLflowによって自動的に設定されます：

注記

`mlflow.note.content`は、タグが自動的に設定されず、ユーザーが実行に関する追加情報を含めるために上書きできる例外的なケースです。

キー | 説明
--- | ---
`mlflow.note.content` | この実行についての説明的なメモ。この予約タグは自動的に設定されず、ユーザーが実行に関する追加情報を含めるために上書きできます。内容は実行のページのノートセクションに表示されます。
`mlflow.parentRunId` | これがネストされた実行である場合の親実行のID。
`mlflow.user` | 実行を作成したユーザーの識別子。
`mlflow.source.type` | ソースタイプ。可能な値: "NOTEBOOK", "JOB", "PROJECT", "LOCAL", "UNKNOWN"
`mlflow.source.name` | ソース識別子（例：GitHubのURL、ローカルのPythonファイル名、ノートブックの名前）
`mlflow.source.git.commit` | ギットリポジトリにある実行されたコードのコミットハッシュ。このタグは、Pythonスクリプト（例：python train.py）またはプロジェクトとしてコードが実行された場合にのみログされます。ノートブックでコードが実行された場合、このタグはログされません。
`mlflow.source.git.branch` | 実行されたコードのブランチ名（ギットリポジトリ内）。このタグはMLflowプロジェクトおよびMLflowレシピのコンテキスト内でのみログされます。
`mlflow.source.git.repoURL` | 実行されたコードがクローンされたURL。このタグはMLflowプロジェクトおよびMLflowレシピのコンテキスト内でのみログされます。
`mlflow.project.env` | MLflowプロジェクトによって使用されたランタイムコンテキスト。可能な値: "docker"、"conda"。
`mlflow.project.entryPoint` | 現在の実行に関連付けられたプロジェクトのエントリポイントの名前（ある場合）。
`mlflow.docker.image.name` | この実行を実行するために使用されたDockerイメージの名前。
`mlflow.docker.image.id` | この実行を実行するために使用されたDockerイメージのID。
`mlflow.log-model.history` | log-modelコールによって収集されたモデルメタデータ。実行にログされたMLModelモデルファイルのシリアライズされた形式を含みますが、正確な形式と情報は変更される可能性があります。


<!--
### Custom Tags

The `MlflowClient.set_tag()` function lets you add custom tags to runs. A tag can only have a single unique value mapped to it at a time. For example:
-->

### カスタムタグ

`MlflowClient.set_tag()`関数を使用して、実行にカスタムタグを追加することができます。タグは一度に1つのユニークな値のみを持つことができます。例えば：

```python
client.set_tag(run.info.run_id, "tag_key", "tag_value")
```

<!--
## Get MLflow Run instance from autologged results

In some cases, you may want to access the MLflow Run instance associated with the autologged results, similarly to you can get the run context with `mlflow.start_run()` You can access the most recent autolog run through the mlflow.`last_active_run()` function. Here’s a short sklearn autolog example that makes use of this function:
-->

## 自動ログの結果からMLflow実行インスタンスを取得

場合によっては、`mlflow.start_run()`で実行コンテキストを取得するように、自動ログの結果に関連付けられたMLflow実行インスタンスにアクセスしたいことがあります。最新の自動ログ実行には、`mlflow.last_active_run()`関数を通じてアクセスできます。以下は、この機能を使用した短いsklearn自動ログの例です：

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

mlflow.autolog()

db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

# Create and train models.
rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
rf.fit(X_train, y_train)

# Use the model to make predictions on the test dataset.
predictions = rf.predict(X_test)
autolog_run = mlflow.last_active_run()
print(autolog_run)
# <Run:
#    data=<RunData:
#        metrics={'accuracy': 0.0},
#        params={'n_estimators': '100', 'max_depth': '6', 'max_features': '3'},
#        tags={'estimator_class': 'sklearn.ensemble._forest.RandomForestRegressor', 'estimator_name': 'RandomForestRegressor'}
#    >,
#    info=<RunInfo:
#        artifact_uri='file:///Users/andrew/Code/mlflow/mlruns/0/0c0b.../artifacts',
#        end_time=163...0,
#        run_id='0c0b...',
#        run_uuid='0c0b...',
#        start_time=163...0,
#        status='FINISHED',
#        user_id='ME'>
#    >
```
