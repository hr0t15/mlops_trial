<!--
# Leveraging Child Runs in MLflow for Hyperparameter Tuning

* [https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.html](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.html)

In the world of machine learning, the task of hyperparameter tuning is central to model optimization. This process involves performing multiple runs with varying parameters to identify the most effective combination, ultimately enhancing model performance. However, this can lead to a large volume of runs, making it challenging to track, organize, and compare these experiments effectively.

MLflow incorporates the ability to simplify the large-data-volume issue by offering a structured approach to manage this complexity. In this notebook, we will explore the concept of Parent and Child Runs in MLflow, a feature that provides a hierarchical structure to organize runs. This hierarchy allows us to bundle a set of runs under a parent run, making it much more manageable and intuitive to analyze and compare the results of different hyperparameter combinations. This structure proves to be especially beneficial in understanding and visualizing the outcomes of hyperparameter tuning processes.

Throughout this notebook, we will: - Understand the usage and benefits of parent and child runs in MLflow. - Walk through a practical example demonstrating the organization of runs without and with child runs. - Observe how child runs aid in effectively tracking and comparing the results of different parameter combinations. - Demonstrate a further refinement by having the parent run maintain the state of the best conditions from child run iterations.
-->

# MLflowにおける子実行の活用：ハイパーパラメータチューニング

* [https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.html](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.html)

機械学習において、モデル最適化の中心的な作業はハイパーパラメータチューニングです。このプロセスには、最も効果的なパラメータの組み合わせを特定するために、異なるパラメータで複数の実行を行うことが含まれます。これにより、モデルのパフォーマンスが向上します。しかし、これは多くの実行を発生させ、これらの実験を効果的に追跡、整理、比較することを困難にします。

MLflowは、この大量データ問題を簡素化するために、この複雑さを管理する構造的なアプローチを提供します。このノートブックでは、実行を階層的に整理する機能であるMLflowの親子実行のコンセプトを探ります。この階層構造により、一連の実行を親実行の下に束ねることができ、異なるハイパーパラメータの組み合わせの結果を分析し比較することがより管理しやすく直感的になります。特に、ハイパーパラメータチューニングプロセスの結果を理解し可視化するのに有益です。

このノートブックを通じて、以下を行います:
- MLflowにおける親子実行の使用法と利点を理解する。
- 子実行なしで実行を整理する実用的な例を通じて説明する。
- 子実行が異なるパラメータの組み合わせの結果を効果的に追跡し比較するのにどのように役立つかを観察する。
- 親実行が子実行のイテレーションから最良の条件の状態を維持することによるさらなる洗練を示す。

<!--
## Starting Without Child Runs

Before diving into the structured world of parent and child runs, let’s begin by observing the scenario without utilizing child runs in MLflow. In this section, we perform multiple runs with different parameters and metrics without associating them as child runs of a parent run.

Below is the code executing five hyperparameter tuning runs. These runs are not organized as child runs, and hence, each run is treated as an independent entity in MLflow. We will observe the challenges this approach poses in tracking and comparing runs, setting the stage for the introduction of child runs in the subsequent sections.

After running the above code, you can proceed to the MLflow UI to view the logged runs. Observing the organization (or lack thereof) of these runs will help in appreciating the structured approach offered by using child runs, which we will explore in the next sections of this notebook.
-->

## 子実行なしでの開始

親子実行の構造化された世界に飛び込む前に、MLflowで子実行を使用しないシナリオから始めましょう。このセクションでは、親実行の子実行として関連付けられていない異なるパラメータとメトリクスで複数の実行を行います。

以下のコードは、子実行として整理されていないため、それぞれの実行がMLflowで独立したエンティティとして扱われる五つのハイパーパラメータチューニング実行を実行します。このアプローチが実行の追跡と比較にどのような課題をもたらすかを観察し、次のセクションで子実行の導入につなげます。

上記のコードを実行した後、MLflow UIに進んでログ記録された実行を確認できます。これらの実行の整理（またはその欠如）を観察することで、次のセクションで探る子実行を使用した構造化アプローチの利点を評価する手助けとなります。


```python
import random
from functools import partial
from itertools import starmap

from more_itertools import consume

import mlflow
```

```python
# Define a function to log parameters and metrics
def log_run(run_name, test_no):
    with mlflow.start_run(run_name=run_name):
        mlflow.log_param("param1", random.choice(["a", "b", "c"]))
        mlflow.log_param("param2", random.choice(["d", "e", "f"]))
        mlflow.log_metric("metric1", random.uniform(0, 1))
        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))


# Generate run names
def generate_run_names(test_no, num_runs=5):
    return (f"run_{i}_test_{test_no}" for i in range(num_runs))


# Execute tuning function
def execute_tuning(test_no):
    # Partial application of the log_run function
    log_current_run = partial(log_run, test_no=test_no)
    # Generate run names and apply log_current_run function to each run name
    runs = starmap(log_current_run, ((run_name,) for run_name in generate_run_names(test_no)))
    # Consume the iterator to execute the runs
    consume(runs)


# Set the tracking uri and experiment
mlflow.set_tracking_uri("http://localhost:8080")
mlflow.set_experiment("No Child Runs")

# Execute 5 hyperparameter tuning runs
consume(starmap(execute_tuning, ((x,) for x in range(5))))
```

<!--
## Iterative development simulation

It is very rare that a tuning run will be conducted in isolation. Typically, we will run many iterations of combinations of parameters, refining our search space to achieve the best possible potential results in the shortest amount of execution time.

In order to arrive at this limited set of selection parameters ranges and conditions, we will be executing many such tests.
-->

## イテレーションによる開発のシミュレーション

ハイパーパラメータのチューニングが単独で実行されることは非常にまれです。通常、複数のパラメータの組み合わせによる多くのイテレーションを実行し、最短の実行時間で最適な潜在的結果を達成するために検索範囲を絞り込みます。

この限定されたパラメータ範囲と条件のセットに到達するために、多くのテストを実行することになります。


```python
# What if we need to run this again?
consume(starmap(execute_tuning, ((x,) for x in range(5))))
```

<!--
## Using Child Runs for Improved Organization

As we proceed, the spotlight now shifts to the utilization of Child Runs in MLflow. This feature brings forth an organized structure, inherently solving the challenges we observed in the previous section. The child runs are neatly nested under a parent run, providing a clear, hierarchical view of all the runs, making it exceptionally convenient to analyze and compare the outcomes.

### Benefits of Using Child Runs:

* Structured View: The child runs, grouped under a parent run, offer a clean and structured view in the MLflow UI.
* Efficient Filtering: The hierarchical organization facilitates efficient filtering and selection, enhancing the usability of the MLflow UI and search APIs.
* Distinct Naming: Utilizing visually distinct naming for runs aids in effortless identification and selection within the UI.

In this section, the code is enhanced to use child runs. Each execute_tuning function call creates a parent run, under which multiple child runs are nested. These child runs are performed with different parameters and metrics. Additionally, we incorporate tags to further enhance the search and filter capabilities in MLflow.

Notice the inclusion of the `nested=True` parameter in the `mlflow.start_run()` function, indicating the creation of a child run. The addition of tags, using the `mlflow.set_tag()` function, provides an extra layer of information, useful for filtering and searching runs effectively.

Let’s dive into the code and observe the seamless organization and enhanced functionality brought about by the use of child runs in MLflow.
-->

## 子ランを使用した改善された整理

進行するにつれて、MLflowでの子ランの利用に焦点が移ります。この機能は、組織的な構造を提供し、前セクションで観察した課題を根本的に解決します。子ランは親ランの下にきれいにネストされ、すべてのランの明確な階層ビューを提供し、結果の分析と比較を非常に便利にします。

### 子ランの使用の利点：

* 構造化されたビュー：親ランの下にグループ化された子ランは、MLflow UIで清潔で構造化されたビューを提供します。
* 効率的なフィルタリング：階層的な組織は効率的なフィルタリングと選択を促進し、MLflow UIおよび検索APIの使用性を向上させます。
* 明確な命名：視覚的に区別される命名を使用することで、UI内での識別と選択が容易になります。

このセクションでは、コードが強化されて子ランを使用するようになります。各 `execute_tuning` 関数呼び出しは、親ランを作成し、その下に複数の子ランがネストされます。これらの子ランは、異なるパラメータとメトリクスで実行されます。さらに、MLflowでの検索とフィルタリング機能を強化するために、タグを追加しています。

`mlflow.start_run()` 関数での `nested=True` パラメータの含まれることに注意してください。これは、子ランの作成を示しています。`mlflow.set_tag()` 関数を使用したタグの追加は、ランを効果的にフィルタリングして検索するための追加の情報層を提供します。

MLflowで子ランを使用することによるシームレスな組織と機能の向上を観察するために、コードに飛び込みましょう。

```python
# Define a function to log parameters and metrics and add tag
# logging for search_runs functionality
def log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):
    with mlflow.start_run(run_name=run_name, nested=True):
        mlflow.log_param("param1", random.choice(param1_choices))
        mlflow.log_param("param2", random.choice(param2_choices))
        mlflow.log_metric("metric1", random.uniform(0, 1))
        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))
        mlflow.set_tag("test_identifier", tag_ident)


# Generate run names
def generate_run_names(test_no, num_runs=5):
    return (f"run_{i}_test_{test_no}" for i in range(num_runs))


# Execute tuning function, allowing for param overrides,
# run_name disambiguation, and tagging support
def execute_tuning(
    test_no, param1_choices=("a", "b", "c"), param2_choices=("d", "e", "f"), test_identifier=""
):
    ident = "default" if not test_identifier else test_identifier
    # Use a parent run to encapsulate the child runs
    with mlflow.start_run(run_name=f"parent_run_test_{ident}_{test_no}"):
        # Partial application of the log_run function
        log_current_run = partial(
            log_run,
            test_no=test_no,
            param1_choices=param1_choices,
            param2_choices=param2_choices,
            tag_ident=ident,
        )
        mlflow.set_tag("test_identifier", ident)
        # Generate run names and apply log_current_run function to each run name
        runs = starmap(log_current_run, ((run_name,) for run_name in generate_run_names(test_no)))
        # Consume the iterator to execute the runs
        consume(runs)


# Set the tracking uri and experiment
mlflow.set_tracking_uri("http://localhost:8080")
mlflow.set_experiment("Nested Child Association")

# Define custom parameters
param_1_values = ["x", "y", "z"]
param_2_values = ["u", "v", "w"]

# Execute hyperparameter tuning runs with custom parameter choices
consume(starmap(execute_tuning, ((x, param_1_values, param_2_values) for x in range(5))))
```

<!--
## Tailoring the Hyperparameter Tuning Process

In this segment, we are taking a step further in our iterative process of hyperparameter tuning. Observe the execution of additional hyperparameter tuning runs, where we introduce custom parameter choices and a unique identifier for tagging.

### What Are We Doing?

Custom Parameter Choices: We are now employing different parameter values (`param_1_values` as `["x", "y", "z"]` and `param_2_values` as `["u", "v", "w"]`) for the runs.

Unique Identifier for Tagging: A distinct identifier (ident) is used for tagging, which provides an easy and efficient way to filter and search these specific runs in the MLflow UI.

### How Does It Apply to Hyperparameter Tuning?

* Parameter Sensitivity Analysis: This step allows us to analyze the sensitivity of the model to different parameter values, aiding in a more informed and effective tuning process.
* Efficient Search and Filter: The use of a unique identifier for tagging facilitates an efficient and quick search for these specific runs among a multitude of others, enhancing the user experience in the MLflow UI.

This approach, employing custom parameters and tagging, enhances the clarity and efficiency of the hyperparameter tuning process, contributing to building a more robust and optimized model.

Let’s execute this section of the code and delve deeper into the insights and improvements it offers in the hyperparameter tuning process.
-->

## ハイパーパラメータチューニングプロセスの調整

このセグメントでは、ハイパーパラメータチューニングの反復プロセスをさらに進めています。カスタムパラメータ選択と独自の識別子を用いたタグ付けを導入した追加のハイパーパラメータチューニングランの実行に注目してください。

### 何をしているのか？

カスタムパラメータ選択：異なるパラメータ値（`param_1_values` として `["x", "y", "z"]` と `param_2_values` として `["u", "v", "w"]`）をランで使用しています。

タグ付けのための独自の識別子：独自の識別子（`ident`）をタグ付けに使用し、MLflow UIでこれらの特定のランを効率的かつ簡単にフィルタリングし検索することができます。

### ハイパーパラメータチューニングにどのように適用されるか？

* パラメータ感度分析：このステップにより、異なるパラメータ値に対するモデルの感度を分析でき、より情報に基づいた効果的なチューニングプロセスが支援されます。
* 効率的な検索とフィルタリング：タグ付けのための独自の識別子の使用により、他の多数のランの中からこれらの特定のランを効率的かつ迅速に検索することができ、MLflow UIのユーザー体験が向上します。

カスタムパラメータとタグ付けを採用するこのアプローチは、ハイパーパラメータチューニングプロセスの明瞭性と効率を向上させ、より堅牢で最適化されたモデルの構築に寄与します。

このコードのセクションを実行して、ハイパーパラメータチューニングプロセスで提供される洞察と改善にさらに深く潜りましょう。

```python
# Execute additional hyperparameter tuning runs with custom parameter choices
param_1_values = ["x", "y", "z"]
param_2_values = ["u", "v", "w"]
ident = "params_test_2"
consume(starmap(execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))))
```

<!--
## Refining the Hyperparameter Search Space

In this phase, we focus on refining the hyperparameter search space. This is a crucial step in the hyperparameter tuning process. After a broad exploration of the parameter space, we are now narrowing down our search to a subset of parameter values.

### What Are We Doing?

Sub-setting Parameter Values: We are focusing on a more specific set of parameter values (`param_1_values` as `["b", "c"]` and `param_2_values` as `["d", "f"]`) based on insights gathered from previous runs.

Tagging the Runs: Using a unique identifier (`ident`) for tagging ensures easy filtering and searching of these runs in the MLflow UI.

### How Does It Apply to Hyperparameter Tuning?

* Focused Search: This narrowed search allows us to deeply explore the interactions and impacts of a specific set of parameter values, potentially leading to more optimized models.
* Efficient Resource Utilization: It enables more efficient use of computational resources by focusing the search on promising areas of the parameter space.

### Caution

While this approach is a common tactic in hyperparameter tuning, it’s crucial to acknowledge the implications. Comparing results from the narrowed search space directly with those from the original, broader search space can be misleading.


### Why Is It Invalid to Compare?

* Nature of Bayesian Tuning Algorithms: Bayesian optimization and other tuning algorithms often depend on the exploration of a broad parameter space to make informed decisions. Restricting the parameter space can influence the behavior of these algorithms, leading to biased or suboptimal results.
* Interaction of Hyperparameter Selection Values: Different parameter values have different interactions and impacts on the model performance. A narrowed search space may miss out on capturing these interactions, leading to incomplete or skewed insights.

In conclusion, while refining the search space is essential for efficient and effective hyperparameter tuning, it’s imperative to approach the comparison of results with caution, acknowledging the intricacies and potential biases involved.
-->

## ハイパーパラメータ検索空間の洗練

この段階では、ハイパーパラメータ検索空間を洗練することに焦点を当てています。これは、ハイパーパラメータチューニングプロセスの中で重要なステップです。パラメータ空間の広範な探索の後、私たちは検索を部分的なパラメータ値のサブセットに絞り込んでいます。

### 何をしているのか？

サブセッティングパラメータ値：以前のランから得られた洞察に基づいて、より具体的なパラメータ値のセット（`param_1_values` として `["b", "c"]` および `param_2_values` として `["d", "f"]`）に焦点を当てています。

ランのタグ付け：独自の識別子（`ident`）を使用してタグ付けを行い、これにより MLflow UI でこれらのランを簡単にフィルタリングして検索できます。

### ハイパーパラメータチューニングにどのように適用されるか？

* 集中検索：この絞り込まれた検索により、特定のパラメータ値セットの相互作用と影響を深く探ることができ、より最適化されたモデルにつながる可能性があります。
* 効率的なリソース利用：有望なパラメータ空間のエリアに検索を集中することにより、計算リソースの使用をより効率的にします。

### 注意点

このアプローチはハイパーパラメータチューニングで一般的な戦術であるものの、その意味するところを理解することが重要です。狭められた検索空間からの結果を元の、より広い検索空間からの結果と直接比較することは誤解を招く可能性があります。

### なぜ比較が無効なのか？

* ベイジアンチューニングアルゴリズムの性質：ベイジアン最適化や他のチューニングアルゴリズムは、広いパラメータ空間の探索に依存して情報に基づいた決定を下すことが多いです。パラメータ空間を制限することは、これらのアルゴリズムの挙動に影響を与え、バイアスがかかったり最適でない結果をもたらすことがあります。
* ハイパーパラメータ選択値の相互作用：異なるパラメータ値はモデルのパフォーマンスに異なる影響を及ぼし、相互作用します。狭められた検索空間は、これらの相互作用を捉えることができず、不完全または偏った洞察をもたらす可能性があります。

結論として、検索空間を洗練することは、効率的かつ効果的なハイパーパラメータチューニングには不可欠ですが、結果の比較には慎重にアプローチし、関与する複雑さと潜在的なバイアスを認識することが不可欠です。


```python
param_1_values = ["b", "c"]
param_2_values = ["d", "f"]
ident = "params_test_3"
consume(starmap(execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))))
```

<!--
## Challenge: Logging Best Metrics and Parameters

In the real world of machine learning, it is crucial to keep track of the best performing models and their corresponding parameters for easy comparison and reproduction. Your challenge is to enhance the ``execute_tuning`` function to log the best metrics and parameters from the child runs in each parent run. This way, you can easily compare the best-performing models across different parent runs within the MLflow UI.

### Your Task:

1. Modify the `execute_tuning` function such that for each parent run, it logs the best (minimum) `metric1` found among all its child runs.
2. Alongside the best `metric1`, also log the parameters `param1` and `param2` that yielded this best `metric1`.
3. Ensure that the `execute_tuning` function can accept a `num_child_runs` parameter to specify how many child iterations to perform per parent run.

This is a common practice that allows you to keep your MLflow experiments organized and easily retrievable, making the model selection process smoother and more efficient.

Hint: You might want to return values from the `log_run` function and use these returned values in the `execute_tuning` function to keep track of the best metrics and parameters.

### Note:

Before moving on to the solution below, give it a try yourself! This exercise is a great opportunity to familiarize yourself with advanced features of MLflow and improve your MLOps skills. If you get stuck or want to compare your solution, you can scroll down to see a possible implementation.
-->

## チャレンジ: 最適なメトリクスとパラメータのログ記録

実際の機械学習の世界では、最高のパフォーマンスを発揮するモデルとそれに対応するパラメータを追跡し、簡単に比較および再現できるようにすることが非常に重要です。あなたのチャレンジは、`execute_tuning` 関数を強化して、各親ランからすべての子ランの中で見つかった最良の（最小の）`metric1`をログに記録し、MLflow UI内で異なる親ラン間の最高性能モデルを簡単に比較できるようにすることです。

### あなたの課題:

1. `execute_tuning` 関数を変更して、各親ランにおいて、その中のすべての子ランから見つかった最良の`metric1`をログに記録します。
2. 最良の`metric1`を出した`param1`と`param2`も同時にログに記録します。
3. `execute_tuning`関数が`num_child_runs`パラメータを受け取り、親ランごとに実行する子イテレーションの数を指定できるようにします。

これは、MLflow実験を整理して簡単に取得できるようにする一般的な実践であり、モデル選択プロセスをスムーズかつ効率的にします。

ヒント: `log_run` 関数から値を返して、これらの値を `execute_tuning` 関数で使用し、最良のメトリクスとパラメータを追跡すると良いでしょう。

### 注意:

以下の解決策に進む前に、自分自身で試してみてください！この演習は、MLflowの高度な機能に慣れ親しむ絶好の機会であり、あなたのMLOpsスキルを向上させることができます。詰まった場合や自分の解決策と比較したい場合は、可能な実装を確認するために下にスクロールしてください。


```python
# Define a function to log parameters and metrics and add tag
# logging for search_runs functionality
def log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):
    with mlflow.start_run(run_name=run_name, nested=True) as run:
        param1 = random.choice(param1_choices)
        param2 = random.choice(param2_choices)
        metric1 = random.uniform(0, 1)
        metric2 = abs(random.gauss(5, 2.5))

        mlflow.log_param("param1", param1)
        mlflow.log_param("param2", param2)
        mlflow.log_metric("metric1", metric1)
        mlflow.log_metric("metric2", metric2)
        mlflow.set_tag("test_identifier", tag_ident)

        return run.info.run_id, metric1, param1, param2


# Generate run names
def generate_run_names(test_no, num_runs=5):
    return (f"run_{i}_test_{test_no}" for i in range(num_runs))


# Execute tuning function, allowing for param overrides,
# run_name disambiguation, and tagging support
def execute_tuning(
    test_no,
    param1_choices=("a", "b", "c"),
    param2_choices=("d", "e", "f"),
    test_identifier="",
    num_child_runs=5,
):
    ident = "default" if not test_identifier else test_identifier
    best_metric1 = float("inf")
    best_params = None
    # Use a parent run to encapsulate the child runs
    with mlflow.start_run(run_name=f"parent_run_test_{ident}_{test_no}"):
        # Partial application of the log_run function
        log_current_run = partial(
            log_run,
            test_no=test_no,
            param1_choices=param1_choices,
            param2_choices=param2_choices,
            tag_ident=ident,
        )
        mlflow.set_tag("test_identifier", ident)
        # Generate run names and apply log_current_run function to each run name
        results = list(
            starmap(
                log_current_run,
                ((run_name,) for run_name in generate_run_names(test_no, num_child_runs)),
            )
        )

        for _, metric1, param1, param2 in results:
            if metric1 < best_metric1:
                best_metric1 = metric1
                best_params = (param1, param2)

        mlflow.log_metric("best_metric1", best_metric1)
        mlflow.log_param("best_param1", best_params[0])
        mlflow.log_param("best_param2", best_params[1])
        # Consume the iterator to execute the runs
        consume(results)


# Set the tracking uri and experiment
mlflow.set_tracking_uri("http://localhost:8080")
mlflow.set_experiment("Parent Child Association Challenge")

param_1_values = ["a", "b"]
param_2_values = ["d", "f"]

# Execute hyperparameter tuning runs with custom parameter choices
consume(
    starmap(
        execute_tuning, ((x, param_1_values, param_2_values, "subset_test", 25) for x in range(5))
    )
)
```
