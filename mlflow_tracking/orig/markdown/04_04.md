<!--
# Creating Experiments

* [https://mlflow.org/docs/latest/getting-started/logging-first-model/step3-create-experiment.html](https://mlflow.org/docs/latest/getting-started/logging-first-model/step3-create-experiment.html)

In the previous section, we became familiar with the MLflow Client and its search_experiments API. Before we get into creating experiments and adding metadata tags to them, let’s take a brief look at the MLflow UI.

In the first section of this tutorial, we started the MLflow Tracking Server from a command prompt, specifying the host as `127.0.0.1` and the port as `8080`. Let’s go to the UI and see what the Default Experiment looks like.
-->

# 実験の作成

* [https://mlflow.org/docs/latest/getting-started/logging-first-model/step3-create-experiment.html](https://mlflow.org/docs/latest/getting-started/logging-first-model/step3-create-experiment.html)

前のセクションでは、MLflowクライアントおよびそのsearch_experiments APIについて学びました。実験を作成し、それらにメタデータタグを追加する前に、簡単にMLflow UIについて見てみましょう。

このチュートリアルの最初のセクションで、ホストを`127.0.0.1`、ポートを`8080`と指定してコマンドプロンプトからMLflow Tracking サーバを起動しました。UIにアクセスしてデフォルト実験がどのように見えるか確認しましょう。

<!--
## Viewing the MLflow UI

In order to see the MLflow UI, we simply have to use a web browser to connect to the MLflow Tracking Server and navigate to [http://127.0.0.1:8080](http://127.0.0.1:8080). Once navigating to the url for the MLflow UI, you will see the default experiment with no run data.

![A freshly initialized MLflow UI](https://mlflow.org/docs/latest/_images/default-ui.png)
The default MLflow UI

As you can see, there are no runs recorded and only the Default Experiment (with an ID of 0) is present. This won’t be the case for long, as we’re about to add a new Experiment.
-->

## MLflow UIの表示

MLflow UIを表示するためには、Webブラウザを使用してMLflow Tracking サーバに接続し、[http://127.0.0.1:8080](http://127.0.0.1:8080)にナビゲートするだけです。MLflow UIのURLにアクセスすると、実行データのないデフォルト実験が表示されます。

![初期化直後のMLflow UI](https://mlflow.org/docs/latest/_images/default-ui.png)
デフォルトのMLflow UI

ご覧の通り、記録された実行はなく、IDが0のデフォルト実験のみが存在します。しかし、これは長くは続きません。なぜなら、新しい実験を追加しようとしているからです。

<!--
## Notes on Tags vs Experiments

While MLflow does provide a default experiment, it primarily serves as a ‘catch-all’ safety net for runs initiated without a specified active experiment. However, it’s not recommended for regular use. Instead, creating unique experiments for specific collections of runs offers numerous advantages, as we’ll explore below.
-->

## タグと実験に関する注意点

MLflowはデフォルト実験を提供していますが、これは主に指定されたアクティブな実験なしで開始された実行の「キャッチオール」安全網として機能します。しかし、定期的な使用には推奨されません。代わりに、特定の実行のコレクションに対してユニークな実験を作成することには、以下で探る多くの利点があります。


<!--
### Benefits of Defining Unique Experiments:

1. Enhanced Organization: Experiments allow you to group related runs, making it easier to track and compare them. This is especially helpful when managing numerous runs, as in large-scale projects.
2. Metadata Annotation: Experiments can carry metadata that aids in organizing and associating runs with larger projects.

Consider the scenario below: we’re simulating participation in a large demand forecasting project. This project involves building forecasting models for various departments in a chain of grocery stores, each housing numerous products. Our focus here is the ‘produce’ department, which has several distinct items, each requiring its own forecast model. Organizing these models becomes paramount to ensure easy navigation and comparison.
-->

### ユニークな実験を定義する利点：

1. 強化された組織化：実験を通じて関連する実行をグループ化することで、追跡および比較が容易になります。これは、大規模プロジェクトで数多くの実行を管理する場合に特に役立ちます。
2. メタデータの注釈付け：実験は、より大きなプロジェクトと実行を組織し、関連付けるのに役立つメタデータを持つことができます。

以下のシナリオを考えてみましょう：大規模な需要予測プロジェクトに参加していると仮定します。このプロジェクトには、スーパーチェーンの各部門で予測モデルを構築することが含まれており、それぞれに多数の商品があります。ここでの焦点は「生鮮食品」部門で、それぞれ独自の予測モデルが必要ないくつかの異なるアイテムがあります。これらのモデルを整理することが、簡単にナビゲートして比較するためには最も重要です。


<!--
### When Should You Define an Experiment?

The guiding principle for creating an experiment is the consistency of the input data. If multiple runs use the same input dataset (even if they utilize different portions of it), they logically belong to the same experiment. For other hierarchical categorizations, using tags is advisable.
-->

### 実験を定義するべきタイミングはいつか？

実験を作成するための指導原則は、入力データの一貫性です。複数の実行が同じ入力データセットを使用する場合（異なる部分を利用していても）、論理的には同じ実験に属します。その他の階層的なカテゴリー分けには、タグの使用が推奨されます。


<!--
### Example:

Consider the following structure of the models, mapped to the business product hierarchy:

* Demand Forecasting Project
  * Dairy
    * Cheese
      * Parmesan
      * Cheddar
    * Milk
      * Whole
      * 2%
  * Produce
    * Fruit
      * Apples
      * Cherries
    * Vegetables
      * Carrots

Here, the produce and dairy departments are part of the same overarching project, but they rely on distinct datasets and will likely produce different model metrics. Grouping the departments together definitely doesn’t make sense.

However, the temptation might arise to group all produce together. Grouping diverse items like apples, cherries, and carrots under a single experiment could dilute the effectiveness of run comparisons within that experiment. Thus, it’s essential to demarcate clear boundaries for your experiments to ensure meaningful insights.

Note

While the business product hierarchy in this case doesn’t explicitly need to be captured within the tags, there is nothing preventing you from doing so. There isn’t a limit to the number of tags that you can apply. Provided that the keys being used are consistent across experiments and runs to permit search to function properly, any number of arbitrary mappings between tracked models and your specific business rules can be applied.

To apply these boundaries effectively, as is shown in the figure below, tags should be employed.

![Tags, experiments, and runs relationships](https://mlflow.org/docs/latest/_images/tag-exp-run-relationship.svg)

Effective grouping of modeling runs for a large project
-->

### 例：

以下のモデルの構造をビジネス製品の階層にマッピングしてみましょう：

* 需要予測プロジェクト
  * 乳製品
    * チーズ
      * パルメザン
      * チェダー
    * 牛乳
      * 全脂肪
      * 低脂肪
  * 生鮮食品
    * 果物
      * リンゴ
      * サクランボ
    * 野菜
      * 人参

ここでは、生鮮食品部門と乳製品部門は同じ大規模プロジェクトの一部ですが、異なるデータセットに依存し、おそらく異なるモデルメトリクスを生成するでしょう。部門を一緒にグループ化することは明らかに意味がありません。

しかし、すべての生鮮食品を一緒にグループ化する誘惑が生じるかもしれません。リンゴ、サクランボ、人参のような多様なアイテムを単一の実験の下でグループ化すると、その実験内での実行比較の効果が薄れる可能性があります。したがって、意味のある洞察を確実にするためには、実験のための明確な境界を設定することが不可欠です。

注記

この場合のビジネス製品階層は、タグ内で明示的に捉える必要はありませんが、そうすることを妨げるものは何もありません。適用できるタグの数に制限はありません。実験や実行間で一貫したキーを使用することが条件で、検索が適切に機能することが前提ですが、追跡されるモデルと特定のビジネスルールとの間に任意のマッピングを適用することができます。

これらの境界を効果的に適用するためには、以下の図に示されるように、タグを使用するべきです。

![タグ、実験、および実行の関係](https://mlflow.org/docs/latest/_images/tag-exp-run-relationship.svg)

大規模プロジェクトのためのモデリング実行の効果的なグループ化


<!--
## Creating the Apples Experiment with Meaningful tags
-->

## リンゴ実験の作成と意味のあるタグの使用

```Python
# Provide an Experiment description that will appear in the UI
experiment_description = (
    "This is the grocery forecasting project. "
    "This experiment contains the produce models for apples."
)

# Provide searchable tags that define characteristics of the Runs that
# will be in this Experiment
experiment_tags = {
    "project_name": "grocery-forecasting",
    "store_dept": "produce",
    "team": "stores-ml",
    "project_quarter": "Q3-2023",
    "mlflow.note.content": experiment_description,
}

# Create the Experiment, providing a unique name
produce_apples_experiment = client.create_experiment(
    name="Apple_Models", tags=experiment_tags
)
```

<!--
In the next section, we’ll take a look at what these `tags` can be used for, which are visible in the UI, and how we can leverage the power of tags to simplify access to experiments that are part of a larger project.
-->

次のセクションでは、UIで見ることができるこれらの`タグ`がどのように使用されるか、およびより大きなプロジェクトの一部である実験へのアクセスを簡素化するためにタグの力をどのように活用できるかについて見ていきます。
